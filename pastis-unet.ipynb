{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-14T14:14:21.036937Z","iopub.status.busy":"2023-12-14T14:14:21.036237Z","iopub.status.idle":"2023-12-14T14:14:23.694559Z","shell.execute_reply":"2023-12-14T14:14:23.693740Z","shell.execute_reply.started":"2023-12-14T14:14:21.036901Z"},"trusted":true},"outputs":[],"source":["import json\n","import os\n","from datetime import datetime\n","import pickle as pkl\n","\n","import geopandas as gpd\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.utils.data as tdata\n","import time\n","import torch.nn as nn\n","import torch.nn.init as init\n","\n","import collections.abc\n","import re\n","\n","import torch\n","from torch.nn import functional as F\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# %pip install geopandas"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T14:14:23.697124Z","iopub.status.busy":"2023-12-14T14:14:23.696693Z","iopub.status.idle":"2023-12-14T14:14:23.739906Z","shell.execute_reply":"2023-12-14T14:14:23.739056Z","shell.execute_reply.started":"2023-12-14T14:14:23.697091Z"},"trusted":true},"outputs":[],"source":["class PASTIS_Dataset(tdata.Dataset):\n","    def __init__(\n","        self,\n","        folder,\n","        norm=True,\n","        target=\"semantic\",\n","        cache=False,\n","        mem16=False,\n","        folds=None,\n","        reference_date=\"2018-09-01\",\n","        class_mapping=None,\n","        mono_date=None,\n","        sats=[\"S2\"],\n","    ):\n","        \"\"\"\n","        Pytorch Dataset class to load samples from the PASTIS dataset, for semantic and\n","        panoptic segmentation.\n","\n","        The Dataset yields ((data, dates), target) tuples, where:\n","            - data contains the image time series\n","            - dates contains the date sequence of the observations expressed in number\n","              of days since a reference date\n","            - target is the semantic or instance target\n","\n","        Args:\n","            folder (str): Path to the dataset\n","            norm (bool): If true, images are standardised using pre-computed\n","                channel-wise means and standard deviations.\n","            reference_date (str, Format : 'YYYY-MM-DD'): Defines the reference date\n","                based on which all observation dates are expressed. Along with the image\n","                time series and the target tensor, this dataloader yields the sequence\n","                of observation dates (in terms of number of days since the reference\n","                date). This sequence of dates is used for instance for the positional\n","                encoding in attention based approaches.\n","            target (str): 'semantic' or 'instance'. Defines which type of target is\n","                returned by the dataloader.\n","                * If 'semantic' the target tensor is a tensor containing the class of\n","                  each pixel.\n","                * If 'instance' the target tensor is the concatenation of several\n","                  signals, necessary to train the Parcel-as-Points module:\n","                    - the centerness heatmap,\n","                    - the instance ids,\n","                    - the voronoi partitioning of the patch with regards to the parcels'\n","                      centers,\n","                    - the (height, width) size of each parcel\n","                    - the semantic label of each parcel\n","                    - the semantic label of each pixel\n","            cache (bool): If True, the loaded samples stay in RAM, default False.\n","            mem16 (bool): Additional argument for cache. If True, the image time\n","                series tensors are stored in half precision in RAM for efficiency.\n","                They are cast back to float32 when returned by __getitem__.\n","            folds (list, optional): List of ints specifying which of the 5 official\n","                folds to load. By default (when None is specified) all folds are loaded.\n","            class_mapping (dict, optional): Dictionary to define a mapping between the\n","                default 18 class nomenclature and another class grouping, optional.\n","            mono_date (int or str, optional): If provided only one date of the\n","                available time series is loaded. If argument is an int it defines the\n","                position of the date that is loaded. If it is a string, it should be\n","                in format 'YYYY-MM-DD' and the closest available date will be selected.\n","            sats (list): defines the satellites to use. If you are using PASTIS-R, you have access to\n","                Sentinel-2 imagery and Sentinel-1 observations in Ascending and Descending orbits,\n","                respectively S2, S1A, and S1D.\n","                For example use sats=['S2', 'S1A'] for Sentinel-2 + Sentinel-1 ascending time series,\n","                or sats=['S2', 'S1A','S1D'] to retrieve all time series.\n","                If you are using PASTIS, only  S2 observations are available.\n","        \"\"\"\n","        super(PASTIS_Dataset, self).__init__()\n","        self.folder = folder\n","        self.norm = norm\n","        self.reference_date = datetime(*map(int, reference_date.split(\"-\")))\n","        self.cache = cache\n","        self.mem16 = mem16\n","        self.mono_date = None\n","        if mono_date is not None:\n","            self.mono_date = (\n","                datetime(*map(int, mono_date.split(\"-\")))\n","                if \"-\" in mono_date\n","                else int(mono_date)\n","            )\n","        self.memory = {}\n","        self.memory_dates = {}\n","        self.class_mapping = (\n","            np.vectorize(lambda x: class_mapping[x])\n","            if class_mapping is not None\n","            else class_mapping\n","        )\n","        self.target = target\n","        self.sats = sats\n","\n","        # Get metadata\n","        print(\"Reading patch metadata . . .\")\n","        self.meta_patch = gpd.read_file(os.path.join(folder, \"metadata.geojson\"))\n","        self.meta_patch.index = self.meta_patch[\"ID_PATCH\"].astype(int)\n","        self.meta_patch.sort_index(inplace=True)\n","\n","        self.date_tables = {s: None for s in sats}\n","        self.date_range = np.array(range(-200, 600))\n","        for s in sats:\n","            dates = self.meta_patch[\"dates-{}\".format(s)]\n","            date_table = pd.DataFrame(\n","                index=self.meta_patch.index, columns=self.date_range, dtype=int\n","            )\n","            for pid, date_seq in dates.items():\n","                d = pd.DataFrame().from_dict(date_seq, orient=\"index\")\n","                d = d[0].apply(\n","                    lambda x: (\n","                        datetime(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:]))\n","                        - self.reference_date\n","                    ).days\n","                )\n","                date_table.loc[pid, d.values] = 1\n","            date_table = date_table.fillna(0)\n","            self.date_tables[s] = {\n","                index: np.array(list(d.values()))\n","                for index, d in date_table.to_dict(orient=\"index\").items()\n","            }\n","\n","        print(\"Done.\")\n","\n","        # Select Fold samples\n","        if folds is not None:\n","            self.meta_patch = pd.concat(\n","                [self.meta_patch[self.meta_patch[\"Fold\"] == f] for f in folds]\n","            )\n","\n","        self.len = self.meta_patch.shape[0]\n","        self.id_patches = self.meta_patch.index\n","\n","        # Get normalisation values\n","        if norm:\n","            self.norm = {}\n","            for s in self.sats:\n","                with open(\n","                    os.path.join(folder, \"NORM_{}_patch.json\".format(s)), \"r\"\n","                ) as file:\n","                    normvals = json.loads(file.read())\n","                selected_folds = folds if folds is not None else range(1, 6)\n","                means = [normvals[\"Fold_{}\".format(f)][\"mean\"] for f in selected_folds]\n","                stds = [normvals[\"Fold_{}\".format(f)][\"std\"] for f in selected_folds]\n","                self.norm[s] = np.stack(means).mean(axis=0), np.stack(stds).mean(axis=0)\n","                self.norm[s] = (\n","                    torch.from_numpy(self.norm[s][0]).float(),\n","                    torch.from_numpy(self.norm[s][1]).float(),\n","                )\n","        else:\n","            self.norm = None\n","        print(\"Dataset ready.\")\n","\n","    def __len__(self):\n","        return self.len\n","\n","    def get_dates(self, id_patch, sat):\n","        return self.date_range[np.where(self.date_tables[sat][id_patch] == 1)[0]]\n","\n","    def __getitem__(self, item):\n","        id_patch = self.id_patches[item]\n","\n","        # Retrieve and prepare satellite data\n","        if not self.cache or item not in self.memory.keys():\n","            data = {\n","                satellite: np.load(\n","                    os.path.join(\n","                        self.folder,\n","                        \"DATA_{}\".format(satellite),\n","                        \"{}_{}.npy\".format(satellite, id_patch),\n","                    )\n","                ).astype(np.float32)\n","                for satellite in self.sats\n","            }  # T x C x H x W arrays\n","            data = {s: torch.from_numpy(a) for s, a in data.items()}\n","\n","            if self.norm is not None:\n","                data = {\n","                    s: (d - self.norm[s][0][None, :, None, None])\n","                    / self.norm[s][1][None, :, None, None]\n","                    for s, d in data.items()\n","                }\n","\n","            if self.target == \"semantic\":\n","                target = np.load(\n","                    os.path.join(\n","                        self.folder, \"ANNOTATIONS\", \"TARGET_{}.npy\".format(id_patch)\n","                    )\n","                )\n","                target = torch.from_numpy(target[0].astype(int))\n","\n","                if self.class_mapping is not None:\n","                    target = self.class_mapping(target)\n","\n","            elif self.target == \"instance\":\n","                heatmap = np.load(\n","                    os.path.join(\n","                        self.folder,\n","                        \"INSTANCE_ANNOTATIONS\",\n","                        \"HEATMAP_{}.npy\".format(id_patch),\n","                    )\n","                )\n","\n","                instance_ids = np.load(\n","                    os.path.join(\n","                        self.folder,\n","                        \"INSTANCE_ANNOTATIONS\",\n","                        \"INSTANCES_{}.npy\".format(id_patch),\n","                    )\n","                )\n","                pixel_to_object_mapping = np.load(\n","                    os.path.join(\n","                        self.folder,\n","                        \"INSTANCE_ANNOTATIONS\",\n","                        \"ZONES_{}.npy\".format(id_patch),\n","                    )\n","                )\n","\n","                pixel_semantic_annotation = np.load(\n","                    os.path.join(\n","                        self.folder, \"ANNOTATIONS\", \"TARGET_{}.npy\".format(id_patch)\n","                    )\n","                )\n","\n","                if self.class_mapping is not None:\n","                    pixel_semantic_annotation = self.class_mapping(\n","                        pixel_semantic_annotation[0]\n","                    )\n","                else:\n","                    pixel_semantic_annotation = pixel_semantic_annotation[0]\n","\n","                size = np.zeros((*instance_ids.shape, 2))\n","                object_semantic_annotation = np.zeros(instance_ids.shape)\n","                for instance_id in np.unique(instance_ids):\n","                    if instance_id != 0:\n","                        h = (instance_ids == instance_id).any(axis=-1).sum()\n","                        w = (instance_ids == instance_id).any(axis=-2).sum()\n","                        size[pixel_to_object_mapping == instance_id] = (h, w)\n","                        object_semantic_annotation[\n","                            pixel_to_object_mapping == instance_id\n","                        ] = pixel_semantic_annotation[instance_ids == instance_id][0]\n","\n","                target = torch.from_numpy(\n","                    np.concatenate(\n","                        [\n","                            heatmap[:, :, None],  # 0\n","                            instance_ids[:, :, None],  # 1\n","                            pixel_to_object_mapping[:, :, None],  # 2\n","                            size,  # 3-4\n","                            object_semantic_annotation[:, :, None],  # 5\n","                            pixel_semantic_annotation[:, :, None],  # 6\n","                        ],\n","                        axis=-1,\n","                    )\n","                ).float()\n","\n","            if self.cache:\n","                if self.mem16:\n","                    self.memory[item] = [{k: v.half() for k, v in data.items()}, target]\n","                else:\n","                    self.memory[item] = [data, target]\n","\n","        else:\n","            data, target = self.memory[item]\n","            if self.mem16:\n","                data = {k: v.float() for k, v in data.items()}\n","\n","        # Retrieve date sequences\n","        if not self.cache or id_patch not in self.memory_dates.keys():\n","            dates = {\n","                s: torch.from_numpy(self.get_dates(id_patch, s)) for s in self.sats\n","            }\n","            if self.cache:\n","                self.memory_dates[id_patch] = dates\n","        else:\n","            dates = self.memory_dates[id_patch]\n","\n","        if self.mono_date is not None:\n","            if isinstance(self.mono_date, int):\n","                data = {s: data[s][self.mono_date].unsqueeze(0) for s in self.sats}\n","                dates = {s: dates[s][self.mono_date] for s in self.sats}\n","            else:\n","                mono_delta = (self.mono_date - self.reference_date).days\n","                mono_date = {\n","                    s: int((dates[s] - mono_delta).abs().argmin()) for s in self.sats\n","                }\n","                data = {s: data[s][mono_date[s]].unsqueeze(0) for s in self.sats}\n","                dates = {s: dates[s][mono_date[s]] for s in self.sats}\n","\n","        if self.mem16:\n","            return ({k: v.float() for k, v in data.items()}, dates), target\n","        else:\n","            return (data, dates), target"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T14:14:23.741683Z","iopub.status.busy":"2023-12-14T14:14:23.741269Z","iopub.status.idle":"2023-12-14T14:14:23.759184Z","shell.execute_reply":"2023-12-14T14:14:23.758263Z","shell.execute_reply.started":"2023-12-14T14:14:23.741649Z"},"trusted":true},"outputs":[],"source":["\n","\n","\n","def pad_tensor(x, l, pad_value=0):\n","    padlen = l - x.shape[0]\n","    pad = [0 for _ in range(2 * len(x.shape[1:]))] + [0, padlen]\n","    return F.pad(x, pad=pad, value=pad_value)\n","\n","\n","np_str_obj_array_pattern = re.compile(r\"[SaUO]\")\n","\n","\n","def pad_collate(batch, pad_value=0):\n","    # Utility function to be used as collate_fn for the PyTorch dataloader\n","    # to handle sequences of varying length.\n","    # Sequences are padded with zeros by default.\n","    #\n","    # Modified default_collate from the official pytorch repo\n","    # https://github.com/pytorch/pytorch/blob/master/torch/utils/data/_utils/collate.py\n","    elem = batch[0]\n","    elem_type = type(elem)\n","    if isinstance(elem, torch.Tensor):\n","        out = None\n","        if len(elem.shape) > 0:\n","            sizes = [e.shape[0] for e in batch]\n","            m = max(sizes)\n","            if not all(s == m for s in sizes):\n","                # pad tensors which have a temporal dimension\n","                batch = [pad_tensor(e, m, pad_value=pad_value) for e in batch]\n","        if torch.utils.data.get_worker_info() is not None:\n","            # If we're in a background process, concatenate directly into a\n","            # shared memory tensor to avoid an extra copy\n","            numel = sum([x.numel() for x in batch])\n","            storage = elem.storage()._new_shared(numel)\n","            out = elem.new(storage)\n","        return torch.stack(batch, 0, out=out)\n","    elif (\n","        elem_type.__module__ == \"numpy\"\n","        and elem_type.__name__ != \"str_\"\n","        and elem_type.__name__ != \"string_\"\n","    ):\n","        if elem_type.__name__ == \"ndarray\" or elem_type.__name__ == \"memmap\":\n","            # array of string classes and object\n","            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n","                raise TypeError(\"Format not managed : {}\".format(elem.dtype))\n","\n","            return pad_collate([torch.as_tensor(b) for b in batch])\n","        elif elem.shape == ():  # scalars\n","            return torch.as_tensor(batch)\n","\n","    elif isinstance(elem, collections.abc.Mapping):\n","        return {key: pad_collate([d[key] for d in batch]) for key in elem}\n","\n","    elif isinstance(elem, tuple) and hasattr(elem, \"_fields\"):  # namedtuple\n","        return elem_type(*(pad_collate(samples) for samples in zip(*batch)))\n","\n","    elif isinstance(elem, collections.abc.Sequence):\n","        # check to make sure that the elements in batch have consistent size\n","        it = iter(batch)\n","        elem_size = len(next(it))\n","        if not all(len(elem) == elem_size for elem in it):\n","            raise RuntimeError(\"each element in list of batch should be of equal size\")\n","        transposed = zip(*batch)\n","        return [pad_collate(samples) for samples in transposed]\n","\n","    raise TypeError(\"Format not managed : {}\".format(elem_type))\n","    \n","def get_ntrainparams(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T14:14:23.760774Z","iopub.status.busy":"2023-12-14T14:14:23.760329Z","iopub.status.idle":"2023-12-14T14:14:23.771845Z","shell.execute_reply":"2023-12-14T14:14:23.771111Z","shell.execute_reply.started":"2023-12-14T14:14:23.760738Z"},"trusted":true},"outputs":[],"source":["# fold_sequence = [\n","#         [[1, 2, 3], [4], [5]],\n","#         [[2, 3, 4], [5], [1]],\n","#         [[3, 4, 5], [1], [2]],\n","#         [[4, 5, 1], [2], [3]],\n","#         [[5, 1, 2], [3], [4]],\n","#     ]\n","# for fold, (train_folds, val_fold, test_fold) in enumerate(fold_sequence):\n","#     print((train_folds, val_fold, test_fold))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Model definition"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T14:14:23.775374Z","iopub.status.busy":"2023-12-14T14:14:23.774886Z","iopub.status.idle":"2023-12-14T14:14:23.796765Z","shell.execute_reply":"2023-12-14T14:14:23.795833Z","shell.execute_reply.started":"2023-12-14T14:14:23.775348Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","\n","def conv_block(in_dim, middle_dim, out_dim):\n","    model = nn.Sequential(\n","        nn.Conv3d(in_dim, middle_dim, kernel_size=3, stride=1, padding=1),\n","        nn.BatchNorm3d(middle_dim),\n","        nn.LeakyReLU(inplace=True),\n","        nn.Conv3d(middle_dim, out_dim, kernel_size=3, stride=1, padding=1),\n","        nn.BatchNorm3d(out_dim),\n","        nn.LeakyReLU(inplace=True),\n","    )\n","    return model\n","\n","\n","def center_in(in_dim, out_dim):\n","    model = nn.Sequential(\n","        nn.Conv3d(in_dim, out_dim, kernel_size=3, stride=1, padding=1),\n","        nn.BatchNorm3d(out_dim),\n","        nn.LeakyReLU(inplace=True))\n","    return model\n","\n","\n","def center_out(in_dim, out_dim):\n","    model = nn.Sequential(\n","        nn.Conv3d(in_dim, in_dim, kernel_size=3, stride=1, padding=1),\n","        nn.BatchNorm3d(in_dim),\n","        nn.LeakyReLU(inplace=True),\n","        nn.ConvTranspose3d(in_dim, out_dim, kernel_size=3, stride=2, padding=1, output_padding=1))\n","    return model\n","\n","\n","def up_conv_block(in_dim, out_dim):\n","    model = nn.Sequential(\n","        nn.ConvTranspose3d(in_dim, out_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n","        nn.BatchNorm3d(out_dim),\n","        nn.LeakyReLU(inplace=True),\n","    )\n","    return model\n","\n","\n","class UNet3D(nn.Module):\n","    def __init__(self, in_channel, n_classes, feats=8, pad_value=None, zero_pad=True):\n","        super(UNet3D, self).__init__()\n","        self.in_channel = in_channel\n","        self.n_classes = n_classes\n","        self.pad_value = pad_value\n","        self.zero_pad = zero_pad\n","\n","        self.en3 = conv_block(in_channel, feats * 4, feats * 4)\n","        self.pool_3 = nn.MaxPool3d(kernel_size=2, stride=2, padding=0)\n","        self.en4 = conv_block(feats * 4, feats * 8, feats * 8)\n","        self.pool_4 = nn.MaxPool3d(kernel_size=2, stride=2, padding=0)\n","        self.center_in = center_in(feats * 8, feats * 16)\n","        self.center_out = center_out(feats * 16, feats * 8)\n","        self.dc4 = conv_block(feats * 16, feats * 8, feats * 8)\n","        self.trans3 = up_conv_block(feats * 8, feats * 4)\n","        self.dc3 = conv_block(feats * 8, feats * 4, feats * 2)\n","        self.final = nn.Conv3d(feats * 2, n_classes, kernel_size=3, stride=1, padding=1)\n","        # self.fn = nn.Linear(timesteps, 1)\n","        # self.logsoftmax = nn.LogSoftmax(dim=1)\n","        # self.dropout = nn.Dropout(p=dropout, inplace=True)\n","\n","    def forward(self, x, batch_positions=None):\n","        out = x.permute(0, 2, 1, 3, 4)\n","        if self.pad_value is not None:\n","            pad_mask = (out == self.pad_value).all(dim=-1).all(dim=-1).all(dim=1)  # BxT pad mask\n","            if self.zero_pad:\n","                out[out == self.pad_value] = 0\n","        en3 = self.en3(out)\n","        pool_3 = self.pool_3(en3)\n","        en4 = self.en4(pool_3)\n","        pool_4 = self.pool_4(en4)\n","        center_in = self.center_in(pool_4)\n","        center_out = self.center_out(center_in)\n","        concat4 = torch.cat([center_out, en4[:, :, :center_out.shape[2], :, :]], dim=1)\n","        dc4 = self.dc4(concat4)\n","        trans3 = self.trans3(dc4)\n","        concat3 = torch.cat([trans3, en3[:, :, :trans3.shape[2], :, :]], dim=1)\n","        dc3 = self.dc3(concat3)\n","        final = self.final(dc3)\n","        final = final.permute(0, 1, 3, 4, 2)  # BxCxHxWxT\n","\n","        # shape_num = final.shape[0:4]\n","        # final = final.reshape(-1,final.shape[4])\n","        if self.pad_value is not None:\n","            if pad_mask.any():\n","                # masked mean\n","                pad_mask = pad_mask[:, :final.shape[-1]] #match new temporal length (due to pooling)\n","                pad_mask = ~pad_mask # 0 on padded values\n","                out = (final.permute(1, 2, 3, 0, 4) * pad_mask[None, None, None, :, :]).sum(dim=-1) / pad_mask.sum(\n","                    dim=-1)[None, None, None, :]\n","                out = out.permute(3, 0, 1, 2)\n","            else:\n","                out = final.mean(dim=-1)\n","        else:\n","            out = final.mean(dim=-1)\n","        # final = self.dropout(final)\n","        # final = self.fn(final)\n","        # final = final.reshape(shape_num)\n","\n","        return out"]},{"cell_type":"markdown","metadata":{},"source":["### METRICS"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T14:14:23.798333Z","iopub.status.busy":"2023-12-14T14:14:23.797986Z","iopub.status.idle":"2023-12-14T14:14:23.829049Z","shell.execute_reply":"2023-12-14T14:14:23.828092Z","shell.execute_reply.started":"2023-12-14T14:14:23.798302Z"},"trusted":true},"outputs":[],"source":["class Metric(object):\n","    \"\"\"Base class for all metrics.\n","    From: https://github.com/pytorch/tnt/blob/master/torchnet/meter/meter.py\n","    \"\"\"\n","\n","    def reset(self):\n","        pass\n","\n","    def add(self):\n","        pass\n","\n","    def value(self):\n","        pass\n","\n","\n","class ConfusionMatrix(Metric):\n","    \"\"\"Constructs a confusion matrix for a multi-class classification problems.\n","\n","    Does not support multi-label, multi-class problems.\n","\n","    Keyword arguments:\n","    - num_classes (int): number of classes in the classification problem.\n","    - normalized (boolean, optional): Determines whether or not the confusion\n","    matrix is normalized or not. Default: False.\n","\n","    Modified from: https://github.com/pytorch/tnt/blob/master/torchnet/meter/confusionmeter.py\n","    \"\"\"\n","\n","    def __init__(self, num_classes, normalized=False, device='cpu', lazy=True):\n","        super().__init__()\n","        if device == 'cpu':\n","            self.conf = np.ndarray((num_classes, num_classes), dtype=np.int64)\n","        else:\n","            self.conf = torch.zeros((num_classes, num_classes)).cuda()\n","        self.normalized = normalized\n","        self.num_classes = num_classes\n","        self.device = device\n","        self.reset()\n","        self.lazy = lazy\n","\n","    def reset(self):\n","        if self.device == 'cpu':\n","            self.conf.fill(0)\n","        else:\n","            self.conf = torch.zeros(self.conf.shape).cuda()\n","\n","    def add(self, predicted, target):\n","        \"\"\"Computes the confusion matrix\n","\n","        The shape of the confusion matrix is K x K, where K is the number\n","        of classes.\n","\n","        Keyword arguments:\n","        - predicted (Tensor or numpy.ndarray): Can be an N x K tensor/array of\n","        predicted scores obtained from the model for N examples and K classes,\n","        or an N-tensor/array of integer values between 0 and K-1.\n","        - target (Tensor or numpy.ndarray): Can be an N x K tensor/array of\n","        ground-truth classes for N examples and K classes, or an N-tensor/array\n","        of integer values between 0 and K-1.\n","\n","        \"\"\"\n","\n","        # If target and/or predicted are tensors, convert them to numpy arrays\n","        if self.device == 'cpu':\n","            if torch.is_tensor(predicted):\n","                predicted = predicted.cpu().numpy()\n","            if torch.is_tensor(target):\n","                target = target.cpu().numpy()\n","\n","        assert predicted.shape[0] == target.shape[0], \\\n","            'number of targets and predicted outputs do not match'\n","\n","        if len(predicted.shape) != 1:\n","            assert predicted.shape[1] == self.num_classes, \\\n","                'number of predictions does not match size of confusion matrix'\n","            predicted = predicted.argmax(1)\n","        else:\n","            if not self.lazy:\n","                assert (predicted.max() < self.num_classes) and (predicted.min() >= 0), \\\n","                    'predicted values are not between 0 and k-1'\n","\n","        if len(target.shape) != 1:\n","            if not self.lazy:\n","                assert target.shape[1] == self.num_classes, \\\n","                    'Onehot target does not match size of confusion matrix'\n","                assert (target >= 0).all() and (target <= 1).all(), \\\n","                    'in one-hot encoding, target values should be 0 or 1'\n","                assert (target.sum(1) == 1).all(), \\\n","                    'multi-label setting is not supported'\n","            target = target.argmax(1)\n","        else:\n","            if not self.lazy:\n","                assert (target.max() < self.num_classes) and (target.min() >= 0), \\\n","                    'target values are not between 0 and k-1'\n","\n","        # hack for bincounting 2 arrays together\n","        x = predicted + self.num_classes * target\n","\n","        if self.device == 'cpu':\n","            bincount_2d = np.bincount(\n","                x.astype(np.int64), minlength=self.num_classes ** 2)\n","            assert bincount_2d.size == self.num_classes ** 2\n","            conf = bincount_2d.reshape((self.num_classes, self.num_classes))\n","        else:\n","            bincount_2d = torch.bincount(\n","                x, minlength=self.num_classes ** 2)\n","\n","            conf = bincount_2d.view((self.num_classes, self.num_classes))\n","        self.conf += conf\n","\n","    def value(self):\n","        \"\"\"\n","        Returns:\n","            Confustion matrix of K rows and K columns, where rows corresponds\n","            to ground-truth targets and columns corresponds to predicted\n","            targets.\n","        \"\"\"\n","        if self.normalized:\n","            conf = self.conf.astype(np.float32)\n","            return conf / conf.sum(1).clip(min=1e-12)[:, None]\n","        else:\n","            return self.conf\n","\n","\n","class IoU(Metric):\n","    \"\"\"Computes the intersection over union (IoU) per class and corresponding\n","    mean (mIoU).\n","\n","    Intersection over union (IoU) is a common evaluation metric for semantic\n","    segmentation. The predictions are first accumulated in a confusion matrix\n","    and the IoU is computed from it as follows:\n","\n","        IoU = true_positive / (true_positive + false_positive + false_negative).\n","\n","    Keyword arguments:\n","    - num_classes (int): number of classes in the classification problem\n","    - normalized (boolean, optional): Determines whether or not the confusion\n","    matrix is normalized or not. Default: False.\n","    - ignore_index (int or iterable, optional): Index of the classes to ignore\n","    when computing the IoU. Can be an int, or any iterable of ints.\n","    \"\"\"\n","\n","    def __init__(self, num_classes, normalized=False, ignore_index=None, cm_device='cpu', lazy=True):\n","        super().__init__()\n","        self.conf_metric = ConfusionMatrix(num_classes, normalized, device=cm_device, lazy=lazy)\n","        self.lazy = lazy\n","        if ignore_index is None:\n","            self.ignore_index = None\n","        elif isinstance(ignore_index, int):\n","            self.ignore_index = (ignore_index,)\n","        else:\n","            try:\n","                self.ignore_index = tuple(ignore_index)\n","            except TypeError:\n","                raise ValueError(\"'ignore_index' must be an int or iterable\")\n","\n","    def reset(self):\n","        self.conf_metric.reset()\n","\n","    def add(self, predicted, target):\n","        \"\"\"Adds the predicted and target pair to the IoU metric.\n","\n","        Keyword arguments:\n","        - predicted (Tensor): Can be a (N, K, H, W) tensor of\n","        predicted scores obtained from the model for N examples and K classes,\n","        or (N, H, W) tensor of integer values between 0 and K-1.\n","        - target (Tensor): Can be a (N, K, H, W) tensor of\n","        target scores for N examples and K classes, or (N, H, W) tensor of\n","        integer values between 0 and K-1.\n","\n","        \"\"\"\n","        # Dimensions check\n","        assert predicted.size(0) == target.size(0), \\\n","            'number of targets and predicted outputs do not match'\n","        assert predicted.dim() == 3 or predicted.dim() == 4, \\\n","            \"predictions must be of dimension (N, H, W) or (N, K, H, W)\"\n","        assert target.dim() == 3 or target.dim() == 4, \\\n","            \"targets must be of dimension (N, H, W) or (N, K, H, W)\"\n","\n","        # If the tensor is in categorical format convert it to integer format\n","        if predicted.dim() == 4:\n","            _, predicted = predicted.max(1)\n","        if target.dim() == 4:\n","            _, target = target.max(1)\n","\n","        self.conf_metric.add(predicted.view(-1), target.view(-1))\n","\n","    def value(self):\n","        \"\"\"Computes the IoU and mean IoU.\n","\n","        The mean computation ignores NaN elements of the IoU array.\n","\n","        Returns:\n","            Tuple: (IoU, mIoU). The first output is the per class IoU,\n","            for K classes it's numpy.ndarray with K elements. The second output,\n","            is the mean IoU.\n","        \"\"\"\n","        conf_matrix = self.conf_metric.value()\n","        if self.ignore_index is not None:\n","            conf_matrix[:, self.ignore_index] = 0\n","            conf_matrix[self.ignore_index, :] = 0\n","        true_positive = np.diag(conf_matrix)\n","        false_positive = np.sum(conf_matrix, 0) - true_positive\n","        false_negative = np.sum(conf_matrix, 1) - true_positive\n","\n","        # Just in case we get a division by 0, ignore/hide the error\n","        with np.errstate(divide='ignore', invalid='ignore'):\n","            iou = true_positive / (true_positive + false_positive + false_negative)\n","\n","        return iou, np.nanmean(iou)\n","\n","    def get_miou_acc(self):\n","        conf_matrix = self.conf_metric.value()\n","        if torch.is_tensor(conf_matrix):\n","            conf_matrix = conf_matrix.cpu().numpy()\n","        if self.ignore_index is not None:\n","            conf_matrix[:, self.ignore_index] = 0\n","            conf_matrix[self.ignore_index, :] = 0\n","        true_positive = np.diag(conf_matrix)\n","        false_positive = np.sum(conf_matrix, 0) - true_positive\n","        false_negative = np.sum(conf_matrix, 1) - true_positive\n","\n","        # Just in case we get a division by 0, ignore/hide the error\n","        with np.errstate(divide='ignore', invalid='ignore'):\n","            iou = true_positive / (true_positive + false_positive + false_negative)\n","        miou = float(np.nanmean(iou) * 100)\n","        acc = float(np.diag(conf_matrix).sum() / conf_matrix.sum() * 100)\n","\n","        return miou, acc"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T14:14:23.830576Z","iopub.status.busy":"2023-12-14T14:14:23.830311Z","iopub.status.idle":"2023-12-14T14:14:23.845969Z","shell.execute_reply":"2023-12-14T14:14:23.845111Z","shell.execute_reply.started":"2023-12-14T14:14:23.830553Z"},"trusted":true},"outputs":[],"source":["def confusion_matrix_analysis(mat):\n","    \"\"\"\n","    This method computes all the performance metrics from the confusion matrix. In addition to overall accuracy, the\n","    precision, recall, f-score and IoU for each class is computed.\n","    The class-wise metrics are averaged to provide overall indicators in two ways (MICRO and MACRO average)\n","    Args:\n","        mat (array): confusion matrix\n","\n","    Returns:\n","        per_class (dict) : per class metrics\n","        overall (dict): overall metrics\n","\n","    \"\"\"\n","    TP = 0\n","    FP = 0\n","    FN = 0\n","\n","    per_class = {}\n","\n","    for j in range(mat.shape[0]):\n","        d = {}\n","        tp = np.sum(mat[j, j])\n","        fp = np.sum(mat[:, j]) - tp\n","        fn = np.sum(mat[j, :]) - tp\n","\n","        d['IoU'] = tp / (tp + fp + fn)\n","        d['Precision'] = tp / (tp + fp)\n","        d['Recall'] = tp / (tp + fn)\n","        d['F1-score'] = 2 * tp / (2 * tp + fp + fn)\n","\n","        per_class[str(j)] = d\n","\n","        TP += tp\n","        FP += fp\n","        FN += fn\n","\n","    overall = {}\n","    overall['micro_IoU'] = TP / (TP + FP + FN)\n","    overall['micro_Precision'] = TP / (TP + FP)\n","    overall['micro_Recall'] = TP / (TP + FN)\n","    overall['micro_F1-score'] = 2 * TP / (2 * TP + FP + FN)\n","\n","    macro = pd.DataFrame(per_class).transpose().mean()\n","    overall['MACRO_IoU'] = macro.loc['IoU']\n","    overall['MACRO_Precision'] = macro.loc['Precision']\n","    overall['MACRO_Recall'] = macro.loc['Recall']\n","    overall['MACRO_F1-score'] = macro.loc['F1-score']\n","\n","    overall['Accuracy'] = np.sum(np.diag(mat)) / np.sum(mat)\n","\n","    return per_class, overall"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T14:14:23.848229Z","iopub.status.busy":"2023-12-14T14:14:23.847883Z","iopub.status.idle":"2023-12-14T14:14:23.860681Z","shell.execute_reply":"2023-12-14T14:14:23.859882Z","shell.execute_reply.started":"2023-12-14T14:14:23.848199Z"},"trusted":true},"outputs":[],"source":["def iterate(\n","    model, data_loader, criterion, optimizer=None, mode=\"train\", device=None\n","):\n","    iou_meter = IoU(\n","        num_classes=NUM_CLASSES,\n","        ignore_index=IGNORE_INDEX,\n","        cm_device=DEVICE,\n","    )\n","\n","    t_start = time.time()\n","    for i, batch in enumerate(data_loader):\n","        if device is not None:\n","            batch = recursive_todevice(batch, DEVICE)\n","        (x, dates), y = batch\n","        (x, dates) = x['S2'], dates['S2']\n","        y = y.long()\n","\n","        if mode != \"train\":\n","            with torch.no_grad():\n","                out = model(x, batch_positions=dates)\n","        else:\n","            optimizer.zero_grad()\n","            out = model(x, batch_positions=dates)\n","\n","        loss = criterion(out, y)\n","        if mode == \"train\":\n","            loss.backward()\n","            optimizer.step()\n","\n","        with torch.no_grad():\n","            pred = out.argmax(dim=1)\n","        iou_meter.add(pred, y)\n","#         loss_meter.add(loss.item())\n","\n","        if (i + 1) % DISPLAY_SETUP == 0:\n","            miou, acc = iou_meter.get_miou_acc()\n","            print(\n","                \"Step [{}/{}], Loss: {:.4f}, Acc : {:.2f}, mIoU {:.2f}\".format(\n","                    i + 1, len(data_loader), loss.item(), acc, miou\n","                )\n","            )\n","\n","    t_end = time.time()\n","    total_time = t_end - t_start\n","    print(\"Epoch time : {:.1f}s\".format(total_time))\n","    miou, acc = iou_meter.get_miou_acc()\n","    metrics = {\n","        \"{}_accuracy\".format(mode): acc,\n","        \"{}_loss\".format(mode): loss.item(),\n","        \"{}_IoU\".format(mode): miou,\n","        \"{}_epoch_time\".format(mode): total_time,\n","    }\n","\n","    if mode == \"test\":\n","        return metrics, iou_meter.conf_metric.value()  # confusion matrix\n","    else:\n","        return metrics\n","\n","\n","def recursive_todevice(x, device):\n","    if isinstance(x, torch.Tensor):\n","        return x.to(device)\n","    elif isinstance(x, dict):\n","        return {k: recursive_todevice(v, device) for k, v in x.items()}\n","    else:\n","        return [recursive_todevice(c, device) for c in x]"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T14:14:23.862180Z","iopub.status.busy":"2023-12-14T14:14:23.861891Z","iopub.status.idle":"2023-12-14T14:14:23.880698Z","shell.execute_reply":"2023-12-14T14:14:23.879853Z","shell.execute_reply.started":"2023-12-14T14:14:23.862158Z"},"trusted":true},"outputs":[],"source":["\n","\n","def weight_init(m):\n","    '''\n","    Initializes a model's parameters.\n","    Credits to: https://gist.github.com/jeasinema\n","\n","    Usage:\n","        model = Model()\n","        model.apply(weight_init)\n","    '''\n","    if isinstance(m, nn.Conv1d):\n","        init.normal_(m.weight.data)\n","        if m.bias is not None:\n","            init.normal_(m.bias.data)\n","    elif isinstance(m, nn.Conv2d):\n","        init.xavier_normal_(m.weight.data)\n","        if m.bias is not None:\n","            init.normal_(m.bias.data)\n","    elif isinstance(m, nn.Conv3d):\n","        init.xavier_normal_(m.weight.data)\n","        if m.bias is not None:\n","            init.normal_(m.bias.data)\n","    elif isinstance(m, nn.ConvTranspose1d):\n","        init.normal_(m.weight.data)\n","        if m.bias is not None:\n","            init.normal_(m.bias.data)\n","    elif isinstance(m, nn.ConvTranspose2d):\n","        init.xavier_normal_(m.weight.data)\n","        if m.bias is not None:\n","            init.normal_(m.bias.data)\n","    elif isinstance(m, nn.ConvTranspose3d):\n","        init.xavier_normal_(m.weight.data)\n","        if m.bias is not None:\n","            init.normal_(m.bias.data)\n","    elif isinstance(m, nn.BatchNorm1d):\n","        init.normal_(m.weight.data, mean=0, std=1)\n","        init.constant_(m.bias.data, 0)\n","    elif isinstance(m, nn.BatchNorm2d):\n","        init.normal_(m.weight.data, mean=0, std=1)\n","        init.constant_(m.bias.data, 0)\n","    elif isinstance(m, nn.BatchNorm3d):\n","        init.normal_(m.weight.data, mean=0, std=1)\n","        init.constant_(m.bias.data, 0)\n","    elif isinstance(m, nn.Linear):\n","        init.xavier_normal_(m.weight.data)\n","        try:\n","            init.normal_(m.bias.data)\n","        except AttributeError:\n","            pass\n","    elif isinstance(m, nn.LSTM):\n","        for param in m.parameters():\n","            if len(param.shape) >= 2:\n","                init.orthogonal_(param.data)\n","            else:\n","                init.normal_(param.data)\n","    elif isinstance(m, nn.LSTMCell):\n","        for param in m.parameters():\n","            if len(param.shape) >= 2:\n","                init.orthogonal_(param.data)\n","            else:\n","                init.normal_(param.data)\n","    elif isinstance(m, nn.GRU):\n","        for param in m.parameters():\n","            if len(param.shape) >= 2:\n","                init.orthogonal_(param.data)\n","            else:\n","                init.normal_(param.data)\n","    elif isinstance(m, nn.GRUCell):\n","        for param in m.parameters():\n","            if len(param.shape) >= 2:\n","                init.orthogonal_(param.data)\n","            else:\n","                init.normal_(param.data)\n","                \n","                \n","def save_results(fold, metrics, conf_mat):\n","    with open(\n","        os.path.join(RES_DIR,f\"Fold_{fold}_test_metrics.json\"), \"w\" \n","    ) as outfile:\n","        json.dump(metrics, outfile, indent=4)\n","    pkl.dump(\n","        conf_mat,\n","        open(\n","            os.path.join(RES_DIR, f\"Fold_{fold}_conf_mat.pkl\"), \"wb\"\n","        ),\n","    )"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T14:14:23.882209Z","iopub.status.busy":"2023-12-14T14:14:23.881913Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading patch metadata . . .\n","Done.\n","Dataset ready.\n","Reading patch metadata . . .\n","Done.\n","Dataset ready.\n","Reading patch metadata . . .\n","Done.\n","Dataset ready.\n","Train 363, Val 120, Test 124\n","Model has 1553796 trainable params\n","UNet3D(\n","  (en3): Sequential(\n","    (0): Conv3d(10, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (pool_3): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (en4): Sequential(\n","    (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (pool_4): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (center_in): Sequential(\n","    (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (center_out): Sequential(\n","    (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): ConvTranspose3d(128, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n","  )\n","  (dc4): Sequential(\n","    (0): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (trans3): Sequential(\n","    (0): ConvTranspose3d(64, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n","    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (dc3): Sequential(\n","    (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (4): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (final): Conv3d(16, 20, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",")\n","EPOCH 1/100\n","Step [50/363], Loss: 1.9960, Acc : 43.52, mIoU 4.76\n","Step [100/363], Loss: 1.5052, Acc : 48.58, mIoU 5.82\n","Step [150/363], Loss: 1.1455, Acc : 50.52, mIoU 6.20\n","Step [200/363], Loss: 1.0565, Acc : 52.32, mIoU 7.10\n","Step [250/363], Loss: 0.9973, Acc : 54.12, mIoU 8.48\n","Step [300/363], Loss: 0.9709, Acc : 55.96, mIoU 9.79\n","Step [350/363], Loss: 0.7008, Acc : 57.60, mIoU 11.26\n","Epoch time : 345.5s\n","Validation . . . \n","Step [50/120], Loss: 0.8590, Acc : 69.51, mIoU 18.68\n","Step [100/120], Loss: 0.8263, Acc : 68.77, mIoU 18.95\n","Epoch time : 41.7s\n","Loss 1.0655,  Acc 68.76,  IoU 18.8914\n","EPOCH 2/100\n","Step [50/363], Loss: 1.1756, Acc : 69.46, mIoU 20.28\n","Step [100/363], Loss: 0.9653, Acc : 68.95, mIoU 21.33\n","Step [150/363], Loss: 0.8105, Acc : 69.42, mIoU 22.28\n","Step [200/363], Loss: 0.5789, Acc : 69.64, mIoU 22.54\n","Step [250/363], Loss: 0.8388, Acc : 69.98, mIoU 22.92\n","Step [300/363], Loss: 0.9345, Acc : 70.24, mIoU 23.60\n","Step [350/363], Loss: 1.0776, Acc : 70.50, mIoU 24.53\n","Epoch time : 341.6s\n","Validation . . . \n","Step [50/120], Loss: 1.0694, Acc : 69.71, mIoU 29.67\n","Step [100/120], Loss: 1.3060, Acc : 69.17, mIoU 29.64\n","Epoch time : 40.3s\n","Loss 1.0880,  Acc 68.93,  IoU 29.7191\n","EPOCH 3/100\n","Step [50/363], Loss: 1.1215, Acc : 71.51, mIoU 27.76\n","Step [100/363], Loss: 0.9542, Acc : 72.55, mIoU 28.77\n","Step [150/363], Loss: 1.0470, Acc : 73.05, mIoU 29.56\n","Step [200/363], Loss: 0.7358, Acc : 72.84, mIoU 29.67\n","Step [250/363], Loss: 0.5647, Acc : 72.77, mIoU 30.06\n","Step [300/363], Loss: 0.8885, Acc : 72.77, mIoU 30.47\n","Step [350/363], Loss: 1.0067, Acc : 72.93, mIoU 30.86\n","Epoch time : 338.7s\n","Validation . . . \n","Step [50/120], Loss: 0.6063, Acc : 75.91, mIoU 36.02\n","Step [100/120], Loss: 0.6056, Acc : 75.10, mIoU 35.19\n","Epoch time : 41.4s\n","Loss 1.3960,  Acc 75.14,  IoU 34.8681\n","EPOCH 4/100\n","Step [50/363], Loss: 0.7569, Acc : 74.41, mIoU 36.74\n","Step [100/363], Loss: 0.6383, Acc : 74.22, mIoU 35.47\n","Step [150/363], Loss: 0.4631, Acc : 74.88, mIoU 36.19\n","Step [200/363], Loss: 0.6998, Acc : 74.78, mIoU 36.13\n","Step [250/363], Loss: 0.4922, Acc : 74.72, mIoU 35.77\n","Step [300/363], Loss: 1.1110, Acc : 74.61, mIoU 35.65\n","Step [350/363], Loss: 0.4647, Acc : 74.64, mIoU 35.97\n","Epoch time : 336.7s\n","Validation . . . \n","Step [50/120], Loss: 0.3958, Acc : 74.82, mIoU 38.38\n","Step [100/120], Loss: 0.5663, Acc : 75.85, mIoU 39.09\n","Epoch time : 41.6s\n","Loss 0.9753,  Acc 75.93,  IoU 38.9595\n","EPOCH 5/100\n","Step [50/363], Loss: 0.7753, Acc : 75.57, mIoU 36.01\n","Step [100/363], Loss: 0.9383, Acc : 75.75, mIoU 36.95\n","Step [150/363], Loss: 0.6901, Acc : 75.73, mIoU 37.76\n","Step [200/363], Loss: 0.7965, Acc : 75.89, mIoU 38.49\n","Step [250/363], Loss: 0.9482, Acc : 75.86, mIoU 38.48\n","Step [300/363], Loss: 0.6045, Acc : 75.70, mIoU 38.55\n","Step [350/363], Loss: 1.0357, Acc : 75.57, mIoU 38.52\n","Epoch time : 339.7s\n","Validation . . . \n","Step [50/120], Loss: 0.7751, Acc : 76.14, mIoU 38.30\n","Step [100/120], Loss: 0.5224, Acc : 76.38, mIoU 39.75\n","Epoch time : 40.9s\n","Loss 0.7465,  Acc 76.82,  IoU 40.3872\n","EPOCH 6/100\n","Step [50/363], Loss: 1.0926, Acc : 77.97, mIoU 40.72\n","Step [100/363], Loss: 0.5544, Acc : 76.89, mIoU 40.55\n","Step [150/363], Loss: 0.8133, Acc : 76.20, mIoU 40.71\n","Step [200/363], Loss: 0.5656, Acc : 76.27, mIoU 40.71\n","Step [250/363], Loss: 0.7031, Acc : 76.25, mIoU 40.96\n","Step [300/363], Loss: 0.8342, Acc : 76.39, mIoU 41.08\n","Step [350/363], Loss: 0.6036, Acc : 76.56, mIoU 41.55\n","Epoch time : 337.0s\n","Validation . . . \n","Step [50/120], Loss: 0.7202, Acc : 77.38, mIoU 41.66\n","Step [100/120], Loss: 0.6065, Acc : 76.35, mIoU 40.50\n","Epoch time : 39.6s\n","Loss 0.7292,  Acc 76.16,  IoU 40.3178\n","EPOCH 7/100\n","Step [50/363], Loss: 0.7035, Acc : 76.46, mIoU 42.34\n","Step [100/363], Loss: 0.7608, Acc : 76.74, mIoU 41.51\n","Step [150/363], Loss: 0.9682, Acc : 77.03, mIoU 42.54\n","Step [200/363], Loss: 0.5055, Acc : 76.95, mIoU 42.49\n","Step [250/363], Loss: 0.4721, Acc : 77.01, mIoU 42.82\n","Step [300/363], Loss: 0.5925, Acc : 77.00, mIoU 42.86\n","Step [350/363], Loss: 0.7436, Acc : 77.15, mIoU 42.97\n","Epoch time : 324.0s\n","Validation . . . \n","Step [50/120], Loss: 0.7117, Acc : 76.79, mIoU 44.07\n","Step [100/120], Loss: 0.5533, Acc : 77.86, mIoU 44.29\n","Epoch time : 39.5s\n","Loss 0.5112,  Acc 78.14,  IoU 44.6765\n","EPOCH 8/100\n","Step [50/363], Loss: 0.6237, Acc : 77.99, mIoU 44.14\n","Step [100/363], Loss: 0.5256, Acc : 78.47, mIoU 44.62\n","Step [150/363], Loss: 0.7085, Acc : 78.20, mIoU 45.29\n","Step [200/363], Loss: 0.5535, Acc : 78.11, mIoU 45.38\n","Step [250/363], Loss: 0.7177, Acc : 78.02, mIoU 45.46\n","Step [300/363], Loss: 0.6718, Acc : 78.21, mIoU 45.71\n","Step [350/363], Loss: 0.8140, Acc : 78.19, mIoU 45.59\n","Epoch time : 321.6s\n","Validation . . . \n","Step [50/120], Loss: 0.4795, Acc : 78.87, mIoU 47.72\n","Step [100/120], Loss: 0.5075, Acc : 79.36, mIoU 48.51\n","Epoch time : 39.6s\n","Loss 0.7764,  Acc 79.25,  IoU 48.5078\n","EPOCH 9/100\n","Step [50/363], Loss: 0.8657, Acc : 79.05, mIoU 46.19\n","Step [100/363], Loss: 0.7367, Acc : 78.07, mIoU 46.79\n","Step [150/363], Loss: 0.5627, Acc : 78.25, mIoU 46.87\n","Step [200/363], Loss: 0.5518, Acc : 78.60, mIoU 47.43\n","Step [250/363], Loss: 0.6007, Acc : 78.37, mIoU 47.12\n","Step [300/363], Loss: 0.5645, Acc : 78.38, mIoU 47.05\n","Step [350/363], Loss: 0.6168, Acc : 78.39, mIoU 47.15\n","Epoch time : 321.9s\n","Validation . . . \n","Step [50/120], Loss: 0.5685, Acc : 77.92, mIoU 42.41\n","Step [100/120], Loss: 0.5588, Acc : 78.69, mIoU 42.95\n","Epoch time : 39.2s\n","Loss 0.7284,  Acc 78.37,  IoU 43.7311\n","EPOCH 10/100\n","Step [50/363], Loss: 0.4974, Acc : 78.68, mIoU 46.37\n","Step [100/363], Loss: 0.7246, Acc : 78.97, mIoU 46.17\n","Step [150/363], Loss: 0.7232, Acc : 78.40, mIoU 47.18\n","Step [200/363], Loss: 0.5081, Acc : 78.44, mIoU 47.38\n","Step [250/363], Loss: 0.6575, Acc : 78.57, mIoU 47.88\n","Step [300/363], Loss: 0.6706, Acc : 78.55, mIoU 47.76\n","Step [350/363], Loss: 0.6518, Acc : 78.51, mIoU 47.79\n","Epoch time : 321.7s\n","Validation . . . \n","Step [50/120], Loss: 0.6380, Acc : 78.05, mIoU 44.61\n","Step [100/120], Loss: 0.4799, Acc : 77.80, mIoU 44.72\n","Epoch time : 39.9s\n","Loss 0.4946,  Acc 78.01,  IoU 44.6555\n","EPOCH 11/100\n","Step [50/363], Loss: 0.8241, Acc : 78.89, mIoU 45.30\n","Step [100/363], Loss: 0.5419, Acc : 79.44, mIoU 47.27\n","Step [150/363], Loss: 0.7175, Acc : 79.54, mIoU 48.26\n","Step [200/363], Loss: 0.6594, Acc : 79.54, mIoU 48.38\n","Step [250/363], Loss: 0.5170, Acc : 79.27, mIoU 48.36\n","Step [300/363], Loss: 0.5756, Acc : 79.35, mIoU 49.02\n","Step [350/363], Loss: 0.9026, Acc : 78.99, mIoU 48.86\n","Epoch time : 323.0s\n","Validation . . . \n","Step [50/120], Loss: 0.6824, Acc : 79.11, mIoU 49.07\n","Step [100/120], Loss: 0.8905, Acc : 79.91, mIoU 49.53\n","Epoch time : 39.4s\n","Loss 0.6196,  Acc 80.04,  IoU 49.7560\n","EPOCH 12/100\n","Step [50/363], Loss: 0.5601, Acc : 80.06, mIoU 50.61\n","Step [100/363], Loss: 0.4710, Acc : 80.09, mIoU 51.18\n","Step [150/363], Loss: 0.7984, Acc : 79.93, mIoU 50.32\n","Step [200/363], Loss: 0.7117, Acc : 79.78, mIoU 50.53\n","Step [250/363], Loss: 0.3994, Acc : 79.73, mIoU 50.44\n","Step [300/363], Loss: 0.4329, Acc : 79.59, mIoU 50.06\n","Step [350/363], Loss: 0.7417, Acc : 79.41, mIoU 49.93\n","Epoch time : 323.3s\n","Validation . . . \n","Step [50/120], Loss: 0.8519, Acc : 78.24, mIoU 44.92\n","Step [100/120], Loss: 1.2384, Acc : 78.48, mIoU 46.37\n","Epoch time : 39.7s\n","Loss 0.4844,  Acc 78.56,  IoU 46.9836\n","EPOCH 13/100\n","Step [50/363], Loss: 0.7927, Acc : 78.23, mIoU 49.64\n","Step [100/363], Loss: 0.5668, Acc : 79.05, mIoU 50.11\n","Step [150/363], Loss: 0.5000, Acc : 79.64, mIoU 51.23\n","Step [200/363], Loss: 0.5333, Acc : 79.31, mIoU 50.97\n","Step [250/363], Loss: 0.5003, Acc : 79.41, mIoU 50.93\n","Step [300/363], Loss: 0.4099, Acc : 79.66, mIoU 50.92\n","Step [350/363], Loss: 0.6205, Acc : 79.77, mIoU 51.16\n","Epoch time : 323.5s\n","Validation . . . \n","Step [50/120], Loss: 0.5716, Acc : 80.36, mIoU 51.87\n","Step [100/120], Loss: 0.7090, Acc : 80.21, mIoU 51.45\n","Epoch time : 37.8s\n","Loss 0.4682,  Acc 80.08,  IoU 51.6010\n","EPOCH 14/100\n","Step [50/363], Loss: 0.4931, Acc : 80.12, mIoU 49.52\n","Step [100/363], Loss: 0.6177, Acc : 79.89, mIoU 50.02\n","Step [150/363], Loss: 0.5927, Acc : 80.06, mIoU 51.19\n","Step [200/363], Loss: 0.5165, Acc : 80.04, mIoU 51.43\n","Step [250/363], Loss: 0.4678, Acc : 79.70, mIoU 51.02\n","Step [300/363], Loss: 0.4821, Acc : 79.72, mIoU 51.39\n","Step [350/363], Loss: 0.6579, Acc : 79.86, mIoU 51.84\n","Epoch time : 314.5s\n","Validation . . . \n","Step [50/120], Loss: 0.5233, Acc : 79.06, mIoU 49.84\n","Step [100/120], Loss: 0.5705, Acc : 79.01, mIoU 49.17\n","Epoch time : 40.0s\n","Loss 0.5330,  Acc 78.91,  IoU 49.1305\n","EPOCH 15/100\n","Step [50/363], Loss: 0.7202, Acc : 80.55, mIoU 52.96\n","Step [100/363], Loss: 0.5427, Acc : 79.98, mIoU 52.31\n","Step [150/363], Loss: 0.4022, Acc : 80.29, mIoU 52.99\n","Step [200/363], Loss: 0.6506, Acc : 80.26, mIoU 52.87\n","Step [250/363], Loss: 0.4454, Acc : 80.41, mIoU 53.42\n","Step [300/363], Loss: 0.5992, Acc : 80.35, mIoU 53.35\n","Step [350/363], Loss: 0.5381, Acc : 80.27, mIoU 53.44\n","Epoch time : 315.7s\n","Validation . . . \n","Step [50/120], Loss: 0.4941, Acc : 78.29, mIoU 51.39\n","Step [100/120], Loss: 0.5165, Acc : 78.89, mIoU 50.91\n","Epoch time : 37.6s\n","Loss 0.7064,  Acc 78.94,  IoU 51.4307\n","EPOCH 16/100\n","Step [50/363], Loss: 0.6912, Acc : 80.44, mIoU 53.03\n","Step [100/363], Loss: 0.5440, Acc : 80.81, mIoU 53.70\n","Step [150/363], Loss: 0.9244, Acc : 80.56, mIoU 53.66\n","Step [200/363], Loss: 0.3628, Acc : 80.78, mIoU 53.94\n","Step [250/363], Loss: 0.4014, Acc : 80.42, mIoU 53.89\n","Step [300/363], Loss: 0.4671, Acc : 80.51, mIoU 53.71\n","Step [350/363], Loss: 0.5434, Acc : 80.49, mIoU 53.78\n","Epoch time : 314.5s\n","Validation . . . \n","Step [50/120], Loss: 0.3517, Acc : 79.98, mIoU 50.36\n","Step [100/120], Loss: 0.7454, Acc : 79.54, mIoU 49.53\n","Epoch time : 38.4s\n","Loss 0.4842,  Acc 79.93,  IoU 49.9941\n","EPOCH 17/100\n","Step [50/363], Loss: 0.4530, Acc : 80.55, mIoU 54.75\n","Step [100/363], Loss: 0.4891, Acc : 80.76, mIoU 54.64\n","Step [150/363], Loss: 0.4110, Acc : 80.54, mIoU 54.03\n","Step [200/363], Loss: 0.4992, Acc : 80.37, mIoU 53.82\n","Step [250/363], Loss: 0.3408, Acc : 80.61, mIoU 54.33\n","Step [300/363], Loss: 0.4825, Acc : 80.71, mIoU 54.30\n","Step [350/363], Loss: 0.4289, Acc : 80.69, mIoU 54.27\n","Epoch time : 312.5s\n","Validation . . . \n","Step [50/120], Loss: 0.8531, Acc : 81.03, mIoU 52.57\n","Step [100/120], Loss: 0.5337, Acc : 80.72, mIoU 51.18\n","Epoch time : 38.2s\n","Loss 0.3947,  Acc 80.62,  IoU 52.0063\n","EPOCH 18/100\n","Step [50/363], Loss: 0.3370, Acc : 81.12, mIoU 53.27\n","Step [100/363], Loss: 0.8807, Acc : 81.24, mIoU 55.43\n","Step [150/363], Loss: 0.3985, Acc : 81.28, mIoU 55.79\n","Step [200/363], Loss: 0.4867, Acc : 81.42, mIoU 56.07\n","Step [250/363], Loss: 0.8384, Acc : 81.32, mIoU 55.58\n","Step [300/363], Loss: 0.5592, Acc : 81.12, mIoU 55.16\n","Step [350/363], Loss: 0.5602, Acc : 81.17, mIoU 55.48\n","Epoch time : 309.2s\n","Validation . . . \n","Step [50/120], Loss: 0.5101, Acc : 80.85, mIoU 51.96\n","Step [100/120], Loss: 0.3761, Acc : 80.52, mIoU 54.08\n","Epoch time : 38.5s\n","Loss 0.5615,  Acc 80.67,  IoU 54.4071\n","EPOCH 19/100\n","Step [50/363], Loss: 0.6377, Acc : 80.23, mIoU 55.60\n","Step [100/363], Loss: 0.6309, Acc : 80.62, mIoU 55.19\n","Step [150/363], Loss: 0.3503, Acc : 80.93, mIoU 55.78\n","Step [200/363], Loss: 0.4465, Acc : 80.80, mIoU 56.13\n","Step [250/363], Loss: 0.4806, Acc : 80.99, mIoU 56.28\n","Step [300/363], Loss: 0.4549, Acc : 81.08, mIoU 56.25\n","Step [350/363], Loss: 0.8232, Acc : 81.10, mIoU 56.19\n","Epoch time : 309.1s\n","Validation . . . \n","Step [50/120], Loss: 0.6520, Acc : 80.49, mIoU 52.10\n","Step [100/120], Loss: 0.7980, Acc : 80.88, mIoU 53.44\n","Epoch time : 38.3s\n","Loss 0.5802,  Acc 81.08,  IoU 53.6362\n","EPOCH 20/100\n","Step [50/363], Loss: 0.4249, Acc : 83.02, mIoU 58.19\n","Step [100/363], Loss: 0.3938, Acc : 82.36, mIoU 57.47\n","Step [150/363], Loss: 0.6197, Acc : 81.94, mIoU 57.37\n","Step [200/363], Loss: 0.5098, Acc : 81.74, mIoU 58.00\n","Step [250/363], Loss: 0.4007, Acc : 81.70, mIoU 57.56\n","Step [300/363], Loss: 0.6842, Acc : 81.63, mIoU 57.23\n","Step [350/363], Loss: 0.4796, Acc : 81.52, mIoU 56.74\n","Epoch time : 320.9s\n","Validation . . . \n","Step [50/120], Loss: 0.5459, Acc : 80.70, mIoU 51.51\n","Step [100/120], Loss: 0.4118, Acc : 80.62, mIoU 52.69\n","Epoch time : 40.5s\n","Loss 0.6192,  Acc 80.83,  IoU 52.7177\n","EPOCH 21/100\n","Step [50/363], Loss: 0.4677, Acc : 82.24, mIoU 56.10\n","Step [100/363], Loss: 0.5771, Acc : 82.02, mIoU 57.02\n","Step [150/363], Loss: 0.3202, Acc : 82.19, mIoU 57.47\n","Step [200/363], Loss: 0.4853, Acc : 81.96, mIoU 57.46\n","Step [250/363], Loss: 0.5768, Acc : 82.26, mIoU 58.13\n","Step [300/363], Loss: 0.6220, Acc : 81.92, mIoU 58.25\n","Step [350/363], Loss: 0.5029, Acc : 81.71, mIoU 58.03\n","Epoch time : 337.0s\n","Validation . . . \n","Step [50/120], Loss: 0.7010, Acc : 81.04, mIoU 53.60\n","Step [100/120], Loss: 0.4596, Acc : 81.33, mIoU 55.15\n","Epoch time : 40.6s\n","Loss 0.4671,  Acc 81.38,  IoU 55.2045\n","EPOCH 22/100\n","Step [50/363], Loss: 0.4487, Acc : 81.99, mIoU 58.15\n","Step [100/363], Loss: 0.4841, Acc : 81.67, mIoU 57.28\n","Step [150/363], Loss: 0.4379, Acc : 81.65, mIoU 58.13\n","Step [200/363], Loss: 0.4583, Acc : 81.79, mIoU 57.95\n","Step [250/363], Loss: 0.6132, Acc : 81.70, mIoU 58.04\n","Step [300/363], Loss: 0.4565, Acc : 81.71, mIoU 58.07\n","Step [350/363], Loss: 0.4591, Acc : 81.84, mIoU 58.18\n","Epoch time : 338.0s\n","Validation . . . \n","Step [50/120], Loss: 0.4458, Acc : 81.79, mIoU 55.33\n","Step [100/120], Loss: 0.5692, Acc : 81.25, mIoU 55.20\n","Epoch time : 40.9s\n","Loss 0.3435,  Acc 81.12,  IoU 55.2458\n","EPOCH 23/100\n","Step [50/363], Loss: 0.2784, Acc : 82.91, mIoU 59.12\n","Step [100/363], Loss: 0.4658, Acc : 82.29, mIoU 58.84\n","Step [150/363], Loss: 0.8202, Acc : 82.03, mIoU 58.94\n","Step [200/363], Loss: 0.5891, Acc : 81.92, mIoU 58.69\n","Step [250/363], Loss: 0.4668, Acc : 81.98, mIoU 58.47\n","Step [300/363], Loss: 0.7112, Acc : 81.96, mIoU 58.87\n","Step [350/363], Loss: 0.5578, Acc : 81.98, mIoU 58.68\n","Epoch time : 343.1s\n","Validation . . . \n","Step [50/120], Loss: 0.5864, Acc : 81.64, mIoU 56.04\n","Step [100/120], Loss: 0.5243, Acc : 80.94, mIoU 54.87\n","Epoch time : 40.6s\n","Loss 0.4457,  Acc 80.85,  IoU 55.6050\n","EPOCH 24/100\n","Step [50/363], Loss: 0.6278, Acc : 82.78, mIoU 60.89\n","Step [100/363], Loss: 0.5062, Acc : 82.60, mIoU 60.15\n","Step [150/363], Loss: 0.5204, Acc : 82.45, mIoU 59.42\n","Step [200/363], Loss: 0.4647, Acc : 82.34, mIoU 59.68\n","Step [250/363], Loss: 0.3863, Acc : 82.43, mIoU 59.67\n","Step [300/363], Loss: 0.3567, Acc : 82.08, mIoU 59.72\n","Step [350/363], Loss: 0.4886, Acc : 82.16, mIoU 59.67\n","Epoch time : 339.5s\n","Validation . . . \n","Step [50/120], Loss: 0.5460, Acc : 81.58, mIoU 56.34\n","Step [100/120], Loss: 0.6937, Acc : 81.30, mIoU 56.30\n","Epoch time : 41.7s\n","Loss 0.5534,  Acc 81.35,  IoU 56.6634\n","EPOCH 25/100\n","Step [50/363], Loss: 0.3279, Acc : 82.76, mIoU 58.07\n","Step [100/363], Loss: 0.5038, Acc : 82.34, mIoU 59.12\n","Step [150/363], Loss: 0.6650, Acc : 82.39, mIoU 59.94\n","Step [200/363], Loss: 0.3943, Acc : 82.58, mIoU 60.31\n","Step [250/363], Loss: 0.4466, Acc : 82.55, mIoU 60.18\n","Step [300/363], Loss: 0.5540, Acc : 82.48, mIoU 60.24\n","Step [350/363], Loss: 0.3996, Acc : 82.47, mIoU 60.14\n","Epoch time : 342.5s\n","Validation . . . \n","Step [50/120], Loss: 0.7000, Acc : 81.44, mIoU 55.79\n","Step [100/120], Loss: 0.5819, Acc : 81.44, mIoU 56.24\n","Epoch time : 42.0s\n","Loss 0.4156,  Acc 81.46,  IoU 56.0308\n","EPOCH 26/100\n","Step [50/363], Loss: 0.4530, Acc : 82.77, mIoU 60.32\n","Step [100/363], Loss: 0.4363, Acc : 82.17, mIoU 61.01\n","Step [150/363], Loss: 0.3715, Acc : 82.45, mIoU 60.63\n","Step [200/363], Loss: 0.4157, Acc : 82.47, mIoU 60.48\n","Step [250/363], Loss: 0.3551, Acc : 82.42, mIoU 60.45\n","Step [300/363], Loss: 0.3592, Acc : 82.37, mIoU 60.39\n","Step [350/363], Loss: 0.5559, Acc : 82.39, mIoU 60.57\n","Epoch time : 341.5s\n","Validation . . . \n","Step [50/120], Loss: 0.6173, Acc : 82.18, mIoU 57.48\n","Step [100/120], Loss: 0.6161, Acc : 81.69, mIoU 57.63\n","Epoch time : 40.9s\n","Loss 0.3409,  Acc 81.52,  IoU 57.1916\n","EPOCH 27/100\n","Step [50/363], Loss: 0.4613, Acc : 83.52, mIoU 61.90\n","Step [100/363], Loss: 0.2853, Acc : 83.38, mIoU 60.96\n","Step [150/363], Loss: 0.5682, Acc : 83.27, mIoU 60.46\n","Step [200/363], Loss: 0.4735, Acc : 83.36, mIoU 61.50\n","Step [250/363], Loss: 0.4961, Acc : 82.98, mIoU 61.39\n","Step [300/363], Loss: 0.4902, Acc : 82.79, mIoU 61.24\n","Step [350/363], Loss: 0.4281, Acc : 82.67, mIoU 60.98\n","Epoch time : 339.8s\n","Validation . . . \n","Step [50/120], Loss: 0.5678, Acc : 81.82, mIoU 59.28\n","Step [100/120], Loss: 1.0357, Acc : 81.54, mIoU 57.89\n","Epoch time : 40.7s\n","Loss 0.7885,  Acc 81.32,  IoU 57.4288\n","EPOCH 28/100\n","Step [50/363], Loss: 0.3483, Acc : 83.42, mIoU 61.61\n","Step [100/363], Loss: 0.3904, Acc : 83.12, mIoU 61.32\n","Step [150/363], Loss: 0.5045, Acc : 83.37, mIoU 61.06\n","Step [200/363], Loss: 0.2873, Acc : 83.12, mIoU 61.18\n","Step [250/363], Loss: 0.6135, Acc : 83.13, mIoU 61.50\n","Step [300/363], Loss: 0.6897, Acc : 83.07, mIoU 61.91\n","Step [350/363], Loss: 0.6398, Acc : 82.83, mIoU 61.34\n","Epoch time : 344.0s\n","Validation . . . \n","Step [50/120], Loss: 0.4167, Acc : 80.93, mIoU 55.23\n","Step [100/120], Loss: 0.5050, Acc : 81.68, mIoU 56.18\n","Epoch time : 41.5s\n","Loss 0.6543,  Acc 81.41,  IoU 56.1382\n","EPOCH 29/100\n","Step [50/363], Loss: 0.5120, Acc : 82.17, mIoU 61.14\n","Step [100/363], Loss: 0.4751, Acc : 82.69, mIoU 60.76\n","Step [150/363], Loss: 0.3793, Acc : 83.01, mIoU 62.14\n","Step [200/363], Loss: 0.2281, Acc : 83.23, mIoU 62.03\n","Step [250/363], Loss: 0.5224, Acc : 83.09, mIoU 61.94\n","Step [300/363], Loss: 0.4514, Acc : 82.98, mIoU 62.15\n","Step [350/363], Loss: 0.4368, Acc : 82.94, mIoU 62.11\n","Epoch time : 341.8s\n","Validation . . . \n","Step [50/120], Loss: 0.4460, Acc : 79.60, mIoU 53.07\n","Step [100/120], Loss: 0.4223, Acc : 80.80, mIoU 53.91\n","Epoch time : 41.7s\n","Loss 0.4327,  Acc 81.12,  IoU 55.0005\n","EPOCH 30/100\n","Step [50/363], Loss: 0.4068, Acc : 82.48, mIoU 60.76\n","Step [100/363], Loss: 0.3966, Acc : 83.19, mIoU 62.91\n","Step [150/363], Loss: 0.9783, Acc : 83.32, mIoU 63.20\n","Step [200/363], Loss: 0.5313, Acc : 83.35, mIoU 62.70\n","Step [250/363], Loss: 0.4888, Acc : 83.25, mIoU 62.63\n","Step [300/363], Loss: 0.4254, Acc : 83.45, mIoU 62.73\n","Step [350/363], Loss: 0.4908, Acc : 83.22, mIoU 62.83\n","Epoch time : 340.6s\n","Validation . . . \n","Step [50/120], Loss: 0.3631, Acc : 81.40, mIoU 59.08\n","Step [100/120], Loss: 0.6423, Acc : 81.25, mIoU 57.93\n","Epoch time : 40.5s\n","Loss 0.5616,  Acc 81.23,  IoU 57.5700\n","EPOCH 31/100\n","Step [50/363], Loss: 0.3429, Acc : 83.43, mIoU 63.89\n","Step [100/363], Loss: 0.3655, Acc : 83.69, mIoU 63.79\n","Step [150/363], Loss: 0.7179, Acc : 83.43, mIoU 64.20\n","Step [200/363], Loss: 0.4532, Acc : 83.33, mIoU 63.91\n","Step [250/363], Loss: 0.3773, Acc : 83.35, mIoU 63.68\n","Step [300/363], Loss: 0.5795, Acc : 83.39, mIoU 63.64\n","Step [350/363], Loss: 0.4946, Acc : 83.32, mIoU 63.52\n","Epoch time : 334.4s\n","Validation . . . \n","Step [50/120], Loss: 0.3749, Acc : 81.76, mIoU 57.48\n","Step [100/120], Loss: 0.4525, Acc : 81.78, mIoU 58.25\n","Epoch time : 40.8s\n","Loss 0.4100,  Acc 82.09,  IoU 58.3535\n","EPOCH 32/100\n","Step [50/363], Loss: 0.3916, Acc : 84.05, mIoU 61.09\n","Step [100/363], Loss: 0.3160, Acc : 83.94, mIoU 63.51\n","Step [150/363], Loss: 0.4896, Acc : 83.84, mIoU 63.84\n","Step [200/363], Loss: 0.4292, Acc : 83.81, mIoU 64.31\n","Step [250/363], Loss: 0.5380, Acc : 83.61, mIoU 64.25\n","Step [300/363], Loss: 0.5399, Acc : 83.50, mIoU 63.88\n","Step [350/363], Loss: 0.5864, Acc : 83.50, mIoU 63.89\n","Epoch time : 337.9s\n","Validation . . . \n","Step [50/120], Loss: 0.6475, Acc : 81.92, mIoU 56.15\n","Step [100/120], Loss: 0.4960, Acc : 82.00, mIoU 56.65\n","Epoch time : 38.5s\n","Loss 0.5276,  Acc 81.67,  IoU 56.3930\n","EPOCH 33/100\n","Step [50/363], Loss: 0.3937, Acc : 84.59, mIoU 63.30\n","Step [100/363], Loss: 0.4587, Acc : 84.65, mIoU 64.12\n","Step [150/363], Loss: 0.4903, Acc : 84.54, mIoU 64.54\n","Step [200/363], Loss: 0.3927, Acc : 84.34, mIoU 64.08\n","Step [250/363], Loss: 0.5439, Acc : 83.97, mIoU 63.70\n","Step [300/363], Loss: 0.4932, Acc : 83.82, mIoU 63.80\n","Step [350/363], Loss: 0.6822, Acc : 83.70, mIoU 63.85\n","Epoch time : 337.8s\n","Validation . . . \n","Step [50/120], Loss: 0.5582, Acc : 80.77, mIoU 57.21\n","Step [100/120], Loss: 0.7162, Acc : 81.62, mIoU 56.95\n","Epoch time : 40.2s\n","Loss 0.6946,  Acc 81.67,  IoU 57.6833\n","EPOCH 34/100\n","Step [50/363], Loss: 0.4340, Acc : 84.20, mIoU 65.84\n","Step [100/363], Loss: 0.3393, Acc : 84.25, mIoU 66.02\n","Step [150/363], Loss: 0.4080, Acc : 84.29, mIoU 66.50\n","Step [200/363], Loss: 0.4958, Acc : 84.32, mIoU 65.69\n","Step [250/363], Loss: 0.6099, Acc : 84.34, mIoU 65.75\n","Step [300/363], Loss: 0.4116, Acc : 84.26, mIoU 65.58\n","Step [350/363], Loss: 0.4663, Acc : 84.10, mIoU 65.52\n","Epoch time : 338.1s\n","Validation . . . \n","Step [50/120], Loss: 0.4766, Acc : 81.36, mIoU 55.09\n","Step [100/120], Loss: 0.5074, Acc : 81.28, mIoU 56.92\n","Epoch time : 39.0s\n","Loss 0.3010,  Acc 81.49,  IoU 57.6420\n","EPOCH 35/100\n","Step [50/363], Loss: 0.2981, Acc : 84.30, mIoU 64.64\n","Step [100/363], Loss: 0.3498, Acc : 84.19, mIoU 64.89\n","Step [150/363], Loss: 0.5449, Acc : 84.29, mIoU 65.56\n","Step [200/363], Loss: 0.5834, Acc : 84.23, mIoU 65.00\n","Step [250/363], Loss: 0.4501, Acc : 84.09, mIoU 65.00\n","Step [300/363], Loss: 0.3455, Acc : 84.18, mIoU 65.29\n","Step [350/363], Loss: 0.5399, Acc : 83.97, mIoU 64.86\n","Epoch time : 341.2s\n","Validation . . . \n","Step [50/120], Loss: 0.5371, Acc : 81.49, mIoU 55.36\n","Step [100/120], Loss: 0.6924, Acc : 81.88, mIoU 57.61\n","Epoch time : 42.0s\n","Loss 0.7944,  Acc 81.78,  IoU 57.6309\n","EPOCH 36/100\n","Step [50/363], Loss: 0.4223, Acc : 84.30, mIoU 67.52\n","Step [100/363], Loss: 0.4779, Acc : 84.96, mIoU 68.90\n","Step [150/363], Loss: 0.3907, Acc : 84.50, mIoU 67.98\n","Step [200/363], Loss: 0.4040, Acc : 84.28, mIoU 67.50\n","Step [250/363], Loss: 0.5105, Acc : 84.23, mIoU 66.56\n","Step [300/363], Loss: 0.3174, Acc : 84.30, mIoU 66.42\n","Step [350/363], Loss: 0.4034, Acc : 84.34, mIoU 66.24\n","Epoch time : 339.5s\n","Validation . . . \n","Step [50/120], Loss: 0.3818, Acc : 81.66, mIoU 56.93\n","Step [100/120], Loss: 0.5706, Acc : 80.54, mIoU 56.81\n","Epoch time : 41.9s\n","Loss 0.4868,  Acc 80.80,  IoU 57.1839\n","EPOCH 37/100\n","Step [50/363], Loss: 0.5304, Acc : 84.11, mIoU 65.59\n","Step [100/363], Loss: 0.3294, Acc : 84.14, mIoU 64.59\n","Step [150/363], Loss: 0.4537, Acc : 84.15, mIoU 65.53\n","Step [200/363], Loss: 0.2830, Acc : 84.46, mIoU 66.37\n","Step [250/363], Loss: 0.3799, Acc : 84.51, mIoU 66.42\n","Step [300/363], Loss: 0.3698, Acc : 84.35, mIoU 66.18\n","Step [350/363], Loss: 0.4465, Acc : 84.39, mIoU 65.95\n","Epoch time : 341.0s\n","Validation . . . \n","Step [50/120], Loss: 0.6180, Acc : 80.63, mIoU 57.12\n","Step [100/120], Loss: 0.5818, Acc : 80.87, mIoU 58.60\n","Epoch time : 40.5s\n","Loss 0.6318,  Acc 80.93,  IoU 58.5197\n","EPOCH 38/100\n","Step [50/363], Loss: 0.3530, Acc : 85.50, mIoU 68.27\n","Step [100/363], Loss: 0.5111, Acc : 85.21, mIoU 68.05\n","Step [150/363], Loss: 0.5230, Acc : 84.97, mIoU 67.58\n","Step [200/363], Loss: 0.4182, Acc : 84.92, mIoU 67.89\n","Step [250/363], Loss: 0.2951, Acc : 84.78, mIoU 67.70\n","Step [300/363], Loss: 0.3879, Acc : 84.87, mIoU 67.49\n","Step [350/363], Loss: 0.3888, Acc : 84.70, mIoU 67.30\n","Epoch time : 338.8s\n","Validation . . . \n","Step [50/120], Loss: 0.4115, Acc : 82.19, mIoU 57.19\n","Step [100/120], Loss: 0.5525, Acc : 82.08, mIoU 57.54\n","Epoch time : 41.3s\n","Loss 0.6150,  Acc 82.05,  IoU 57.2568\n","EPOCH 39/100\n","Step [50/363], Loss: 0.6333, Acc : 84.52, mIoU 67.10\n","Step [100/363], Loss: 0.5071, Acc : 85.13, mIoU 67.60\n","Step [150/363], Loss: 0.5466, Acc : 84.76, mIoU 67.62\n","Step [200/363], Loss: 0.3816, Acc : 84.58, mIoU 67.15\n","Step [250/363], Loss: 0.5296, Acc : 84.55, mIoU 67.22\n","Step [300/363], Loss: 0.4373, Acc : 84.67, mIoU 67.15\n","Step [350/363], Loss: 0.4259, Acc : 84.68, mIoU 67.19\n","Epoch time : 341.1s\n","Validation . . . \n","Step [50/120], Loss: 0.5068, Acc : 81.86, mIoU 58.63\n","Step [100/120], Loss: 0.3781, Acc : 81.63, mIoU 57.85\n","Epoch time : 40.2s\n","Loss 0.4993,  Acc 81.82,  IoU 57.7443\n","EPOCH 40/100\n","Step [50/363], Loss: 0.5166, Acc : 82.92, mIoU 63.63\n","Step [100/363], Loss: 0.4482, Acc : 83.70, mIoU 64.28\n","Step [150/363], Loss: 0.4598, Acc : 83.98, mIoU 64.90\n","Step [200/363], Loss: 0.3590, Acc : 84.24, mIoU 65.99\n","Step [250/363], Loss: 0.3820, Acc : 84.39, mIoU 66.62\n","Step [300/363], Loss: 0.4457, Acc : 84.60, mIoU 66.86\n","Step [350/363], Loss: 0.5499, Acc : 84.64, mIoU 66.87\n","Epoch time : 338.9s\n","Validation . . . \n","Step [50/120], Loss: 0.3897, Acc : 81.82, mIoU 57.79\n","Step [100/120], Loss: 0.5941, Acc : 81.97, mIoU 58.89\n","Epoch time : 41.9s\n","Loss 0.5129,  Acc 81.56,  IoU 58.0140\n","EPOCH 41/100\n","Step [50/363], Loss: 0.2427, Acc : 85.34, mIoU 67.80\n","Step [100/363], Loss: 0.3255, Acc : 85.19, mIoU 68.09\n","Step [150/363], Loss: 0.4937, Acc : 85.29, mIoU 68.83\n","Step [200/363], Loss: 0.4715, Acc : 85.40, mIoU 68.91\n","Step [250/363], Loss: 0.3796, Acc : 85.19, mIoU 68.96\n","Step [300/363], Loss: 0.4411, Acc : 85.19, mIoU 69.13\n","Step [350/363], Loss: 0.4252, Acc : 85.18, mIoU 68.78\n","Epoch time : 343.0s\n","Validation . . . \n","Step [50/120], Loss: 0.6618, Acc : 81.52, mIoU 56.76\n","Step [100/120], Loss: 0.5856, Acc : 81.30, mIoU 57.64\n","Epoch time : 40.3s\n","Loss 0.6043,  Acc 81.44,  IoU 57.4575\n","EPOCH 42/100\n","Step [50/363], Loss: 0.3789, Acc : 85.41, mIoU 66.61\n","Step [100/363], Loss: 0.3206, Acc : 85.45, mIoU 68.53\n","Step [150/363], Loss: 0.2651, Acc : 85.70, mIoU 68.73\n","Step [200/363], Loss: 0.2692, Acc : 85.64, mIoU 68.66\n","Step [250/363], Loss: 0.3289, Acc : 85.44, mIoU 68.79\n","Step [300/363], Loss: 0.4185, Acc : 85.30, mIoU 68.77\n","Step [350/363], Loss: 0.4578, Acc : 85.16, mIoU 68.63\n","Epoch time : 338.5s\n","Validation . . . \n","Step [50/120], Loss: 0.5335, Acc : 80.61, mIoU 56.39\n","Step [100/120], Loss: 0.5961, Acc : 81.00, mIoU 57.24\n","Epoch time : 41.3s\n","Loss 0.6238,  Acc 80.84,  IoU 57.4834\n","EPOCH 43/100\n","Step [50/363], Loss: 0.6026, Acc : 85.51, mIoU 69.08\n","Step [100/363], Loss: 0.3168, Acc : 85.58, mIoU 69.76\n","Step [150/363], Loss: 0.3842, Acc : 85.23, mIoU 69.29\n","Step [200/363], Loss: 0.2432, Acc : 85.34, mIoU 69.17\n","Step [250/363], Loss: 0.3170, Acc : 85.42, mIoU 69.15\n","Step [300/363], Loss: 0.4127, Acc : 85.48, mIoU 69.12\n","Step [350/363], Loss: 0.8732, Acc : 85.43, mIoU 69.21\n","Epoch time : 341.5s\n","Validation . . . \n","Step [50/120], Loss: 0.3862, Acc : 81.32, mIoU 58.26\n","Step [100/120], Loss: 0.4480, Acc : 81.26, mIoU 57.81\n","Epoch time : 41.5s\n","Loss 0.4816,  Acc 81.50,  IoU 57.8928\n","EPOCH 44/100\n","Step [50/363], Loss: 0.2669, Acc : 85.15, mIoU 67.79\n","Step [100/363], Loss: 0.2276, Acc : 85.44, mIoU 68.45\n","Step [150/363], Loss: 0.3252, Acc : 85.49, mIoU 69.19\n","Step [200/363], Loss: 0.3051, Acc : 85.53, mIoU 69.44\n","Step [250/363], Loss: 0.3605, Acc : 85.55, mIoU 69.72\n","Step [300/363], Loss: 0.2609, Acc : 85.55, mIoU 69.69\n","Step [350/363], Loss: 0.4457, Acc : 85.61, mIoU 69.92\n","Epoch time : 340.2s\n","Validation . . . \n","Step [50/120], Loss: 0.6450, Acc : 82.52, mIoU 59.05\n","Step [100/120], Loss: 0.6845, Acc : 81.95, mIoU 58.14\n","Epoch time : 40.8s\n","Loss 0.6893,  Acc 81.79,  IoU 58.5094\n","EPOCH 45/100\n","Step [50/363], Loss: 0.3432, Acc : 86.41, mIoU 70.71\n","Step [100/363], Loss: 0.4191, Acc : 86.06, mIoU 70.55\n","Step [150/363], Loss: 0.2802, Acc : 85.87, mIoU 70.30\n","Step [200/363], Loss: 0.3378, Acc : 85.92, mIoU 70.35\n","Step [250/363], Loss: 0.3513, Acc : 85.75, mIoU 70.00\n","Step [300/363], Loss: 0.3684, Acc : 85.74, mIoU 70.07\n","Step [350/363], Loss: 0.4029, Acc : 85.76, mIoU 70.18\n","Epoch time : 341.6s\n","Validation . . . \n","Step [50/120], Loss: 0.4248, Acc : 81.55, mIoU 57.25\n","Step [100/120], Loss: 0.5407, Acc : 81.79, mIoU 58.07\n","Epoch time : 39.7s\n","Loss 0.6755,  Acc 81.92,  IoU 58.5863\n","EPOCH 46/100\n","Step [50/363], Loss: 0.3432, Acc : 86.40, mIoU 71.54\n","Step [100/363], Loss: 0.3120, Acc : 86.42, mIoU 71.81\n","Step [150/363], Loss: 0.4376, Acc : 86.29, mIoU 71.67\n","Step [200/363], Loss: 0.3755, Acc : 86.18, mIoU 71.14\n","Step [250/363], Loss: 0.2860, Acc : 86.21, mIoU 71.60\n","Step [300/363], Loss: 0.2847, Acc : 86.06, mIoU 71.14\n","Step [350/363], Loss: 0.4431, Acc : 86.04, mIoU 70.91\n","Epoch time : 339.6s\n","Validation . . . \n","Step [50/120], Loss: 0.6131, Acc : 80.70, mIoU 56.21\n","Step [100/120], Loss: 0.4412, Acc : 81.13, mIoU 58.31\n","Epoch time : 40.2s\n","Loss 0.5285,  Acc 81.19,  IoU 58.0395\n","EPOCH 47/100\n","Step [50/363], Loss: 0.5892, Acc : 86.13, mIoU 70.07\n","Step [100/363], Loss: 0.3449, Acc : 86.04, mIoU 71.82\n","Step [150/363], Loss: 0.3315, Acc : 86.15, mIoU 71.49\n","Step [200/363], Loss: 0.3963, Acc : 86.13, mIoU 71.23\n","Step [250/363], Loss: 0.3926, Acc : 86.01, mIoU 71.11\n","Step [300/363], Loss: 0.4196, Acc : 85.88, mIoU 71.02\n","Step [350/363], Loss: 0.3743, Acc : 86.01, mIoU 71.16\n","Epoch time : 338.9s\n","Validation . . . \n","Step [50/120], Loss: 0.4044, Acc : 82.20, mIoU 59.53\n","Step [100/120], Loss: 0.5855, Acc : 82.15, mIoU 59.21\n","Epoch time : 41.2s\n","Loss 0.6736,  Acc 82.11,  IoU 59.0695\n","EPOCH 48/100\n","Step [50/363], Loss: 0.2792, Acc : 86.08, mIoU 72.17\n","Step [100/363], Loss: 0.5137, Acc : 86.17, mIoU 72.40\n","Step [150/363], Loss: 0.2601, Acc : 86.61, mIoU 72.68\n","Step [200/363], Loss: 0.4397, Acc : 86.39, mIoU 72.51\n","Step [250/363], Loss: 0.4961, Acc : 86.36, mIoU 72.27\n","Step [300/363], Loss: 0.4546, Acc : 86.27, mIoU 71.87\n","Step [350/363], Loss: 0.5705, Acc : 86.18, mIoU 71.77\n","Epoch time : 342.1s\n","Validation . . . \n","Step [50/120], Loss: 0.6112, Acc : 81.37, mIoU 56.37\n","Step [100/120], Loss: 0.6677, Acc : 80.67, mIoU 56.20\n","Epoch time : 41.9s\n","Loss 0.5722,  Acc 80.49,  IoU 56.2305\n","EPOCH 49/100\n","Step [50/363], Loss: 0.6342, Acc : 86.03, mIoU 71.03\n","Step [100/363], Loss: 0.3210, Acc : 85.97, mIoU 71.40\n","Step [150/363], Loss: 0.2333, Acc : 86.20, mIoU 71.15\n","Step [200/363], Loss: 0.3985, Acc : 86.11, mIoU 71.04\n","Step [250/363], Loss: 0.3460, Acc : 85.93, mIoU 71.18\n","Step [300/363], Loss: 0.2646, Acc : 86.05, mIoU 71.73\n","Step [350/363], Loss: 0.3326, Acc : 86.13, mIoU 71.79\n","Epoch time : 343.8s\n","Validation . . . \n","Step [50/120], Loss: 0.4485, Acc : 80.73, mIoU 58.36\n","Step [100/120], Loss: 0.7196, Acc : 81.29, mIoU 57.67\n","Epoch time : 40.2s\n","Loss 0.4051,  Acc 81.74,  IoU 58.1118\n","EPOCH 50/100\n","Step [50/363], Loss: 0.3445, Acc : 87.57, mIoU 74.22\n","Step [100/363], Loss: 0.3706, Acc : 87.64, mIoU 75.16\n","Step [150/363], Loss: 0.3177, Acc : 87.28, mIoU 74.14\n","Step [200/363], Loss: 0.4675, Acc : 87.18, mIoU 73.97\n","Step [250/363], Loss: 0.3626, Acc : 86.87, mIoU 73.43\n","Step [300/363], Loss: 0.3428, Acc : 86.85, mIoU 73.20\n","Step [350/363], Loss: 0.3299, Acc : 86.74, mIoU 72.86\n","Epoch time : 344.3s\n","Validation . . . \n","Step [50/120], Loss: 0.4778, Acc : 82.01, mIoU 56.35\n","Step [100/120], Loss: 0.6867, Acc : 81.62, mIoU 57.68\n","Epoch time : 41.8s\n","Loss 0.4466,  Acc 81.60,  IoU 58.4200\n","EPOCH 51/100\n","Step [50/363], Loss: 0.4235, Acc : 87.03, mIoU 74.09\n","Step [100/363], Loss: 0.3885, Acc : 87.14, mIoU 74.54\n","Step [150/363], Loss: 0.3503, Acc : 86.88, mIoU 74.05\n","Step [200/363], Loss: 0.5178, Acc : 86.73, mIoU 73.69\n","Step [250/363], Loss: 0.2094, Acc : 86.81, mIoU 73.88\n","Step [300/363], Loss: 0.3397, Acc : 86.78, mIoU 73.72\n","Step [350/363], Loss: 0.3875, Acc : 86.59, mIoU 73.24\n","Epoch time : 339.3s\n","Validation . . . \n","Step [50/120], Loss: 0.6044, Acc : 82.13, mIoU 56.04\n","Step [100/120], Loss: 0.4974, Acc : 81.72, mIoU 57.20\n","Epoch time : 40.8s\n","Loss 0.4373,  Acc 81.90,  IoU 57.3978\n","EPOCH 52/100\n","Step [50/363], Loss: 0.3975, Acc : 86.25, mIoU 71.67\n","Step [100/363], Loss: 0.5218, Acc : 86.23, mIoU 72.76\n","Step [150/363], Loss: 0.2592, Acc : 86.56, mIoU 73.16\n","Step [200/363], Loss: 0.3408, Acc : 86.61, mIoU 73.04\n","Step [250/363], Loss: 0.2941, Acc : 86.77, mIoU 73.13\n","Step [300/363], Loss: 0.3169, Acc : 86.99, mIoU 73.28\n","Step [350/363], Loss: 0.4499, Acc : 87.00, mIoU 73.21\n","Epoch time : 330.5s\n","Validation . . . \n","Step [50/120], Loss: 0.5335, Acc : 80.29, mIoU 57.14\n","Step [100/120], Loss: 0.4143, Acc : 81.31, mIoU 58.68\n","Epoch time : 38.8s\n","Loss 0.4001,  Acc 81.23,  IoU 58.4598\n","EPOCH 53/100\n","Step [50/363], Loss: 0.3274, Acc : 87.61, mIoU 74.10\n","Step [100/363], Loss: 0.2804, Acc : 87.45, mIoU 74.23\n","Step [150/363], Loss: 0.3700, Acc : 87.32, mIoU 74.27\n","Step [200/363], Loss: 0.4281, Acc : 87.24, mIoU 74.14\n","Step [250/363], Loss: 0.2500, Acc : 87.14, mIoU 73.81\n","Step [300/363], Loss: 0.2927, Acc : 87.11, mIoU 73.96\n","Step [350/363], Loss: 0.2996, Acc : 87.11, mIoU 74.02\n","Epoch time : 323.7s\n","Validation . . . \n","Step [50/120], Loss: 0.5656, Acc : 81.71, mIoU 58.25\n","Step [100/120], Loss: 0.4637, Acc : 81.59, mIoU 57.42\n","Epoch time : 38.8s\n","Loss 0.4288,  Acc 81.63,  IoU 57.7017\n","EPOCH 54/100\n","Step [50/363], Loss: 0.3462, Acc : 86.79, mIoU 74.66\n","Step [100/363], Loss: 0.4344, Acc : 87.13, mIoU 75.02\n","Step [150/363], Loss: 0.2744, Acc : 87.08, mIoU 74.49\n","Step [200/363], Loss: 0.2518, Acc : 87.05, mIoU 74.37\n","Step [250/363], Loss: 0.3028, Acc : 87.13, mIoU 74.27\n","Step [300/363], Loss: 0.3496, Acc : 86.95, mIoU 73.87\n","Step [350/363], Loss: 0.4387, Acc : 86.98, mIoU 73.68\n","Epoch time : 325.4s\n","Validation . . . \n","Step [50/120], Loss: 0.3966, Acc : 81.95, mIoU 58.05\n","Step [100/120], Loss: 0.5948, Acc : 81.56, mIoU 57.04\n","Epoch time : 39.8s\n","Loss 0.4964,  Acc 81.43,  IoU 57.4521\n","EPOCH 55/100\n","Step [50/363], Loss: 0.3355, Acc : 88.39, mIoU 76.12\n","Step [100/363], Loss: 0.3056, Acc : 87.65, mIoU 75.69\n","Step [150/363], Loss: 0.3285, Acc : 87.50, mIoU 75.13\n","Step [200/363], Loss: 0.4527, Acc : 87.54, mIoU 75.39\n","Step [250/363], Loss: 0.3123, Acc : 87.43, mIoU 75.37\n","Step [300/363], Loss: 0.4399, Acc : 87.46, mIoU 75.42\n","Step [350/363], Loss: 0.3315, Acc : 87.45, mIoU 75.14\n","Epoch time : 326.0s\n","Validation . . . \n","Step [50/120], Loss: 0.3402, Acc : 81.42, mIoU 57.45\n","Step [100/120], Loss: 0.6796, Acc : 81.46, mIoU 57.44\n","Epoch time : 39.5s\n","Loss 0.5426,  Acc 81.43,  IoU 57.4582\n","EPOCH 56/100\n","Step [50/363], Loss: 0.3397, Acc : 87.16, mIoU 74.20\n","Step [100/363], Loss: 0.3411, Acc : 87.49, mIoU 75.40\n","Step [150/363], Loss: 0.2896, Acc : 87.32, mIoU 74.29\n","Step [200/363], Loss: 0.3511, Acc : 87.42, mIoU 74.63\n","Step [250/363], Loss: 0.3071, Acc : 87.42, mIoU 74.65\n","Step [300/363], Loss: 0.1952, Acc : 87.40, mIoU 74.69\n","Step [350/363], Loss: 0.2851, Acc : 87.46, mIoU 74.73\n","Epoch time : 325.4s\n","Validation . . . \n","Step [50/120], Loss: 0.3400, Acc : 82.48, mIoU 59.35\n","Step [100/120], Loss: 0.4102, Acc : 82.41, mIoU 59.50\n","Epoch time : 40.0s\n","Loss 0.4529,  Acc 81.93,  IoU 58.6868\n","EPOCH 57/100\n","Step [50/363], Loss: 0.2451, Acc : 87.74, mIoU 76.11\n","Step [100/363], Loss: 0.3429, Acc : 87.72, mIoU 75.72\n","Step [150/363], Loss: 0.3456, Acc : 87.63, mIoU 75.95\n","Step [200/363], Loss: 0.3096, Acc : 87.59, mIoU 75.58\n","Step [250/363], Loss: 0.3198, Acc : 87.47, mIoU 75.09\n","Step [300/363], Loss: 0.2517, Acc : 87.45, mIoU 75.10\n","Step [350/363], Loss: 0.3198, Acc : 87.43, mIoU 74.90\n","Epoch time : 326.4s\n","Validation . . . \n","Step [50/120], Loss: 0.6655, Acc : 81.46, mIoU 57.87\n","Step [100/120], Loss: 0.5146, Acc : 81.93, mIoU 58.92\n","Epoch time : 38.4s\n","Loss 0.5579,  Acc 81.86,  IoU 58.4798\n","EPOCH 58/100\n","Step [50/363], Loss: 0.2884, Acc : 87.38, mIoU 74.64\n","Step [100/363], Loss: 0.2366, Acc : 87.24, mIoU 74.90\n","Step [150/363], Loss: 0.2459, Acc : 87.37, mIoU 75.34\n","Step [200/363], Loss: 0.4124, Acc : 87.43, mIoU 75.65\n","Step [250/363], Loss: 0.4228, Acc : 87.45, mIoU 75.43\n","Step [300/363], Loss: 0.2189, Acc : 87.52, mIoU 75.56\n","Step [350/363], Loss: 0.2008, Acc : 87.56, mIoU 75.48\n","Epoch time : 324.6s\n","Validation . . . \n","Step [50/120], Loss: 0.5105, Acc : 81.82, mIoU 57.52\n","Step [100/120], Loss: 0.4994, Acc : 81.26, mIoU 58.28\n","Epoch time : 39.8s\n","Loss 0.5139,  Acc 81.49,  IoU 58.3926\n","EPOCH 59/100\n","Step [50/363], Loss: 0.3456, Acc : 87.45, mIoU 74.49\n","Step [100/363], Loss: 0.4335, Acc : 88.04, mIoU 76.10\n","Step [150/363], Loss: 0.1935, Acc : 88.04, mIoU 76.45\n","Step [200/363], Loss: 0.3129, Acc : 87.90, mIoU 76.08\n","Step [250/363], Loss: 0.3378, Acc : 87.79, mIoU 75.81\n","Step [300/363], Loss: 0.2913, Acc : 87.79, mIoU 75.76\n","Step [350/363], Loss: 0.3535, Acc : 87.85, mIoU 75.84\n","Epoch time : 326.5s\n","Validation . . . \n","Step [50/120], Loss: 0.4925, Acc : 81.94, mIoU 58.46\n","Step [100/120], Loss: 0.2705, Acc : 81.72, mIoU 58.43\n","Epoch time : 39.7s\n","Loss 0.2594,  Acc 81.83,  IoU 58.7188\n","EPOCH 60/100\n","Step [50/363], Loss: 0.3863, Acc : 87.68, mIoU 75.06\n","Step [100/363], Loss: 0.4318, Acc : 88.06, mIoU 76.16\n","Step [150/363], Loss: 0.3401, Acc : 88.10, mIoU 76.33\n","Step [200/363], Loss: 0.2336, Acc : 88.03, mIoU 76.19\n","Step [250/363], Loss: 0.4044, Acc : 88.09, mIoU 76.09\n","Step [300/363], Loss: 0.3839, Acc : 87.90, mIoU 75.78\n","Step [350/363], Loss: 0.3170, Acc : 87.77, mIoU 75.73\n","Epoch time : 325.1s\n","Validation . . . \n","Step [50/120], Loss: 0.7671, Acc : 80.48, mIoU 57.24\n","Step [100/120], Loss: 0.3127, Acc : 81.07, mIoU 57.85\n","Epoch time : 38.3s\n","Loss 0.7882,  Acc 81.37,  IoU 58.0332\n","EPOCH 61/100\n","Step [50/363], Loss: 0.3197, Acc : 87.79, mIoU 74.43\n","Step [100/363], Loss: 0.3840, Acc : 88.08, mIoU 75.25\n","Step [150/363], Loss: 0.3349, Acc : 87.78, mIoU 75.16\n","Step [200/363], Loss: 0.3793, Acc : 87.63, mIoU 74.87\n","Step [250/363], Loss: 0.3797, Acc : 87.74, mIoU 75.37\n","Step [300/363], Loss: 0.1745, Acc : 87.96, mIoU 75.61\n","Step [350/363], Loss: 0.3600, Acc : 88.00, mIoU 75.76\n","Epoch time : 325.0s\n","Validation . . . \n","Step [50/120], Loss: 0.5580, Acc : 81.58, mIoU 57.01\n","Step [100/120], Loss: 0.8626, Acc : 81.94, mIoU 58.51\n","Epoch time : 39.3s\n","Loss 0.6505,  Acc 81.29,  IoU 57.3313\n","EPOCH 62/100\n","Step [50/363], Loss: 0.3187, Acc : 88.47, mIoU 75.86\n","Step [100/363], Loss: 0.2559, Acc : 88.42, mIoU 76.97\n","Step [150/363], Loss: 0.3516, Acc : 88.69, mIoU 78.02\n","Step [200/363], Loss: 0.3861, Acc : 88.60, mIoU 77.89\n","Step [250/363], Loss: 0.2405, Acc : 88.55, mIoU 77.82\n","Step [300/363], Loss: 0.3226, Acc : 88.39, mIoU 77.50\n","Step [350/363], Loss: 0.3046, Acc : 88.46, mIoU 77.45\n","Epoch time : 322.1s\n","Validation . . . \n","Step [50/120], Loss: 0.6289, Acc : 81.39, mIoU 57.36\n","Step [100/120], Loss: 0.3721, Acc : 81.49, mIoU 57.74\n","Epoch time : 39.5s\n","Loss 0.7330,  Acc 80.98,  IoU 57.0857\n","EPOCH 63/100\n","Step [50/363], Loss: 0.2536, Acc : 88.70, mIoU 77.51\n","Step [100/363], Loss: 0.3035, Acc : 88.71, mIoU 77.48\n","Step [150/363], Loss: 0.2955, Acc : 88.88, mIoU 77.79\n","Step [200/363], Loss: 0.2529, Acc : 88.79, mIoU 77.68\n","Step [250/363], Loss: 0.3575, Acc : 88.55, mIoU 77.20\n","Step [300/363], Loss: 0.3196, Acc : 88.27, mIoU 76.73\n","Step [350/363], Loss: 0.2344, Acc : 88.19, mIoU 76.50\n","Epoch time : 325.3s\n","Validation . . . \n","Step [50/120], Loss: 0.4869, Acc : 82.52, mIoU 59.66\n","Step [100/120], Loss: 0.4037, Acc : 81.89, mIoU 58.98\n","Epoch time : 40.5s\n","Loss 0.4572,  Acc 81.63,  IoU 58.8585\n","EPOCH 64/100\n","Step [50/363], Loss: 0.3709, Acc : 88.69, mIoU 77.44\n","Step [100/363], Loss: 0.2716, Acc : 88.69, mIoU 77.50\n","Step [150/363], Loss: 0.2253, Acc : 88.66, mIoU 77.15\n","Step [200/363], Loss: 0.2558, Acc : 88.58, mIoU 76.96\n","Step [250/363], Loss: 0.4703, Acc : 88.41, mIoU 76.82\n","Step [300/363], Loss: 0.2862, Acc : 88.42, mIoU 76.95\n","Step [350/363], Loss: 0.3143, Acc : 88.41, mIoU 77.03\n","Epoch time : 336.8s\n","Validation . . . \n","Step [50/120], Loss: 0.3380, Acc : 81.84, mIoU 58.42\n","Step [100/120], Loss: 0.5046, Acc : 81.64, mIoU 59.09\n","Epoch time : 40.2s\n","Loss 0.3593,  Acc 81.61,  IoU 58.7743\n","EPOCH 65/100\n","Step [50/363], Loss: 0.2805, Acc : 89.35, mIoU 77.89\n","Step [100/363], Loss: 0.3603, Acc : 88.88, mIoU 78.32\n","Step [150/363], Loss: 0.2329, Acc : 88.80, mIoU 78.37\n","Step [200/363], Loss: 0.3413, Acc : 88.59, mIoU 77.84\n","Step [250/363], Loss: 0.3042, Acc : 88.67, mIoU 77.80\n","Step [300/363], Loss: 0.2848, Acc : 88.58, mIoU 77.77\n","Step [350/363], Loss: 0.2268, Acc : 88.56, mIoU 77.67\n","Epoch time : 339.5s\n","Validation . . . \n","Step [50/120], Loss: 0.5896, Acc : 81.34, mIoU 57.68\n","Step [100/120], Loss: 0.4431, Acc : 80.81, mIoU 58.62\n","Epoch time : 39.7s\n","Loss 0.7055,  Acc 80.77,  IoU 58.2783\n","EPOCH 66/100\n","Step [50/363], Loss: 0.2642, Acc : 89.98, mIoU 80.68\n","Step [100/363], Loss: 0.2560, Acc : 89.42, mIoU 79.27\n","Step [150/363], Loss: 0.2743, Acc : 89.32, mIoU 79.17\n","Step [200/363], Loss: 0.2500, Acc : 89.12, mIoU 78.88\n","Step [250/363], Loss: 0.2320, Acc : 88.85, mIoU 78.49\n","Step [300/363], Loss: 0.2483, Acc : 88.79, mIoU 78.21\n","Step [350/363], Loss: 0.2147, Acc : 88.61, mIoU 77.96\n","Epoch time : 334.3s\n","Validation . . . \n","Step [50/120], Loss: 1.2451, Acc : 82.39, mIoU 60.39\n","Step [100/120], Loss: 0.6153, Acc : 81.74, mIoU 58.77\n","Epoch time : 39.1s\n","Loss 0.6559,  Acc 81.49,  IoU 57.9348\n","EPOCH 67/100\n","Step [50/363], Loss: 0.2665, Acc : 88.75, mIoU 78.39\n","Step [100/363], Loss: 0.2341, Acc : 88.75, mIoU 78.49\n","Step [150/363], Loss: 0.3116, Acc : 88.61, mIoU 77.49\n","Step [200/363], Loss: 0.3256, Acc : 88.60, mIoU 77.24\n","Step [250/363], Loss: 0.2839, Acc : 88.58, mIoU 77.29\n","Step [300/363], Loss: 0.3155, Acc : 88.45, mIoU 77.17\n","Step [350/363], Loss: 0.2776, Acc : 88.47, mIoU 77.11\n","Epoch time : 337.4s\n","Validation . . . \n","Step [50/120], Loss: 0.4604, Acc : 80.68, mIoU 56.81\n","Step [100/120], Loss: 0.9229, Acc : 81.26, mIoU 58.03\n","Epoch time : 40.1s\n","Loss 0.5851,  Acc 81.39,  IoU 58.7567\n","EPOCH 68/100\n","Step [50/363], Loss: 0.2405, Acc : 88.59, mIoU 77.68\n","Step [100/363], Loss: 0.3840, Acc : 88.80, mIoU 77.90\n","Step [150/363], Loss: 0.3349, Acc : 88.81, mIoU 78.12\n","Step [200/363], Loss: 0.2404, Acc : 88.87, mIoU 78.79\n","Step [250/363], Loss: 0.2187, Acc : 88.87, mIoU 78.48\n","Step [300/363], Loss: 0.2467, Acc : 88.87, mIoU 78.33\n","Step [350/363], Loss: 0.2086, Acc : 88.88, mIoU 78.12\n","Epoch time : 333.6s\n","Validation . . . \n","Step [50/120], Loss: 0.4008, Acc : 82.09, mIoU 60.14\n","Step [100/120], Loss: 0.5760, Acc : 81.94, mIoU 59.93\n","Epoch time : 39.7s\n","Loss 1.1536,  Acc 81.86,  IoU 59.1020\n","EPOCH 69/100\n","Step [50/363], Loss: 0.3274, Acc : 88.58, mIoU 78.91\n","Step [100/363], Loss: 0.2469, Acc : 88.92, mIoU 79.23\n","Step [150/363], Loss: 0.3678, Acc : 89.05, mIoU 78.97\n","Step [200/363], Loss: 0.2307, Acc : 88.98, mIoU 78.96\n","Step [250/363], Loss: 0.3213, Acc : 89.02, mIoU 79.02\n","Step [300/363], Loss: 0.3979, Acc : 89.08, mIoU 79.09\n","Step [350/363], Loss: 0.4848, Acc : 88.93, mIoU 78.80\n","Epoch time : 335.6s\n","Validation . . . \n","Step [50/120], Loss: 0.4370, Acc : 81.12, mIoU 55.49\n","Step [100/120], Loss: 0.3472, Acc : 81.10, mIoU 56.87\n","Epoch time : 39.8s\n","Loss 0.4730,  Acc 81.20,  IoU 56.8196\n","EPOCH 70/100\n","Step [50/363], Loss: 0.2976, Acc : 88.70, mIoU 77.14\n","Step [100/363], Loss: 0.2805, Acc : 89.00, mIoU 78.56\n","Step [150/363], Loss: 0.2206, Acc : 89.06, mIoU 78.67\n","Step [200/363], Loss: 0.2840, Acc : 89.16, mIoU 78.96\n","Step [250/363], Loss: 0.5279, Acc : 89.24, mIoU 79.20\n","Step [300/363], Loss: 0.2983, Acc : 89.19, mIoU 79.16\n","Step [350/363], Loss: 0.2744, Acc : 89.16, mIoU 79.05\n","Epoch time : 338.9s\n","Validation . . . \n","Step [50/120], Loss: 0.6188, Acc : 82.82, mIoU 58.74\n","Step [100/120], Loss: 0.6047, Acc : 81.57, mIoU 58.68\n","Epoch time : 39.9s\n","Loss 0.7117,  Acc 81.66,  IoU 58.3042\n","EPOCH 71/100\n","Step [50/363], Loss: 0.1865, Acc : 90.18, mIoU 78.88\n","Step [100/363], Loss: 0.2576, Acc : 89.83, mIoU 79.11\n","Step [150/363], Loss: 0.3322, Acc : 89.32, mIoU 79.18\n","Step [200/363], Loss: 0.2860, Acc : 89.30, mIoU 78.98\n","Step [250/363], Loss: 0.2921, Acc : 89.16, mIoU 78.85\n","Step [300/363], Loss: 0.2664, Acc : 89.14, mIoU 78.91\n","Step [350/363], Loss: 0.3720, Acc : 89.12, mIoU 78.81\n","Epoch time : 337.8s\n","Validation . . . \n","Step [50/120], Loss: 0.6979, Acc : 81.44, mIoU 57.19\n","Step [100/120], Loss: 0.4030, Acc : 81.49, mIoU 58.34\n","Epoch time : 41.1s\n","Loss 0.5909,  Acc 81.46,  IoU 58.0614\n","EPOCH 72/100\n","Step [50/363], Loss: 0.2951, Acc : 89.76, mIoU 79.75\n","Step [100/363], Loss: 0.2945, Acc : 89.56, mIoU 80.07\n","Step [150/363], Loss: 0.2800, Acc : 89.69, mIoU 80.03\n","Step [200/363], Loss: 0.3871, Acc : 89.62, mIoU 79.80\n","Step [250/363], Loss: 0.2586, Acc : 89.60, mIoU 80.11\n","Step [300/363], Loss: 0.3547, Acc : 89.50, mIoU 79.76\n","Step [350/363], Loss: 0.1914, Acc : 89.54, mIoU 79.78\n","Epoch time : 335.4s\n","Validation . . . \n","Step [50/120], Loss: 0.6947, Acc : 81.89, mIoU 58.49\n","Step [100/120], Loss: 0.5188, Acc : 81.39, mIoU 57.62\n","Epoch time : 40.0s\n","Loss 0.4074,  Acc 81.68,  IoU 57.4886\n","EPOCH 73/100\n","Step [50/363], Loss: 0.3133, Acc : 89.11, mIoU 79.16\n","Step [100/363], Loss: 0.2472, Acc : 89.43, mIoU 79.74\n","Step [150/363], Loss: 0.1973, Acc : 89.37, mIoU 80.01\n","Step [200/363], Loss: 0.2871, Acc : 89.38, mIoU 79.81\n","Step [250/363], Loss: 0.2714, Acc : 89.49, mIoU 80.01\n","Step [300/363], Loss: 0.2032, Acc : 89.51, mIoU 80.13\n","Step [350/363], Loss: 0.3239, Acc : 89.50, mIoU 79.97\n","Epoch time : 337.1s\n","Validation . . . \n","Step [50/120], Loss: 0.5969, Acc : 81.32, mIoU 57.42\n","Step [100/120], Loss: 0.8254, Acc : 81.23, mIoU 57.03\n","Epoch time : 40.4s\n","Loss 0.5103,  Acc 81.21,  IoU 57.5656\n","EPOCH 74/100\n","Step [50/363], Loss: 0.3220, Acc : 89.91, mIoU 81.34\n","Step [100/363], Loss: 0.2504, Acc : 90.12, mIoU 81.58\n","Step [150/363], Loss: 0.3476, Acc : 89.64, mIoU 80.58\n","Step [200/363], Loss: 0.2729, Acc : 89.56, mIoU 80.01\n","Step [250/363], Loss: 0.4793, Acc : 89.56, mIoU 79.94\n","Step [300/363], Loss: 0.2065, Acc : 89.53, mIoU 79.86\n","Step [350/363], Loss: 0.3388, Acc : 89.55, mIoU 79.99\n","Epoch time : 340.7s\n","Validation . . . \n","Step [50/120], Loss: 0.4911, Acc : 81.48, mIoU 58.85\n","Step [100/120], Loss: 0.5613, Acc : 81.23, mIoU 56.96\n","Epoch time : 40.1s\n","Loss 1.9606,  Acc 81.00,  IoU 57.2958\n","EPOCH 75/100\n","Step [50/363], Loss: 0.3414, Acc : 90.09, mIoU 79.87\n","Step [100/363], Loss: 0.3356, Acc : 89.96, mIoU 80.11\n","Step [150/363], Loss: 0.2068, Acc : 89.91, mIoU 80.37\n","Step [200/363], Loss: 0.2807, Acc : 89.83, mIoU 80.35\n","Step [250/363], Loss: 0.3170, Acc : 89.65, mIoU 80.04\n","Step [300/363], Loss: 0.2749, Acc : 89.61, mIoU 79.93\n","Step [350/363], Loss: 0.2508, Acc : 89.52, mIoU 79.76\n","Epoch time : 342.6s\n","Validation . . . \n","Step [50/120], Loss: 0.5831, Acc : 80.48, mIoU 56.99\n","Step [100/120], Loss: 0.4267, Acc : 81.02, mIoU 57.11\n","Epoch time : 40.6s\n","Loss 0.2728,  Acc 80.95,  IoU 57.1493\n","EPOCH 76/100\n","Step [50/363], Loss: 0.3152, Acc : 90.31, mIoU 81.02\n","Step [100/363], Loss: 0.2352, Acc : 90.29, mIoU 81.34\n","Step [150/363], Loss: 0.1885, Acc : 89.96, mIoU 81.08\n","Step [200/363], Loss: 0.2563, Acc : 89.83, mIoU 80.62\n","Step [250/363], Loss: 0.2148, Acc : 89.86, mIoU 80.61\n","Step [300/363], Loss: 0.3237, Acc : 89.78, mIoU 80.53\n","Step [350/363], Loss: 0.2808, Acc : 89.75, mIoU 80.54\n","Epoch time : 339.3s\n","Validation . . . \n","Step [50/120], Loss: 0.4092, Acc : 81.18, mIoU 58.23\n","Step [100/120], Loss: 0.4362, Acc : 80.88, mIoU 58.41\n","Epoch time : 39.9s\n","Loss 0.8113,  Acc 81.07,  IoU 58.7002\n","EPOCH 77/100\n","Step [50/363], Loss: 0.2490, Acc : 89.39, mIoU 81.10\n","Step [100/363], Loss: 0.2972, Acc : 89.63, mIoU 81.57\n","Step [150/363], Loss: 0.2532, Acc : 89.75, mIoU 81.08\n","Step [200/363], Loss: 0.2883, Acc : 89.73, mIoU 80.71\n","Step [250/363], Loss: 0.2381, Acc : 89.76, mIoU 80.56\n","Step [300/363], Loss: 0.3282, Acc : 89.85, mIoU 80.70\n","Step [350/363], Loss: 0.2222, Acc : 89.87, mIoU 80.72\n","Epoch time : 338.5s\n","Validation . . . \n","Step [50/120], Loss: 0.6282, Acc : 81.46, mIoU 58.38\n","Step [100/120], Loss: 0.5822, Acc : 81.11, mIoU 58.10\n","Epoch time : 40.4s\n","Loss 0.7992,  Acc 81.28,  IoU 57.9719\n","EPOCH 78/100\n","Step [50/363], Loss: 0.2363, Acc : 90.19, mIoU 80.68\n","Step [100/363], Loss: 0.3103, Acc : 90.20, mIoU 81.46\n","Step [150/363], Loss: 0.2710, Acc : 90.20, mIoU 81.10\n","Step [200/363], Loss: 0.2265, Acc : 90.20, mIoU 80.99\n","Step [250/363], Loss: 0.3067, Acc : 90.01, mIoU 80.71\n","Step [300/363], Loss: 0.2395, Acc : 89.91, mIoU 80.57\n","Step [350/363], Loss: 0.2340, Acc : 89.91, mIoU 80.65\n","Epoch time : 340.4s\n","Validation . . . \n","Step [50/120], Loss: 0.4522, Acc : 82.25, mIoU 58.33\n","Step [100/120], Loss: 0.5317, Acc : 81.82, mIoU 58.87\n","Epoch time : 40.0s\n","Loss 0.4316,  Acc 81.72,  IoU 58.7010\n","EPOCH 79/100\n","Step [50/363], Loss: 0.2092, Acc : 90.01, mIoU 82.21\n","Step [100/363], Loss: 0.1847, Acc : 89.96, mIoU 81.73\n","Step [150/363], Loss: 0.1913, Acc : 90.11, mIoU 81.46\n","Step [200/363], Loss: 0.1976, Acc : 90.15, mIoU 81.46\n","Step [250/363], Loss: 0.2728, Acc : 90.03, mIoU 81.15\n","Step [300/363], Loss: 0.2656, Acc : 90.05, mIoU 81.26\n","Step [350/363], Loss: 0.2220, Acc : 90.04, mIoU 81.25\n","Epoch time : 337.3s\n","Validation . . . \n","Step [50/120], Loss: 0.6346, Acc : 80.12, mIoU 56.37\n","Step [100/120], Loss: 0.6198, Acc : 80.63, mIoU 57.07\n","Epoch time : 40.1s\n","Loss 0.3174,  Acc 80.46,  IoU 57.0167\n","EPOCH 80/100\n","Step [50/363], Loss: 0.2128, Acc : 90.48, mIoU 82.18\n","Step [100/363], Loss: 0.2053, Acc : 90.39, mIoU 82.10\n","Step [150/363], Loss: 0.3065, Acc : 90.31, mIoU 81.52\n","Step [200/363], Loss: 0.3314, Acc : 90.06, mIoU 81.18\n","Step [250/363], Loss: 0.3408, Acc : 89.90, mIoU 80.42\n","Step [300/363], Loss: 0.1966, Acc : 89.87, mIoU 80.29\n","Step [350/363], Loss: 0.2882, Acc : 89.99, mIoU 80.47\n","Epoch time : 340.1s\n","Validation . . . \n","Step [50/120], Loss: 0.5252, Acc : 80.81, mIoU 59.39\n","Step [100/120], Loss: 0.5042, Acc : 81.27, mIoU 59.13\n","Epoch time : 39.9s\n","Loss 0.3905,  Acc 81.26,  IoU 58.6069\n","EPOCH 81/100\n","Step [50/363], Loss: 0.2800, Acc : 90.76, mIoU 81.47\n","Step [100/363], Loss: 0.2473, Acc : 90.33, mIoU 81.68\n","Step [150/363], Loss: 0.2385, Acc : 90.45, mIoU 81.79\n","Step [200/363], Loss: 0.2250, Acc : 90.47, mIoU 81.84\n","Step [250/363], Loss: 0.2662, Acc : 90.32, mIoU 81.75\n","Step [300/363], Loss: 0.2395, Acc : 90.29, mIoU 81.56\n","Step [350/363], Loss: 0.1584, Acc : 90.25, mIoU 81.39\n","Epoch time : 336.1s\n","Validation . . . \n","Step [50/120], Loss: 0.4829, Acc : 80.75, mIoU 54.97\n","Step [100/120], Loss: 0.4537, Acc : 80.90, mIoU 56.59\n","Epoch time : 39.8s\n","Loss 0.8093,  Acc 80.85,  IoU 56.6808\n","EPOCH 82/100\n","Step [50/363], Loss: 0.2284, Acc : 90.64, mIoU 82.46\n","Step [100/363], Loss: 0.3483, Acc : 90.49, mIoU 82.16\n","Step [150/363], Loss: 0.2823, Acc : 90.24, mIoU 81.57\n","Step [200/363], Loss: 0.3534, Acc : 90.32, mIoU 81.92\n","Step [250/363], Loss: 0.3191, Acc : 90.40, mIoU 82.09\n","Step [300/363], Loss: 0.1746, Acc : 90.46, mIoU 82.15\n","Step [350/363], Loss: 0.2481, Acc : 90.50, mIoU 82.12\n","Epoch time : 338.6s\n","Validation . . . \n","Step [50/120], Loss: 0.6021, Acc : 81.18, mIoU 57.52\n","Step [100/120], Loss: 0.3774, Acc : 80.74, mIoU 58.47\n","Epoch time : 40.1s\n","Loss 0.5708,  Acc 80.66,  IoU 58.5446\n","EPOCH 83/100\n","Step [50/363], Loss: 0.2710, Acc : 90.55, mIoU 81.64\n","Step [100/363], Loss: 0.2993, Acc : 90.49, mIoU 82.39\n","Step [150/363], Loss: 0.2890, Acc : 90.56, mIoU 82.39\n","Step [200/363], Loss: 0.3130, Acc : 90.63, mIoU 82.54\n","Step [250/363], Loss: 0.3073, Acc : 90.55, mIoU 82.36\n","Step [300/363], Loss: 0.2683, Acc : 90.52, mIoU 82.02\n","Step [350/363], Loss: 0.2842, Acc : 90.38, mIoU 81.77\n","Epoch time : 337.6s\n","Validation . . . \n","Step [50/120], Loss: 0.5056, Acc : 81.15, mIoU 56.51\n","Step [100/120], Loss: 0.4437, Acc : 81.16, mIoU 56.15\n","Epoch time : 39.6s\n","Loss 0.5059,  Acc 80.88,  IoU 55.9824\n","EPOCH 84/100\n","Step [50/363], Loss: 0.2443, Acc : 90.07, mIoU 80.12\n","Step [100/363], Loss: 0.1814, Acc : 90.08, mIoU 80.19\n","Step [150/363], Loss: 0.2949, Acc : 90.16, mIoU 80.66\n","Step [200/363], Loss: 0.2841, Acc : 89.89, mIoU 80.57\n","Step [250/363], Loss: 0.2863, Acc : 89.97, mIoU 80.80\n","Step [300/363], Loss: 0.2215, Acc : 90.09, mIoU 81.07\n","Step [350/363], Loss: 0.1766, Acc : 90.12, mIoU 81.13\n","Epoch time : 338.7s\n","Validation . . . \n","Step [50/120], Loss: 0.8593, Acc : 80.97, mIoU 58.10\n","Step [100/120], Loss: 0.5292, Acc : 81.21, mIoU 57.84\n","Epoch time : 39.4s\n","Loss 0.3847,  Acc 81.40,  IoU 58.2855\n","EPOCH 85/100\n","Step [50/363], Loss: 0.2387, Acc : 90.43, mIoU 82.25\n","Step [100/363], Loss: 0.2106, Acc : 90.62, mIoU 82.24\n","Step [150/363], Loss: 0.1970, Acc : 90.68, mIoU 82.16\n","Step [200/363], Loss: 0.2469, Acc : 90.67, mIoU 82.14\n","Step [250/363], Loss: 0.2467, Acc : 90.54, mIoU 81.80\n","Step [300/363], Loss: 0.2938, Acc : 90.50, mIoU 81.85\n","Step [350/363], Loss: 0.3193, Acc : 90.47, mIoU 81.72\n","Epoch time : 339.5s\n","Validation . . . \n","Step [50/120], Loss: 1.2611, Acc : 81.58, mIoU 57.76\n","Step [100/120], Loss: 0.5186, Acc : 81.07, mIoU 57.20\n","Epoch time : 39.6s\n","Loss 0.6390,  Acc 80.61,  IoU 57.0858\n","EPOCH 86/100\n","Step [50/363], Loss: 0.1700, Acc : 90.99, mIoU 82.91\n","Step [100/363], Loss: 0.1776, Acc : 91.05, mIoU 82.89\n","Step [150/363], Loss: 0.2880, Acc : 91.10, mIoU 83.06\n","Step [200/363], Loss: 0.2878, Acc : 90.93, mIoU 82.40\n","Step [250/363], Loss: 0.2804, Acc : 90.76, mIoU 82.16\n","Step [300/363], Loss: 0.2471, Acc : 90.71, mIoU 82.21\n","Step [350/363], Loss: 0.2654, Acc : 90.65, mIoU 82.06\n","Epoch time : 335.8s\n","Validation . . . \n","Step [50/120], Loss: 0.6717, Acc : 82.12, mIoU 58.80\n","Step [100/120], Loss: 0.5404, Acc : 81.40, mIoU 57.83\n","Epoch time : 39.6s\n","Loss 0.4682,  Acc 81.40,  IoU 57.8700\n","EPOCH 87/100\n","Step [50/363], Loss: 0.2960, Acc : 91.02, mIoU 82.13\n","Step [100/363], Loss: 0.2940, Acc : 91.14, mIoU 83.12\n","Step [150/363], Loss: 0.2500, Acc : 91.17, mIoU 83.18\n","Step [200/363], Loss: 0.2085, Acc : 91.07, mIoU 83.14\n","Step [250/363], Loss: 0.2608, Acc : 91.06, mIoU 83.11\n","Step [300/363], Loss: 0.2759, Acc : 90.93, mIoU 82.95\n","Step [350/363], Loss: 0.2824, Acc : 90.95, mIoU 83.03\n","Epoch time : 335.2s\n","Validation . . . \n","Step [50/120], Loss: 0.9007, Acc : 81.79, mIoU 58.21\n","Step [100/120], Loss: 0.5510, Acc : 82.05, mIoU 58.88\n","Epoch time : 39.9s\n","Loss 0.9718,  Acc 81.63,  IoU 58.2782\n","EPOCH 88/100\n","Step [50/363], Loss: 0.1719, Acc : 90.52, mIoU 81.72\n","Step [100/363], Loss: 0.2873, Acc : 90.53, mIoU 82.19\n","Step [150/363], Loss: 0.2086, Acc : 90.79, mIoU 82.63\n","Step [200/363], Loss: 0.2208, Acc : 90.88, mIoU 82.57\n","Step [250/363], Loss: 0.2605, Acc : 90.83, mIoU 82.69\n","Step [300/363], Loss: 0.2953, Acc : 90.80, mIoU 82.59\n","Step [350/363], Loss: 0.3041, Acc : 90.79, mIoU 82.45\n","Epoch time : 340.7s\n","Validation . . . \n","Step [50/120], Loss: 0.4868, Acc : 81.32, mIoU 57.31\n","Step [100/120], Loss: 0.5340, Acc : 81.75, mIoU 57.53\n","Epoch time : 40.2s\n","Loss 1.2597,  Acc 81.48,  IoU 57.4726\n","EPOCH 89/100\n","Step [50/363], Loss: 0.2476, Acc : 90.96, mIoU 83.54\n","Step [100/363], Loss: 0.2658, Acc : 91.19, mIoU 83.62\n","Step [150/363], Loss: 0.2144, Acc : 91.18, mIoU 83.55\n","Step [200/363], Loss: 0.2628, Acc : 91.25, mIoU 83.56\n","Step [250/363], Loss: 0.3356, Acc : 91.13, mIoU 83.11\n","Step [300/363], Loss: 0.2304, Acc : 91.00, mIoU 82.92\n","Step [350/363], Loss: 0.2193, Acc : 90.88, mIoU 82.58\n","Epoch time : 338.3s\n","Validation . . . \n","Step [50/120], Loss: 0.5883, Acc : 80.71, mIoU 56.21\n","Step [100/120], Loss: 0.7169, Acc : 81.32, mIoU 57.91\n","Epoch time : 40.0s\n","Loss 1.2616,  Acc 81.43,  IoU 58.2307\n","EPOCH 90/100\n","Step [50/363], Loss: 0.2145, Acc : 91.29, mIoU 82.55\n","Step [100/363], Loss: 0.2207, Acc : 91.05, mIoU 83.20\n","Step [150/363], Loss: 0.2263, Acc : 91.10, mIoU 83.38\n","Step [200/363], Loss: 0.2267, Acc : 91.18, mIoU 83.52\n","Step [250/363], Loss: 0.1861, Acc : 91.11, mIoU 83.45\n","Step [300/363], Loss: 0.2820, Acc : 91.05, mIoU 83.15\n","Step [350/363], Loss: 0.2347, Acc : 90.97, mIoU 83.08\n","Epoch time : 339.8s\n","Validation . . . \n","Step [50/120], Loss: 0.6909, Acc : 81.01, mIoU 56.44\n","Step [100/120], Loss: 0.4797, Acc : 81.05, mIoU 56.47\n","Epoch time : 39.9s\n","Loss 0.3735,  Acc 81.06,  IoU 57.1523\n","EPOCH 91/100\n","Step [50/363], Loss: 0.2818, Acc : 91.32, mIoU 83.11\n","Step [100/363], Loss: 0.2648, Acc : 91.22, mIoU 82.67\n","Step [150/363], Loss: 0.2375, Acc : 91.13, mIoU 82.67\n","Step [200/363], Loss: 0.2424, Acc : 90.91, mIoU 82.60\n","Step [250/363], Loss: 0.2550, Acc : 90.79, mIoU 82.18\n","Step [300/363], Loss: 0.2327, Acc : 90.76, mIoU 82.15\n","Step [350/363], Loss: 0.3339, Acc : 90.83, mIoU 82.39\n","Epoch time : 338.2s\n","Validation . . . \n","Step [50/120], Loss: 0.7265, Acc : 81.31, mIoU 57.02\n","Step [100/120], Loss: 0.8872, Acc : 81.42, mIoU 57.97\n","Epoch time : 39.8s\n","Loss 0.6394,  Acc 81.11,  IoU 58.3774\n","EPOCH 92/100\n","Step [50/363], Loss: 0.2122, Acc : 91.02, mIoU 83.50\n","Step [100/363], Loss: 0.1485, Acc : 91.38, mIoU 83.74\n","Step [150/363], Loss: 0.3022, Acc : 91.50, mIoU 84.20\n","Step [200/363], Loss: 0.2923, Acc : 91.54, mIoU 84.07\n","Step [250/363], Loss: 0.2534, Acc : 91.37, mIoU 83.91\n","Step [300/363], Loss: 0.1765, Acc : 91.22, mIoU 83.59\n","Step [350/363], Loss: 0.2525, Acc : 91.20, mIoU 83.57\n","Epoch time : 336.9s\n","Validation . . . \n","Step [50/120], Loss: 0.8079, Acc : 81.34, mIoU 55.82\n","Step [100/120], Loss: 0.6812, Acc : 81.58, mIoU 56.11\n","Epoch time : 39.0s\n","Loss 0.3787,  Acc 81.27,  IoU 56.2949\n","EPOCH 93/100\n","Step [50/363], Loss: 0.1976, Acc : 91.01, mIoU 82.33\n","Step [100/363], Loss: 0.2079, Acc : 91.33, mIoU 83.73\n","Step [150/363], Loss: 0.2307, Acc : 91.30, mIoU 83.72\n","Step [200/363], Loss: 0.3450, Acc : 91.30, mIoU 83.63\n","Step [250/363], Loss: 0.2996, Acc : 91.20, mIoU 83.41\n","Step [300/363], Loss: 0.1938, Acc : 91.16, mIoU 83.35\n","Step [350/363], Loss: 0.2604, Acc : 91.16, mIoU 83.30\n","Epoch time : 331.9s\n","Validation . . . \n","Step [50/120], Loss: 0.9328, Acc : 81.93, mIoU 57.69\n","Step [100/120], Loss: 0.7487, Acc : 81.77, mIoU 57.38\n","Epoch time : 38.7s\n","Loss 0.6352,  Acc 81.35,  IoU 56.8519\n","EPOCH 94/100\n","Step [50/363], Loss: 0.1974, Acc : 91.63, mIoU 83.97\n","Step [100/363], Loss: 0.2026, Acc : 91.58, mIoU 84.16\n","Step [150/363], Loss: 0.2517, Acc : 91.31, mIoU 83.81\n","Step [200/363], Loss: 0.1353, Acc : 91.35, mIoU 83.81\n","Step [250/363], Loss: 0.1236, Acc : 91.39, mIoU 83.89\n","Step [300/363], Loss: 0.1430, Acc : 91.48, mIoU 84.02\n","Step [350/363], Loss: 0.2267, Acc : 91.38, mIoU 83.71\n","Epoch time : 337.5s\n","Validation . . . \n","Step [50/120], Loss: 0.4901, Acc : 82.06, mIoU 56.66\n","Step [100/120], Loss: 1.4124, Acc : 81.08, mIoU 56.90\n","Epoch time : 39.5s\n","Loss 1.6399,  Acc 81.23,  IoU 56.8980\n","EPOCH 95/100\n","Step [50/363], Loss: 0.3889, Acc : 91.66, mIoU 83.21\n","Step [100/363], Loss: 0.1494, Acc : 91.78, mIoU 84.35\n","Step [150/363], Loss: 0.2431, Acc : 91.73, mIoU 84.42\n","Step [200/363], Loss: 0.3728, Acc : 91.72, mIoU 84.13\n","Step [250/363], Loss: 0.1756, Acc : 91.60, mIoU 84.00\n","Step [300/363], Loss: 0.2623, Acc : 91.40, mIoU 83.48\n","Step [350/363], Loss: 0.2162, Acc : 91.25, mIoU 83.22\n","Epoch time : 337.9s\n","Validation . . . \n","Step [50/120], Loss: 0.5995, Acc : 81.35, mIoU 58.54\n","Step [100/120], Loss: 0.6561, Acc : 81.52, mIoU 58.54\n","Epoch time : 40.1s\n","Loss 0.6364,  Acc 81.46,  IoU 58.4183\n","EPOCH 96/100\n","Step [50/363], Loss: 0.1297, Acc : 91.30, mIoU 84.58\n","Step [100/363], Loss: 0.3128, Acc : 90.79, mIoU 83.47\n","Step [150/363], Loss: 0.2100, Acc : 90.60, mIoU 82.10\n","Step [200/363], Loss: 0.2660, Acc : 90.71, mIoU 82.18\n","Step [250/363], Loss: 0.1996, Acc : 90.80, mIoU 82.23\n","Step [300/363], Loss: 0.2546, Acc : 90.81, mIoU 82.28\n","Step [350/363], Loss: 0.2284, Acc : 90.94, mIoU 82.62\n","Epoch time : 325.2s\n","Validation . . . \n","Step [50/120], Loss: 0.4457, Acc : 81.54, mIoU 57.95\n","Step [100/120], Loss: 1.1044, Acc : 81.68, mIoU 58.28\n","Epoch time : 38.7s\n","Loss 0.6575,  Acc 81.67,  IoU 58.4411\n","EPOCH 97/100\n","Step [50/363], Loss: 0.1579, Acc : 92.19, mIoU 85.04\n","Step [100/363], Loss: 0.2730, Acc : 90.77, mIoU 82.41\n","Step [150/363], Loss: 0.1893, Acc : 90.75, mIoU 82.71\n","Step [200/363], Loss: 0.2810, Acc : 90.93, mIoU 82.98\n","Step [250/363], Loss: 0.1856, Acc : 90.95, mIoU 82.98\n","Step [300/363], Loss: 0.3068, Acc : 91.08, mIoU 83.17\n","Step [350/363], Loss: 0.2519, Acc : 91.16, mIoU 83.28\n","Epoch time : 325.1s\n","Validation . . . \n","Step [50/120], Loss: 0.7122, Acc : 81.96, mIoU 58.61\n","Step [100/120], Loss: 0.3872, Acc : 81.62, mIoU 58.72\n","Epoch time : 38.9s\n","Loss 0.6638,  Acc 81.39,  IoU 58.2103\n","EPOCH 98/100\n","Step [50/363], Loss: 0.1564, Acc : 92.16, mIoU 85.49\n","Step [100/363], Loss: 0.2174, Acc : 91.98, mIoU 85.31\n","Step [150/363], Loss: 0.3180, Acc : 91.96, mIoU 85.29\n","Step [200/363], Loss: 0.2045, Acc : 92.01, mIoU 85.38\n","Step [250/363], Loss: 0.1425, Acc : 91.89, mIoU 85.05\n","Step [300/363], Loss: 0.2955, Acc : 91.79, mIoU 84.81\n","Step [350/363], Loss: 0.2052, Acc : 91.73, mIoU 84.52\n","Epoch time : 327.2s\n","Validation . . . \n","Step [50/120], Loss: 0.7708, Acc : 80.51, mIoU 56.31\n","Step [100/120], Loss: 1.3138, Acc : 80.85, mIoU 57.17\n","Epoch time : 39.0s\n","Loss 0.5600,  Acc 80.64,  IoU 56.8384\n","EPOCH 99/100\n","Step [50/363], Loss: 0.1615, Acc : 91.27, mIoU 83.42\n","Step [100/363], Loss: 0.2000, Acc : 91.20, mIoU 83.63\n","Step [150/363], Loss: 0.1490, Acc : 91.29, mIoU 83.46\n","Step [200/363], Loss: 0.1885, Acc : 91.35, mIoU 83.41\n","Step [250/363], Loss: 0.2521, Acc : 91.34, mIoU 83.45\n","Step [300/363], Loss: 0.2974, Acc : 91.27, mIoU 83.39\n","Step [350/363], Loss: 0.1715, Acc : 91.36, mIoU 83.65\n","Epoch time : 323.9s\n","Validation . . . \n","Step [50/120], Loss: 0.5838, Acc : 81.57, mIoU 57.17\n","Step [100/120], Loss: 1.2016, Acc : 81.51, mIoU 58.89\n","Epoch time : 38.6s\n","Loss 0.8612,  Acc 81.45,  IoU 58.8766\n","EPOCH 100/100\n","Step [50/363], Loss: 0.2293, Acc : 92.04, mIoU 85.29\n","Step [100/363], Loss: 0.2114, Acc : 91.61, mIoU 84.10\n","Step [150/363], Loss: 0.2378, Acc : 91.55, mIoU 83.91\n","Step [200/363], Loss: 0.2456, Acc : 91.59, mIoU 84.39\n","Step [250/363], Loss: 0.2268, Acc : 91.55, mIoU 84.25\n","Step [300/363], Loss: 0.2587, Acc : 91.66, mIoU 84.20\n","Step [350/363], Loss: 0.2925, Acc : 91.68, mIoU 84.17\n","Epoch time : 325.1s\n","Validation . . . \n","Step [50/120], Loss: 0.6917, Acc : 80.67, mIoU 56.59\n","Step [100/120], Loss: 0.3364, Acc : 80.52, mIoU 57.31\n","Epoch time : 38.8s\n","Loss 0.8207,  Acc 80.97,  IoU 57.6545\n","Testing best epoch . . .\n","Step [50/124], Loss: 0.8649, Acc : 80.49, mIoU 56.93\n","Step [100/124], Loss: 0.7242, Acc : 80.96, mIoU 57.31\n","Epoch time : 40.1s\n","Loss 1.4119,  Acc 80.94,  IoU 57.6611\n","Reading patch metadata . . .\n","Done.\n","Dataset ready.\n","Reading patch metadata . . .\n","Done.\n","Dataset ready.\n","Reading patch metadata . . .\n","Done.\n","Dataset ready.\n","Train 362, Val 124, Test 121\n","Model has 1553796 trainable params\n","UNet3D(\n","  (en3): Sequential(\n","    (0): Conv3d(10, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (pool_3): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (en4): Sequential(\n","    (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (pool_4): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (center_in): Sequential(\n","    (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (center_out): Sequential(\n","    (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): ConvTranspose3d(128, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n","  )\n","  (dc4): Sequential(\n","    (0): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (trans3): Sequential(\n","    (0): ConvTranspose3d(64, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n","    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (dc3): Sequential(\n","    (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (4): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (final): Conv3d(16, 20, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",")\n","EPOCH 1/100\n","Step [50/362], Loss: 1.7103, Acc : 34.52, mIoU 3.79\n","Step [100/362], Loss: 1.6472, Acc : 41.78, mIoU 4.98\n","Step [150/362], Loss: 1.8751, Acc : 46.99, mIoU 5.97\n","Step [200/362], Loss: 1.2533, Acc : 50.07, mIoU 6.79\n","Step [250/362], Loss: 1.3268, Acc : 52.49, mIoU 7.56\n","Step [300/362], Loss: 1.0170, Acc : 54.38, mIoU 8.73\n","Step [350/362], Loss: 0.6618, Acc : 56.20, mIoU 9.84\n","Epoch time : 326.4s\n","Validation . . . \n","Step [50/124], Loss: 0.9254, Acc : 67.69, mIoU 17.61\n","Step [100/124], Loss: 1.0227, Acc : 67.10, mIoU 17.16\n","Epoch time : 40.7s\n","Loss 1.2516,  Acc 66.90,  IoU 17.2216\n","EPOCH 2/100\n","Step [50/362], Loss: 1.1418, Acc : 66.73, mIoU 18.14\n","Step [100/362], Loss: 1.3969, Acc : 67.41, mIoU 18.96\n","Step [150/362], Loss: 0.6826, Acc : 68.19, mIoU 19.52\n","Step [200/362], Loss: 0.8352, Acc : 68.07, mIoU 20.23\n","Step [250/362], Loss: 1.1096, Acc : 68.56, mIoU 20.92\n","Step [300/362], Loss: 1.0217, Acc : 68.74, mIoU 21.74\n","Step [350/362], Loss: 0.9932, Acc : 68.99, mIoU 22.78\n","Epoch time : 325.3s\n","Validation . . . \n","Step [50/124], Loss: 0.8016, Acc : 73.84, mIoU 29.01\n","Step [100/124], Loss: 0.6994, Acc : 72.37, mIoU 28.92\n","Epoch time : 40.9s\n","Loss 1.0591,  Acc 71.99,  IoU 28.5740\n","EPOCH 3/100\n","Step [50/362], Loss: 0.6490, Acc : 71.84, mIoU 29.32\n","Step [100/362], Loss: 1.0081, Acc : 71.28, mIoU 28.30\n","Step [150/362], Loss: 0.8187, Acc : 71.46, mIoU 28.18\n","Step [200/362], Loss: 0.6125, Acc : 71.62, mIoU 28.70\n","Step [250/362], Loss: 0.7489, Acc : 71.61, mIoU 28.60\n","Step [300/362], Loss: 1.0779, Acc : 71.73, mIoU 28.60\n","Step [350/362], Loss: 1.2977, Acc : 71.92, mIoU 28.95\n","Epoch time : 325.5s\n","Validation . . . \n","Step [50/124], Loss: 1.1413, Acc : 69.17, mIoU 27.47\n","Step [100/124], Loss: 0.9461, Acc : 70.66, mIoU 27.86\n","Epoch time : 41.2s\n","Loss 0.8339,  Acc 70.76,  IoU 27.9266\n","EPOCH 4/100\n","Step [50/362], Loss: 0.9571, Acc : 74.72, mIoU 33.28\n","Step [100/362], Loss: 0.7770, Acc : 73.83, mIoU 32.59\n","Step [150/362], Loss: 0.5678, Acc : 73.95, mIoU 33.32\n","Step [200/362], Loss: 0.9229, Acc : 74.10, mIoU 33.15\n","Step [250/362], Loss: 0.7968, Acc : 74.12, mIoU 33.27\n","Step [300/362], Loss: 0.5685, Acc : 74.03, mIoU 33.30\n","Step [350/362], Loss: 0.6434, Acc : 73.95, mIoU 33.57\n","Epoch time : 325.8s\n","Validation . . . \n","Step [50/124], Loss: 0.9021, Acc : 73.82, mIoU 33.03\n","Step [100/124], Loss: 1.0099, Acc : 73.97, mIoU 34.80\n","Epoch time : 41.5s\n","Loss 0.5108,  Acc 73.95,  IoU 34.5533\n","EPOCH 5/100\n","Step [50/362], Loss: 0.6620, Acc : 73.89, mIoU 35.80\n","Step [100/362], Loss: 0.7864, Acc : 74.23, mIoU 34.78\n","Step [150/362], Loss: 0.7332, Acc : 74.20, mIoU 35.59\n","Step [200/362], Loss: 1.0051, Acc : 74.50, mIoU 35.87\n","Step [250/362], Loss: 0.6966, Acc : 74.75, mIoU 35.98\n","Step [300/362], Loss: 0.6046, Acc : 74.83, mIoU 36.27\n","Step [350/362], Loss: 0.6447, Acc : 75.07, mIoU 36.73\n","Epoch time : 322.3s\n","Validation . . . \n","Step [50/124], Loss: 0.5073, Acc : 76.85, mIoU 38.53\n","Step [100/124], Loss: 0.5330, Acc : 76.33, mIoU 38.62\n","Epoch time : 40.9s\n","Loss 0.9408,  Acc 76.13,  IoU 38.3766\n","EPOCH 6/100\n","Step [50/362], Loss: 0.6201, Acc : 74.86, mIoU 37.83\n","Step [100/362], Loss: 0.6149, Acc : 76.26, mIoU 38.93\n","Step [150/362], Loss: 0.8068, Acc : 76.31, mIoU 39.13\n","Step [200/362], Loss: 0.8128, Acc : 76.35, mIoU 39.72\n","Step [250/362], Loss: 0.5797, Acc : 76.39, mIoU 39.77\n","Step [300/362], Loss: 0.5158, Acc : 76.36, mIoU 40.10\n","Step [350/362], Loss: 0.6960, Acc : 76.47, mIoU 40.16\n","Epoch time : 328.2s\n","Validation . . . \n","Step [50/124], Loss: 0.9660, Acc : 76.51, mIoU 40.51\n","Step [100/124], Loss: 0.5940, Acc : 76.36, mIoU 40.18\n","Epoch time : 41.1s\n","Loss 0.7368,  Acc 76.03,  IoU 39.9451\n","EPOCH 7/100\n","Step [50/362], Loss: 0.5495, Acc : 76.62, mIoU 40.97\n","Step [100/362], Loss: 0.4972, Acc : 76.73, mIoU 40.90\n","Step [150/362], Loss: 0.6958, Acc : 77.15, mIoU 41.65\n","Step [200/362], Loss: 0.6198, Acc : 77.13, mIoU 41.67\n","Step [250/362], Loss: 0.8475, Acc : 77.02, mIoU 41.78\n","Step [300/362], Loss: 0.6041, Acc : 77.11, mIoU 42.19\n","Step [350/362], Loss: 0.5696, Acc : 77.07, mIoU 42.41\n","Epoch time : 326.3s\n","Validation . . . \n","Step [50/124], Loss: 0.6853, Acc : 75.02, mIoU 42.09\n","Step [100/124], Loss: 0.7216, Acc : 75.12, mIoU 41.43\n","Epoch time : 40.6s\n","Loss 0.6448,  Acc 75.56,  IoU 41.9609\n","EPOCH 8/100\n","Step [50/362], Loss: 1.0381, Acc : 76.29, mIoU 42.35\n","Step [100/362], Loss: 0.8325, Acc : 77.45, mIoU 43.37\n","Step [150/362], Loss: 0.6267, Acc : 77.18, mIoU 43.53\n","Step [200/362], Loss: 0.8021, Acc : 77.32, mIoU 43.85\n","Step [250/362], Loss: 0.7151, Acc : 77.38, mIoU 43.90\n","Step [300/362], Loss: 0.7276, Acc : 77.67, mIoU 44.29\n","Step [350/362], Loss: 0.8253, Acc : 77.85, mIoU 44.41\n","Epoch time : 326.5s\n","Validation . . . \n","Step [50/124], Loss: 0.9904, Acc : 75.62, mIoU 42.87\n","Step [100/124], Loss: 0.9947, Acc : 76.46, mIoU 44.46\n","Epoch time : 41.2s\n","Loss 0.5471,  Acc 76.68,  IoU 44.7974\n","EPOCH 9/100\n","Step [50/362], Loss: 0.5759, Acc : 78.60, mIoU 45.54\n","Step [100/362], Loss: 0.8786, Acc : 77.90, mIoU 45.00\n","Step [150/362], Loss: 0.4845, Acc : 78.00, mIoU 45.63\n","Step [200/362], Loss: 0.4484, Acc : 78.47, mIoU 46.07\n","Step [250/362], Loss: 0.4415, Acc : 78.27, mIoU 46.12\n","Step [300/362], Loss: 0.9145, Acc : 78.20, mIoU 46.11\n","Step [350/362], Loss: 0.7421, Acc : 78.25, mIoU 46.15\n","Epoch time : 324.1s\n","Validation . . . \n","Step [50/124], Loss: 0.7845, Acc : 77.30, mIoU 43.25\n","Step [100/124], Loss: 0.9640, Acc : 76.65, mIoU 43.71\n","Epoch time : 41.9s\n","Loss 0.6733,  Acc 76.81,  IoU 43.8928\n","EPOCH 10/100\n","Step [50/362], Loss: 0.5580, Acc : 77.69, mIoU 45.19\n","Step [100/362], Loss: 0.3590, Acc : 78.36, mIoU 46.73\n","Step [150/362], Loss: 0.6445, Acc : 78.49, mIoU 46.93\n","Step [200/362], Loss: 0.5440, Acc : 78.78, mIoU 47.65\n","Step [250/362], Loss: 0.5733, Acc : 78.76, mIoU 47.33\n","Step [300/362], Loss: 0.5509, Acc : 78.59, mIoU 47.52\n","Step [350/362], Loss: 0.6078, Acc : 78.72, mIoU 47.66\n","Epoch time : 322.8s\n","Validation . . . \n","Step [50/124], Loss: 0.7359, Acc : 78.05, mIoU 48.80\n","Step [100/124], Loss: 0.3693, Acc : 77.91, mIoU 47.18\n","Epoch time : 41.4s\n","Loss 1.0413,  Acc 77.78,  IoU 47.3446\n","EPOCH 11/100\n","Step [50/362], Loss: 0.4897, Acc : 80.27, mIoU 49.76\n","Step [100/362], Loss: 0.6414, Acc : 79.54, mIoU 48.54\n","Step [150/362], Loss: 0.7432, Acc : 78.77, mIoU 48.13\n","Step [200/362], Loss: 0.4941, Acc : 78.86, mIoU 49.07\n","Step [250/362], Loss: 0.6705, Acc : 78.75, mIoU 48.88\n","Step [300/362], Loss: 0.5446, Acc : 78.85, mIoU 48.84\n","Step [350/362], Loss: 0.5514, Acc : 78.95, mIoU 48.76\n","Epoch time : 324.2s\n","Validation . . . \n","Step [50/124], Loss: 1.1199, Acc : 78.85, mIoU 47.85\n","Step [100/124], Loss: 0.6714, Acc : 78.07, mIoU 47.57\n","Epoch time : 40.7s\n","Loss 0.6617,  Acc 77.86,  IoU 47.3720\n","EPOCH 12/100\n","Step [50/362], Loss: 0.7608, Acc : 78.45, mIoU 47.12\n","Step [100/362], Loss: 0.4449, Acc : 78.64, mIoU 49.16\n","Step [150/362], Loss: 0.5849, Acc : 79.14, mIoU 49.19\n","Step [200/362], Loss: 0.6960, Acc : 78.76, mIoU 49.02\n","Step [250/362], Loss: 0.4169, Acc : 79.00, mIoU 49.61\n","Step [300/362], Loss: 0.6224, Acc : 79.31, mIoU 49.84\n","Step [350/362], Loss: 0.5775, Acc : 79.34, mIoU 49.92\n","Epoch time : 324.5s\n","Validation . . . \n","Step [50/124], Loss: 0.5147, Acc : 78.11, mIoU 45.56\n","Step [100/124], Loss: 0.5877, Acc : 78.48, mIoU 47.49\n","Epoch time : 42.0s\n","Loss 1.0134,  Acc 78.13,  IoU 46.8827\n","EPOCH 13/100\n","Step [50/362], Loss: 0.6696, Acc : 79.42, mIoU 51.12\n","Step [100/362], Loss: 0.5184, Acc : 80.20, mIoU 51.20\n","Step [150/362], Loss: 0.8490, Acc : 79.82, mIoU 50.71\n","Step [200/362], Loss: 0.5454, Acc : 79.77, mIoU 51.23\n","Step [250/362], Loss: 0.7943, Acc : 79.85, mIoU 51.22\n","Step [300/362], Loss: 0.4808, Acc : 79.71, mIoU 50.95\n","Step [350/362], Loss: 0.7274, Acc : 79.65, mIoU 50.62\n","Epoch time : 323.8s\n","Validation . . . \n","Step [50/124], Loss: 0.7965, Acc : 79.28, mIoU 50.34\n","Step [100/124], Loss: 0.5079, Acc : 79.18, mIoU 50.31\n","Epoch time : 41.7s\n","Loss 0.4035,  Acc 79.31,  IoU 50.4771\n","EPOCH 14/100\n","Step [50/362], Loss: 0.7752, Acc : 79.49, mIoU 52.41\n","Step [100/362], Loss: 0.4525, Acc : 79.55, mIoU 53.56\n","Step [150/362], Loss: 0.7990, Acc : 79.68, mIoU 52.93\n","Step [200/362], Loss: 0.6209, Acc : 79.72, mIoU 52.64\n","Step [250/362], Loss: 0.3898, Acc : 79.93, mIoU 52.95\n","Step [300/362], Loss: 0.4368, Acc : 79.89, mIoU 52.71\n","Step [350/362], Loss: 0.3612, Acc : 80.05, mIoU 52.63\n","Epoch time : 323.9s\n","Validation . . . \n","Step [50/124], Loss: 0.4491, Acc : 78.45, mIoU 50.33\n","Step [100/124], Loss: 0.6111, Acc : 79.56, mIoU 51.76\n","Epoch time : 41.1s\n","Loss 0.9974,  Acc 79.26,  IoU 50.6674\n","EPOCH 15/100\n","Step [50/362], Loss: 0.5970, Acc : 80.22, mIoU 53.81\n","Step [100/362], Loss: 0.8350, Acc : 80.09, mIoU 53.70\n","Step [150/362], Loss: 0.5610, Acc : 80.29, mIoU 53.56\n","Step [200/362], Loss: 0.5055, Acc : 80.36, mIoU 53.99\n","Step [250/362], Loss: 0.4389, Acc : 80.42, mIoU 54.04\n","Step [300/362], Loss: 0.7121, Acc : 80.43, mIoU 53.87\n","Step [350/362], Loss: 0.4530, Acc : 80.38, mIoU 53.67\n","Epoch time : 321.7s\n","Validation . . . \n","Step [50/124], Loss: 0.3907, Acc : 79.32, mIoU 50.90\n","Step [100/124], Loss: 0.4619, Acc : 79.89, mIoU 51.71\n","Epoch time : 41.1s\n","Loss 0.7339,  Acc 79.87,  IoU 51.4180\n","EPOCH 16/100\n","Step [50/362], Loss: 0.6086, Acc : 79.29, mIoU 52.62\n","Step [100/362], Loss: 0.4501, Acc : 79.84, mIoU 53.56\n","Step [150/362], Loss: 0.3972, Acc : 80.22, mIoU 53.63\n","Step [200/362], Loss: 0.5623, Acc : 80.43, mIoU 54.52\n","Step [250/362], Loss: 0.4568, Acc : 80.44, mIoU 54.27\n","Step [300/362], Loss: 0.4593, Acc : 80.59, mIoU 54.40\n","Step [350/362], Loss: 0.6725, Acc : 80.52, mIoU 54.39\n","Epoch time : 322.4s\n","Validation . . . \n","Step [50/124], Loss: 0.5554, Acc : 79.14, mIoU 50.33\n","Step [100/124], Loss: 0.7069, Acc : 79.00, mIoU 50.78\n","Epoch time : 41.3s\n","Loss 0.5066,  Acc 78.82,  IoU 51.0711\n","EPOCH 17/100\n","Step [50/362], Loss: 0.4385, Acc : 80.00, mIoU 53.92\n","Step [100/362], Loss: 0.4434, Acc : 80.71, mIoU 54.91\n","Step [150/362], Loss: 0.6510, Acc : 80.70, mIoU 54.45\n","Step [200/362], Loss: 0.7371, Acc : 80.72, mIoU 54.83\n","Step [250/362], Loss: 0.5099, Acc : 80.77, mIoU 54.48\n","Step [300/362], Loss: 0.6407, Acc : 80.60, mIoU 54.05\n","Step [350/362], Loss: 0.4063, Acc : 80.64, mIoU 54.42\n","Epoch time : 323.6s\n","Validation . . . \n","Step [50/124], Loss: 0.5764, Acc : 79.66, mIoU 50.12\n","Step [100/124], Loss: 0.5918, Acc : 79.19, mIoU 51.42\n","Epoch time : 41.0s\n","Loss 0.4965,  Acc 79.29,  IoU 51.0777\n","EPOCH 18/100\n","Step [50/362], Loss: 0.4269, Acc : 80.64, mIoU 56.90\n","Step [100/362], Loss: 0.5275, Acc : 80.68, mIoU 56.61\n","Step [150/362], Loss: 0.4942, Acc : 80.87, mIoU 55.68\n","Step [200/362], Loss: 0.6572, Acc : 80.94, mIoU 55.63\n","Step [250/362], Loss: 0.5503, Acc : 80.63, mIoU 55.48\n","Step [300/362], Loss: 0.4733, Acc : 80.75, mIoU 55.46\n","Step [350/362], Loss: 0.4356, Acc : 80.93, mIoU 55.52\n","Epoch time : 321.3s\n","Validation . . . \n","Step [50/124], Loss: 0.4178, Acc : 79.93, mIoU 51.03\n","Step [100/124], Loss: 0.5024, Acc : 80.27, mIoU 52.33\n","Epoch time : 40.2s\n","Loss 0.3762,  Acc 80.18,  IoU 52.2850\n","EPOCH 19/100\n","Step [50/362], Loss: 0.4921, Acc : 81.57, mIoU 57.71\n","Step [100/362], Loss: 0.4004, Acc : 81.71, mIoU 57.33\n","Step [150/362], Loss: 0.5543, Acc : 81.66, mIoU 57.18\n","Step [200/362], Loss: 0.6238, Acc : 81.33, mIoU 56.11\n","Step [250/362], Loss: 0.6279, Acc : 81.42, mIoU 56.09\n","Step [300/362], Loss: 0.5419, Acc : 81.21, mIoU 56.24\n","Step [350/362], Loss: 0.5699, Acc : 81.14, mIoU 56.27\n","Epoch time : 324.4s\n","Validation . . . \n","Step [50/124], Loss: 0.6406, Acc : 80.01, mIoU 48.86\n","Step [100/124], Loss: 0.6654, Acc : 80.11, mIoU 52.32\n","Epoch time : 40.8s\n","Loss 0.8795,  Acc 79.98,  IoU 52.7311\n","EPOCH 20/100\n","Step [50/362], Loss: 0.4851, Acc : 82.48, mIoU 58.76\n","Step [100/362], Loss: 0.3828, Acc : 82.19, mIoU 57.79\n","Step [150/362], Loss: 0.5409, Acc : 81.85, mIoU 57.95\n","Step [200/362], Loss: 0.4675, Acc : 81.87, mIoU 57.64\n","Step [250/362], Loss: 0.5690, Acc : 81.73, mIoU 57.43\n","Step [300/362], Loss: 0.5101, Acc : 81.66, mIoU 57.23\n","Step [350/362], Loss: 0.3583, Acc : 81.47, mIoU 57.00\n","Epoch time : 323.8s\n","Validation . . . \n","Step [50/124], Loss: 0.6526, Acc : 79.82, mIoU 53.51\n","Step [100/124], Loss: 0.4390, Acc : 79.91, mIoU 52.85\n","Epoch time : 40.7s\n","Loss 0.4386,  Acc 79.55,  IoU 52.0409\n","EPOCH 21/100\n","Step [50/362], Loss: 0.5053, Acc : 81.06, mIoU 55.71\n","Step [100/362], Loss: 0.4661, Acc : 80.98, mIoU 55.76\n","Step [150/362], Loss: 0.5218, Acc : 81.37, mIoU 56.45\n","Step [200/362], Loss: 0.6693, Acc : 81.48, mIoU 56.96\n","Step [250/362], Loss: 0.6251, Acc : 81.67, mIoU 57.20\n","Step [300/362], Loss: 0.6071, Acc : 81.62, mIoU 57.25\n","Step [350/362], Loss: 0.6501, Acc : 81.59, mIoU 57.33\n","Epoch time : 321.6s\n","Validation . . . \n","Step [50/124], Loss: 0.4986, Acc : 79.76, mIoU 53.10\n","Step [100/124], Loss: 0.5898, Acc : 80.38, mIoU 52.58\n","Epoch time : 41.6s\n","Loss 0.4873,  Acc 80.39,  IoU 53.1281\n","EPOCH 22/100\n","Step [50/362], Loss: 0.4116, Acc : 82.59, mIoU 59.62\n","Step [100/362], Loss: 0.5147, Acc : 82.33, mIoU 59.29\n","Step [150/362], Loss: 0.3355, Acc : 82.18, mIoU 59.74\n","Step [200/362], Loss: 0.7459, Acc : 81.98, mIoU 59.22\n","Step [250/362], Loss: 0.8068, Acc : 81.82, mIoU 58.64\n","Step [300/362], Loss: 0.3340, Acc : 81.75, mIoU 58.07\n","Step [350/362], Loss: 0.5023, Acc : 81.73, mIoU 58.17\n","Epoch time : 322.3s\n","Validation . . . \n","Step [50/124], Loss: 0.2759, Acc : 79.22, mIoU 51.71\n","Step [100/124], Loss: 0.6483, Acc : 80.17, mIoU 52.55\n","Epoch time : 43.8s\n","Loss 0.5650,  Acc 80.53,  IoU 53.3132\n","EPOCH 23/100\n","Step [50/362], Loss: 0.6086, Acc : 82.40, mIoU 59.11\n","Step [100/362], Loss: 0.5722, Acc : 82.22, mIoU 60.68\n","Step [150/362], Loss: 0.5698, Acc : 82.02, mIoU 60.16\n","Step [200/362], Loss: 0.3636, Acc : 82.00, mIoU 59.94\n","Step [250/362], Loss: 0.5500, Acc : 82.01, mIoU 59.76\n","Step [300/362], Loss: 0.3839, Acc : 81.90, mIoU 59.25\n","Step [350/362], Loss: 0.4633, Acc : 81.92, mIoU 58.69\n","Epoch time : 327.5s\n","Validation . . . \n","Step [50/124], Loss: 0.4369, Acc : 79.41, mIoU 50.66\n","Step [100/124], Loss: 0.5722, Acc : 79.42, mIoU 52.49\n","Epoch time : 43.0s\n","Loss 0.7683,  Acc 79.28,  IoU 52.2570\n","EPOCH 24/100\n","Step [50/362], Loss: 0.2894, Acc : 83.17, mIoU 60.93\n","Step [100/362], Loss: 0.4388, Acc : 83.03, mIoU 62.13\n","Step [150/362], Loss: 0.6342, Acc : 82.36, mIoU 60.34\n","Step [200/362], Loss: 0.6400, Acc : 82.22, mIoU 59.97\n","Step [250/362], Loss: 0.6797, Acc : 82.34, mIoU 60.27\n","Step [300/362], Loss: 0.6339, Acc : 82.15, mIoU 59.90\n","Step [350/362], Loss: 0.4706, Acc : 82.13, mIoU 59.55\n","Epoch time : 322.5s\n","Validation . . . \n","Step [50/124], Loss: 0.6711, Acc : 81.34, mIoU 53.76\n","Step [100/124], Loss: 0.4748, Acc : 81.25, mIoU 53.77\n","Epoch time : 41.9s\n","Loss 0.4714,  Acc 80.65,  IoU 53.8111\n","EPOCH 25/100\n","Step [50/362], Loss: 0.4712, Acc : 83.47, mIoU 60.49\n","Step [100/362], Loss: 0.6486, Acc : 83.21, mIoU 61.54\n","Step [150/362], Loss: 0.6511, Acc : 82.70, mIoU 60.04\n","Step [200/362], Loss: 0.5005, Acc : 82.43, mIoU 60.17\n","Step [250/362], Loss: 0.4670, Acc : 82.40, mIoU 60.05\n","Step [300/362], Loss: 0.4512, Acc : 82.33, mIoU 59.87\n","Step [350/362], Loss: 0.5646, Acc : 82.37, mIoU 59.71\n","Epoch time : 324.4s\n","Validation . . . \n","Step [50/124], Loss: 0.7318, Acc : 80.77, mIoU 52.73\n","Step [100/124], Loss: 0.6447, Acc : 80.73, mIoU 52.04\n","Epoch time : 41.7s\n","Loss 0.7096,  Acc 80.26,  IoU 51.6621\n","EPOCH 26/100\n","Step [50/362], Loss: 0.4291, Acc : 82.19, mIoU 58.00\n","Step [100/362], Loss: 0.4335, Acc : 82.99, mIoU 59.68\n","Step [150/362], Loss: 0.5376, Acc : 83.17, mIoU 60.10\n","Step [200/362], Loss: 0.6078, Acc : 83.09, mIoU 61.00\n","Step [250/362], Loss: 0.5725, Acc : 82.79, mIoU 60.40\n","Step [300/362], Loss: 0.6531, Acc : 82.74, mIoU 60.14\n","Step [350/362], Loss: 0.5240, Acc : 82.55, mIoU 60.11\n","Epoch time : 323.7s\n","Validation . . . \n","Step [50/124], Loss: 0.3967, Acc : 79.78, mIoU 54.84\n","Step [100/124], Loss: 0.6156, Acc : 80.49, mIoU 54.83\n","Epoch time : 40.6s\n","Loss 0.6480,  Acc 80.43,  IoU 54.6522\n","EPOCH 27/100\n","Step [50/362], Loss: 0.4718, Acc : 82.84, mIoU 61.99\n","Step [100/362], Loss: 0.4267, Acc : 83.11, mIoU 62.28\n","Step [150/362], Loss: 0.4978, Acc : 83.03, mIoU 62.56\n","Step [200/362], Loss: 0.5475, Acc : 83.22, mIoU 62.36\n","Step [250/362], Loss: 0.5121, Acc : 83.12, mIoU 62.29\n","Step [300/362], Loss: 0.3325, Acc : 82.98, mIoU 62.32\n","Step [350/362], Loss: 0.5340, Acc : 83.09, mIoU 62.38\n","Epoch time : 320.2s\n","Validation . . . \n","Step [50/124], Loss: 0.7617, Acc : 80.07, mIoU 53.53\n","Step [100/124], Loss: 0.5808, Acc : 80.10, mIoU 53.83\n","Epoch time : 41.6s\n","Loss 0.5185,  Acc 80.19,  IoU 53.6012\n","EPOCH 28/100\n","Step [50/362], Loss: 0.3575, Acc : 83.15, mIoU 63.65\n","Step [100/362], Loss: 0.4309, Acc : 83.12, mIoU 62.87\n","Step [150/362], Loss: 0.4462, Acc : 82.96, mIoU 61.74\n","Step [200/362], Loss: 0.3206, Acc : 82.90, mIoU 61.53\n","Step [250/362], Loss: 0.3002, Acc : 82.86, mIoU 61.36\n","Step [300/362], Loss: 0.4680, Acc : 82.97, mIoU 61.74\n","Step [350/362], Loss: 0.3655, Acc : 83.04, mIoU 61.77\n","Epoch time : 320.7s\n","Validation . . . \n","Step [50/124], Loss: 0.7723, Acc : 80.28, mIoU 53.63\n","Step [100/124], Loss: 0.4715, Acc : 80.44, mIoU 53.99\n","Epoch time : 41.7s\n","Loss 0.3233,  Acc 80.38,  IoU 54.2155\n","EPOCH 29/100\n","Step [50/362], Loss: 0.4241, Acc : 83.85, mIoU 62.22\n","Step [100/362], Loss: 0.7015, Acc : 84.13, mIoU 63.31\n","Step [150/362], Loss: 0.3057, Acc : 83.87, mIoU 63.00\n","Step [200/362], Loss: 0.3664, Acc : 83.58, mIoU 62.78\n","Step [250/362], Loss: 0.7468, Acc : 83.41, mIoU 62.63\n","Step [300/362], Loss: 0.4553, Acc : 83.43, mIoU 62.58\n","Step [350/362], Loss: 0.4151, Acc : 83.28, mIoU 62.66\n","Epoch time : 323.4s\n","Validation . . . \n","Step [50/124], Loss: 0.4095, Acc : 80.16, mIoU 54.95\n","Step [100/124], Loss: 0.5439, Acc : 80.70, mIoU 54.50\n","Epoch time : 41.1s\n","Loss 0.6474,  Acc 80.63,  IoU 54.2812\n","EPOCH 30/100\n","Step [50/362], Loss: 0.3593, Acc : 83.80, mIoU 63.11\n","Step [100/362], Loss: 0.2979, Acc : 83.50, mIoU 63.00\n","Step [150/362], Loss: 0.5128, Acc : 83.67, mIoU 63.76\n","Step [200/362], Loss: 0.2823, Acc : 83.74, mIoU 63.94\n","Step [250/362], Loss: 0.3549, Acc : 83.39, mIoU 63.28\n","Step [300/362], Loss: 0.4226, Acc : 83.50, mIoU 63.24\n","Step [350/362], Loss: 0.4232, Acc : 83.40, mIoU 63.20\n","Epoch time : 322.9s\n","Validation . . . \n","Step [50/124], Loss: 0.4911, Acc : 80.18, mIoU 54.44\n","Step [100/124], Loss: 0.5142, Acc : 80.64, mIoU 56.32\n","Epoch time : 42.7s\n","Loss 0.7597,  Acc 80.19,  IoU 55.8297\n","EPOCH 31/100\n","Step [50/362], Loss: 0.4762, Acc : 83.08, mIoU 62.04\n","Step [100/362], Loss: 0.2928, Acc : 83.32, mIoU 62.86\n","Step [150/362], Loss: 0.5453, Acc : 83.50, mIoU 63.96\n","Step [200/362], Loss: 0.5547, Acc : 83.63, mIoU 63.62\n","Step [250/362], Loss: 0.5156, Acc : 83.50, mIoU 63.89\n","Step [300/362], Loss: 0.4790, Acc : 83.42, mIoU 63.44\n","Step [350/362], Loss: 0.3724, Acc : 83.37, mIoU 63.32\n","Epoch time : 327.0s\n","Validation . . . \n","Step [50/124], Loss: 0.7836, Acc : 80.08, mIoU 55.99\n","Step [100/124], Loss: 0.5266, Acc : 80.79, mIoU 56.39\n","Epoch time : 43.5s\n","Loss 0.8466,  Acc 80.94,  IoU 56.1079\n","EPOCH 32/100\n","Step [50/362], Loss: 0.5586, Acc : 84.04, mIoU 64.79\n","Step [100/362], Loss: 0.4360, Acc : 84.01, mIoU 64.79\n","Step [150/362], Loss: 0.2962, Acc : 83.99, mIoU 64.99\n","Step [200/362], Loss: 0.3749, Acc : 83.84, mIoU 64.93\n","Step [250/362], Loss: 0.2529, Acc : 83.68, mIoU 64.46\n","Step [300/362], Loss: 0.4240, Acc : 83.69, mIoU 64.69\n","Step [350/362], Loss: 0.3843, Acc : 83.81, mIoU 64.66\n","Epoch time : 328.2s\n","Validation . . . \n","Step [50/124], Loss: 0.8074, Acc : 80.59, mIoU 56.67\n","Step [100/124], Loss: 0.8733, Acc : 81.01, mIoU 55.84\n","Epoch time : 41.8s\n","Loss 0.5832,  Acc 81.02,  IoU 55.4878\n","EPOCH 33/100\n","Step [50/362], Loss: 0.4948, Acc : 84.26, mIoU 65.59\n","Step [100/362], Loss: 0.4010, Acc : 84.49, mIoU 66.57\n","Step [150/362], Loss: 0.4394, Acc : 84.22, mIoU 66.01\n","Step [200/362], Loss: 0.4341, Acc : 84.13, mIoU 65.40\n","Step [250/362], Loss: 0.3120, Acc : 84.20, mIoU 65.21\n","Step [300/362], Loss: 0.4905, Acc : 84.05, mIoU 64.87\n","Step [350/362], Loss: 0.4959, Acc : 84.05, mIoU 64.97\n","Epoch time : 327.1s\n","Validation . . . \n","Step [50/124], Loss: 0.5140, Acc : 81.38, mIoU 56.08\n","Step [100/124], Loss: 0.6947, Acc : 80.48, mIoU 54.33\n","Epoch time : 44.0s\n","Loss 0.5411,  Acc 80.42,  IoU 54.2441\n","EPOCH 34/100\n","Step [50/362], Loss: 0.5535, Acc : 82.75, mIoU 65.38\n","Step [100/362], Loss: 0.4926, Acc : 83.40, mIoU 65.14\n","Step [150/362], Loss: 0.5547, Acc : 83.95, mIoU 65.43\n","Step [200/362], Loss: 0.4747, Acc : 84.12, mIoU 65.24\n","Step [250/362], Loss: 0.3987, Acc : 84.15, mIoU 65.37\n","Step [300/362], Loss: 0.3458, Acc : 84.21, mIoU 65.13\n","Step [350/362], Loss: 0.3364, Acc : 84.23, mIoU 65.12\n","Epoch time : 327.2s\n","Validation . . . \n","Step [50/124], Loss: 0.3820, Acc : 81.05, mIoU 53.87\n","Step [100/124], Loss: 0.5168, Acc : 80.90, mIoU 55.10\n","Epoch time : 39.9s\n","Loss 0.6186,  Acc 80.86,  IoU 54.9980\n","EPOCH 35/100\n","Step [50/362], Loss: 0.5937, Acc : 84.78, mIoU 64.93\n","Step [100/362], Loss: 0.4116, Acc : 84.49, mIoU 65.76\n","Step [150/362], Loss: 0.3906, Acc : 84.55, mIoU 65.73\n","Step [200/362], Loss: 0.4153, Acc : 84.49, mIoU 66.30\n","Step [250/362], Loss: 0.3310, Acc : 84.33, mIoU 66.17\n","Step [300/362], Loss: 0.4180, Acc : 84.29, mIoU 65.89\n","Step [350/362], Loss: 0.4995, Acc : 84.24, mIoU 65.73\n","Epoch time : 320.6s\n","Validation . . . \n","Step [50/124], Loss: 0.7567, Acc : 80.02, mIoU 54.44\n","Step [100/124], Loss: 0.3950, Acc : 80.11, mIoU 54.09\n","Epoch time : 41.3s\n","Loss 0.8193,  Acc 80.24,  IoU 53.9129\n","EPOCH 36/100\n","Step [50/362], Loss: 0.4088, Acc : 85.23, mIoU 68.85\n","Step [100/362], Loss: 0.4500, Acc : 85.05, mIoU 68.59\n","Step [150/362], Loss: 0.4920, Acc : 84.77, mIoU 67.45\n","Step [200/362], Loss: 0.4747, Acc : 84.55, mIoU 66.82\n","Step [250/362], Loss: 0.4209, Acc : 84.44, mIoU 66.41\n","Step [300/362], Loss: 0.3660, Acc : 84.58, mIoU 66.96\n","Step [350/362], Loss: 0.4872, Acc : 84.43, mIoU 66.51\n","Epoch time : 322.9s\n","Validation . . . \n","Step [50/124], Loss: 0.7676, Acc : 79.49, mIoU 53.96\n","Step [100/124], Loss: 0.7911, Acc : 80.45, mIoU 54.84\n","Epoch time : 41.3s\n","Loss 0.6117,  Acc 80.83,  IoU 55.2399\n","EPOCH 37/100\n","Step [50/362], Loss: 0.4200, Acc : 84.31, mIoU 65.78\n","Step [100/362], Loss: 0.3099, Acc : 84.40, mIoU 65.95\n","Step [150/362], Loss: 0.3986, Acc : 84.55, mIoU 66.49\n","Step [200/362], Loss: 0.4004, Acc : 84.67, mIoU 66.86\n","Step [250/362], Loss: 0.4636, Acc : 84.83, mIoU 66.86\n","Step [300/362], Loss: 0.5449, Acc : 84.74, mIoU 66.76\n","Step [350/362], Loss: 0.5207, Acc : 84.69, mIoU 66.66\n","Epoch time : 323.3s\n","Validation . . . \n","Step [50/124], Loss: 0.6466, Acc : 81.88, mIoU 57.75\n","Step [100/124], Loss: 0.4446, Acc : 81.00, mIoU 56.94\n","Epoch time : 41.0s\n","Loss 0.6482,  Acc 81.12,  IoU 56.7660\n","EPOCH 38/100\n","Step [50/362], Loss: 0.3611, Acc : 85.70, mIoU 68.04\n","Step [100/362], Loss: 0.4734, Acc : 85.59, mIoU 68.02\n","Step [150/362], Loss: 0.3346, Acc : 85.25, mIoU 68.27\n","Step [200/362], Loss: 0.2990, Acc : 85.25, mIoU 68.31\n","Step [250/362], Loss: 0.3719, Acc : 85.07, mIoU 68.27\n","Step [300/362], Loss: 0.3717, Acc : 84.97, mIoU 67.83\n","Step [350/362], Loss: 0.4016, Acc : 84.85, mIoU 67.49\n","Epoch time : 322.5s\n","Validation . . . \n","Step [50/124], Loss: 0.4984, Acc : 80.47, mIoU 54.92\n","Step [100/124], Loss: 0.2796, Acc : 81.02, mIoU 55.26\n","Epoch time : 40.9s\n","Loss 0.5537,  Acc 81.12,  IoU 55.9295\n","EPOCH 39/100\n","Step [50/362], Loss: 0.4219, Acc : 85.07, mIoU 68.61\n","Step [100/362], Loss: 0.4230, Acc : 85.13, mIoU 68.36\n","Step [150/362], Loss: 0.5319, Acc : 84.97, mIoU 68.21\n","Step [200/362], Loss: 0.3754, Acc : 84.78, mIoU 67.86\n","Step [250/362], Loss: 0.3736, Acc : 84.99, mIoU 67.51\n","Step [300/362], Loss: 0.4068, Acc : 85.00, mIoU 67.54\n","Step [350/362], Loss: 0.5501, Acc : 84.84, mIoU 67.35\n","Epoch time : 323.1s\n","Validation . . . \n","Step [50/124], Loss: 0.4531, Acc : 81.06, mIoU 55.72\n","Step [100/124], Loss: 0.5278, Acc : 81.25, mIoU 56.95\n","Epoch time : 40.8s\n","Loss 0.6312,  Acc 81.27,  IoU 57.3621\n","EPOCH 40/100\n","Step [50/362], Loss: 0.3107, Acc : 85.95, mIoU 69.87\n","Step [100/362], Loss: 0.3256, Acc : 85.56, mIoU 68.67\n","Step [150/362], Loss: 0.3210, Acc : 85.68, mIoU 69.12\n","Step [200/362], Loss: 0.4446, Acc : 85.44, mIoU 68.67\n","Step [250/362], Loss: 0.4942, Acc : 85.43, mIoU 68.64\n","Step [300/362], Loss: 0.2585, Acc : 85.28, mIoU 68.47\n","Step [350/362], Loss: 0.7413, Acc : 85.22, mIoU 68.48\n","Epoch time : 308.7s\n","Validation . . . \n","Step [50/124], Loss: 0.4232, Acc : 81.90, mIoU 58.45\n","Step [100/124], Loss: 0.4042, Acc : 81.04, mIoU 57.79\n","Epoch time : 39.6s\n","Loss 0.3196,  Acc 81.42,  IoU 58.0586\n","EPOCH 41/100\n","Step [50/362], Loss: 0.4698, Acc : 86.01, mIoU 69.53\n","Step [100/362], Loss: 0.3213, Acc : 85.55, mIoU 68.93\n","Step [150/362], Loss: 0.3204, Acc : 85.68, mIoU 68.92\n","Step [200/362], Loss: 0.4782, Acc : 85.53, mIoU 68.85\n","Step [250/362], Loss: 0.4091, Acc : 85.42, mIoU 68.90\n","Step [300/362], Loss: 0.4112, Acc : 85.45, mIoU 68.77\n","Step [350/362], Loss: 0.3774, Acc : 85.29, mIoU 68.62\n","Epoch time : 304.8s\n","Validation . . . \n","Step [50/124], Loss: 0.4109, Acc : 81.37, mIoU 58.70\n","Step [100/124], Loss: 0.7220, Acc : 80.76, mIoU 57.37\n","Epoch time : 39.8s\n","Loss 0.5716,  Acc 80.85,  IoU 57.1551\n","EPOCH 42/100\n","Step [50/362], Loss: 0.2632, Acc : 85.56, mIoU 68.70\n","Step [100/362], Loss: 0.3805, Acc : 85.67, mIoU 69.44\n","Step [150/362], Loss: 0.3566, Acc : 85.72, mIoU 69.45\n","Step [200/362], Loss: 0.3171, Acc : 85.88, mIoU 69.80\n","Step [250/362], Loss: 0.3436, Acc : 85.58, mIoU 70.04\n","Step [300/362], Loss: 0.3633, Acc : 85.36, mIoU 69.40\n","Step [350/362], Loss: 0.3830, Acc : 85.43, mIoU 69.44\n","Epoch time : 306.6s\n","Validation . . . \n","Step [50/124], Loss: 0.5971, Acc : 79.64, mIoU 54.14\n","Step [100/124], Loss: 0.7799, Acc : 80.67, mIoU 54.71\n","Epoch time : 40.1s\n","Loss 0.4031,  Acc 80.74,  IoU 55.2965\n","EPOCH 43/100\n","Step [50/362], Loss: 0.3323, Acc : 86.55, mIoU 71.76\n","Step [100/362], Loss: 0.4842, Acc : 86.04, mIoU 70.29\n","Step [150/362], Loss: 0.4122, Acc : 85.50, mIoU 69.22\n","Step [200/362], Loss: 0.4499, Acc : 85.72, mIoU 69.67\n","Step [250/362], Loss: 0.5168, Acc : 85.62, mIoU 69.51\n","Step [300/362], Loss: 0.3587, Acc : 85.57, mIoU 69.25\n","Step [350/362], Loss: 0.4670, Acc : 85.45, mIoU 68.86\n","Epoch time : 305.0s\n","Validation . . . \n","Step [50/124], Loss: 0.4248, Acc : 82.26, mIoU 54.28\n","Step [100/124], Loss: 1.0720, Acc : 80.34, mIoU 53.79\n","Epoch time : 40.1s\n","Loss 0.4688,  Acc 80.27,  IoU 54.1910\n","EPOCH 44/100\n","Step [50/362], Loss: 0.3186, Acc : 86.33, mIoU 68.82\n","Step [100/362], Loss: 0.4271, Acc : 86.00, mIoU 69.55\n","Step [150/362], Loss: 0.5143, Acc : 85.90, mIoU 69.53\n","Step [200/362], Loss: 0.4924, Acc : 85.95, mIoU 70.46\n","Step [250/362], Loss: 0.5420, Acc : 85.89, mIoU 70.56\n","Step [300/362], Loss: 0.3916, Acc : 85.83, mIoU 70.30\n","Step [350/362], Loss: 0.3466, Acc : 85.93, mIoU 70.46\n","Epoch time : 305.7s\n","Validation . . . \n","Step [50/124], Loss: 0.6906, Acc : 80.82, mIoU 57.60\n","Step [100/124], Loss: 0.6761, Acc : 81.48, mIoU 58.18\n","Epoch time : 39.8s\n","Loss 0.7887,  Acc 81.28,  IoU 57.5666\n","EPOCH 45/100\n","Step [50/362], Loss: 0.3333, Acc : 86.58, mIoU 72.37\n","Step [100/362], Loss: 0.2226, Acc : 86.61, mIoU 72.45\n","Step [150/362], Loss: 0.4978, Acc : 86.19, mIoU 71.84\n","Step [200/362], Loss: 0.3894, Acc : 86.31, mIoU 71.86\n","Step [250/362], Loss: 0.3380, Acc : 86.40, mIoU 71.78\n","Step [300/362], Loss: 0.3612, Acc : 86.44, mIoU 71.83\n","Step [350/362], Loss: 0.3696, Acc : 86.24, mIoU 71.21\n","Epoch time : 307.5s\n","Validation . . . \n","Step [50/124], Loss: 0.5963, Acc : 80.28, mIoU 58.65\n","Step [100/124], Loss: 0.8244, Acc : 80.75, mIoU 58.26\n","Epoch time : 40.4s\n","Loss 0.5828,  Acc 80.83,  IoU 57.9827\n","EPOCH 46/100\n","Step [50/362], Loss: 0.2930, Acc : 87.13, mIoU 74.71\n","Step [100/362], Loss: 0.3733, Acc : 86.85, mIoU 73.68\n","Step [150/362], Loss: 0.3862, Acc : 86.75, mIoU 73.10\n","Step [200/362], Loss: 0.3893, Acc : 86.59, mIoU 72.18\n","Step [250/362], Loss: 0.5779, Acc : 86.43, mIoU 71.63\n","Step [300/362], Loss: 0.4352, Acc : 86.26, mIoU 71.43\n","Step [350/362], Loss: 0.3842, Acc : 86.17, mIoU 71.22\n","Epoch time : 307.5s\n","Validation . . . \n","Step [50/124], Loss: 0.5045, Acc : 80.70, mIoU 53.62\n","Step [100/124], Loss: 0.5957, Acc : 80.88, mIoU 55.17\n","Epoch time : 39.6s\n","Loss 0.2569,  Acc 81.04,  IoU 55.9904\n","EPOCH 47/100\n","Step [50/362], Loss: 0.3093, Acc : 86.10, mIoU 71.46\n","Step [100/362], Loss: 0.2973, Acc : 86.35, mIoU 71.79\n","Step [150/362], Loss: 0.3417, Acc : 86.21, mIoU 71.80\n","Step [200/362], Loss: 0.3951, Acc : 86.14, mIoU 71.64\n","Step [250/362], Loss: 0.2652, Acc : 86.15, mIoU 71.52\n","Step [300/362], Loss: 0.3775, Acc : 86.30, mIoU 71.64\n","Step [350/362], Loss: 0.4335, Acc : 86.31, mIoU 71.56\n","Epoch time : 305.5s\n","Validation . . . \n","Step [50/124], Loss: 0.4895, Acc : 78.83, mIoU 55.12\n","Step [100/124], Loss: 0.7198, Acc : 79.04, mIoU 55.43\n","Epoch time : 39.0s\n","Loss 0.4963,  Acc 79.56,  IoU 56.0097\n","EPOCH 48/100\n","Step [50/362], Loss: 0.2755, Acc : 86.56, mIoU 72.54\n","Step [100/362], Loss: 0.4230, Acc : 86.64, mIoU 72.78\n","Step [150/362], Loss: 0.3990, Acc : 86.61, mIoU 72.69\n","Step [200/362], Loss: 0.2770, Acc : 86.71, mIoU 72.90\n","Step [250/362], Loss: 0.4718, Acc : 86.73, mIoU 72.83\n","Step [300/362], Loss: 0.3124, Acc : 86.75, mIoU 72.74\n","Step [350/362], Loss: 0.3784, Acc : 86.74, mIoU 72.73\n","Epoch time : 307.0s\n","Validation . . . \n","Step [50/124], Loss: 0.3790, Acc : 80.21, mIoU 54.85\n","Step [100/124], Loss: 0.4831, Acc : 80.95, mIoU 56.50\n","Epoch time : 39.7s\n","Loss 0.4238,  Acc 81.16,  IoU 57.6434\n","EPOCH 49/100\n","Step [50/362], Loss: 0.3418, Acc : 87.23, mIoU 72.58\n","Step [100/362], Loss: 0.2467, Acc : 86.94, mIoU 73.23\n","Step [150/362], Loss: 0.3427, Acc : 87.05, mIoU 73.08\n","Step [200/362], Loss: 0.6072, Acc : 86.81, mIoU 72.28\n","Step [250/362], Loss: 0.1928, Acc : 86.45, mIoU 71.81\n","Step [300/362], Loss: 0.3139, Acc : 86.49, mIoU 71.81\n","Step [350/362], Loss: 0.3835, Acc : 86.46, mIoU 71.92\n","Epoch time : 308.0s\n","Validation . . . \n","Step [50/124], Loss: 0.5041, Acc : 79.36, mIoU 56.94\n","Step [100/124], Loss: 0.5587, Acc : 80.80, mIoU 57.66\n","Epoch time : 39.6s\n","Loss 0.3805,  Acc 81.12,  IoU 57.8682\n","EPOCH 50/100\n","Step [50/362], Loss: 0.3790, Acc : 87.75, mIoU 75.59\n","Step [100/362], Loss: 0.4005, Acc : 86.93, mIoU 73.83\n","Step [150/362], Loss: 0.3664, Acc : 87.03, mIoU 73.54\n","Step [200/362], Loss: 0.3236, Acc : 87.12, mIoU 73.51\n","Step [250/362], Loss: 0.2314, Acc : 87.13, mIoU 73.52\n","Step [300/362], Loss: 0.3478, Acc : 87.08, mIoU 73.66\n","Step [350/362], Loss: 0.3385, Acc : 87.01, mIoU 73.54\n","Epoch time : 308.6s\n","Validation . . . \n","Step [50/124], Loss: 0.4402, Acc : 80.96, mIoU 56.21\n","Step [100/124], Loss: 1.0991, Acc : 80.20, mIoU 55.83\n","Epoch time : 39.4s\n","Loss 0.8223,  Acc 80.64,  IoU 56.1880\n","EPOCH 51/100\n","Step [50/362], Loss: 0.3234, Acc : 86.43, mIoU 71.74\n","Step [100/362], Loss: 0.3457, Acc : 86.99, mIoU 73.12\n","Step [150/362], Loss: 0.2625, Acc : 87.11, mIoU 73.34\n","Step [200/362], Loss: 0.3620, Acc : 87.11, mIoU 73.74\n","Step [250/362], Loss: 0.3083, Acc : 87.17, mIoU 73.81\n","Step [300/362], Loss: 0.3705, Acc : 87.21, mIoU 73.80\n","Step [350/362], Loss: 0.3535, Acc : 87.22, mIoU 73.81\n","Epoch time : 310.0s\n","Validation . . . \n","Step [50/124], Loss: 0.5231, Acc : 80.26, mIoU 58.55\n","Step [100/124], Loss: 0.6639, Acc : 80.62, mIoU 57.70\n","Epoch time : 39.4s\n","Loss 0.5662,  Acc 80.71,  IoU 58.1561\n","EPOCH 52/100\n","Step [50/362], Loss: 0.3164, Acc : 87.14, mIoU 74.45\n","Step [100/362], Loss: 0.2474, Acc : 87.55, mIoU 74.42\n","Step [150/362], Loss: 0.2746, Acc : 87.64, mIoU 74.34\n","Step [200/362], Loss: 0.3594, Acc : 87.31, mIoU 74.00\n","Step [250/362], Loss: 0.4539, Acc : 87.30, mIoU 74.03\n","Step [300/362], Loss: 0.4209, Acc : 87.32, mIoU 73.93\n","Step [350/362], Loss: 0.2837, Acc : 87.23, mIoU 73.87\n","Epoch time : 311.3s\n","Validation . . . \n","Step [50/124], Loss: 0.6667, Acc : 79.91, mIoU 56.19\n","Step [100/124], Loss: 0.6591, Acc : 80.52, mIoU 56.67\n","Epoch time : 39.3s\n","Loss 0.5151,  Acc 80.62,  IoU 56.0426\n","EPOCH 53/100\n","Step [50/362], Loss: 0.2034, Acc : 87.43, mIoU 73.19\n","Step [100/362], Loss: 0.2476, Acc : 87.52, mIoU 74.99\n","Step [150/362], Loss: 0.3480, Acc : 87.52, mIoU 75.14\n","Step [200/362], Loss: 0.2988, Acc : 87.65, mIoU 75.41\n","Step [250/362], Loss: 0.3816, Acc : 87.56, mIoU 75.16\n","Step [300/362], Loss: 0.3134, Acc : 87.38, mIoU 74.54\n","Step [350/362], Loss: 0.3205, Acc : 87.38, mIoU 74.65\n","Epoch time : 306.4s\n","Validation . . . \n","Step [50/124], Loss: 0.7956, Acc : 82.10, mIoU 55.85\n","Step [100/124], Loss: 0.6740, Acc : 81.62, mIoU 56.04\n","Epoch time : 39.8s\n","Loss 0.5824,  Acc 81.11,  IoU 56.3839\n","EPOCH 54/100\n","Step [50/362], Loss: 0.3533, Acc : 88.16, mIoU 76.42\n","Step [100/362], Loss: 0.3030, Acc : 88.08, mIoU 76.72\n","Step [150/362], Loss: 0.2646, Acc : 87.93, mIoU 76.54\n","Step [200/362], Loss: 0.2697, Acc : 87.43, mIoU 75.63\n","Step [250/362], Loss: 0.4233, Acc : 87.13, mIoU 74.44\n","Step [300/362], Loss: 0.2695, Acc : 87.06, mIoU 74.13\n","Step [350/362], Loss: 0.5809, Acc : 87.05, mIoU 74.07\n","Epoch time : 304.9s\n","Validation . . . \n","Step [50/124], Loss: 1.0259, Acc : 80.05, mIoU 57.44\n","Step [100/124], Loss: 0.5651, Acc : 80.82, mIoU 57.58\n","Epoch time : 39.7s\n","Loss 0.6745,  Acc 80.83,  IoU 57.5805\n","EPOCH 55/100\n","Step [50/362], Loss: 0.3879, Acc : 88.17, mIoU 74.85\n","Step [100/362], Loss: 0.2765, Acc : 87.80, mIoU 74.69\n","Step [150/362], Loss: 0.3387, Acc : 87.93, mIoU 75.40\n","Step [200/362], Loss: 0.2772, Acc : 87.75, mIoU 75.13\n","Step [250/362], Loss: 0.3324, Acc : 87.71, mIoU 75.31\n","Step [300/362], Loss: 0.3310, Acc : 87.86, mIoU 75.49\n","Step [350/362], Loss: 0.5021, Acc : 87.74, mIoU 75.26\n","Epoch time : 308.3s\n","Validation . . . \n","Step [50/124], Loss: 0.9675, Acc : 80.46, mIoU 55.95\n","Step [100/124], Loss: 0.5898, Acc : 80.14, mIoU 56.30\n","Epoch time : 40.0s\n","Loss 0.2950,  Acc 80.16,  IoU 56.3114\n","EPOCH 56/100\n","Step [50/362], Loss: 0.3091, Acc : 87.86, mIoU 76.85\n","Step [100/362], Loss: 0.3051, Acc : 88.01, mIoU 76.64\n","Step [150/362], Loss: 0.1861, Acc : 87.93, mIoU 76.76\n","Step [200/362], Loss: 0.2703, Acc : 87.95, mIoU 76.84\n","Step [250/362], Loss: 0.3903, Acc : 87.94, mIoU 76.54\n","Step [300/362], Loss: 0.3746, Acc : 87.87, mIoU 76.16\n","Step [350/362], Loss: 0.3672, Acc : 87.83, mIoU 75.98\n","Epoch time : 306.3s\n","Validation . . . \n","Step [50/124], Loss: 0.4589, Acc : 79.80, mIoU 57.91\n","Step [100/124], Loss: 0.4559, Acc : 80.60, mIoU 58.49\n","Epoch time : 40.0s\n","Loss 0.9203,  Acc 80.30,  IoU 58.1729\n","EPOCH 57/100\n","Step [50/362], Loss: 0.3625, Acc : 88.02, mIoU 75.76\n","Step [100/362], Loss: 0.3071, Acc : 88.33, mIoU 76.62\n","Step [150/362], Loss: 0.2354, Acc : 88.20, mIoU 76.30\n","Step [200/362], Loss: 0.3730, Acc : 87.91, mIoU 75.99\n","Step [250/362], Loss: 0.2815, Acc : 87.96, mIoU 75.99\n","Step [300/362], Loss: 0.2866, Acc : 87.70, mIoU 75.37\n","Step [350/362], Loss: 0.4297, Acc : 87.71, mIoU 75.30\n","Epoch time : 308.6s\n","Validation . . . \n","Step [50/124], Loss: 0.4340, Acc : 80.47, mIoU 56.23\n","Step [100/124], Loss: 0.5364, Acc : 80.36, mIoU 55.80\n","Epoch time : 39.0s\n","Loss 0.5651,  Acc 80.78,  IoU 55.6714\n","EPOCH 58/100\n","Step [50/362], Loss: 0.2358, Acc : 87.58, mIoU 75.92\n","Step [100/362], Loss: 0.3867, Acc : 87.73, mIoU 76.47\n","Step [150/362], Loss: 0.4057, Acc : 87.99, mIoU 76.54\n","Step [200/362], Loss: 0.3452, Acc : 87.82, mIoU 76.10\n","Step [250/362], Loss: 0.3472, Acc : 87.95, mIoU 76.02\n","Step [300/362], Loss: 0.3044, Acc : 88.04, mIoU 76.02\n","Step [350/362], Loss: 0.4158, Acc : 88.02, mIoU 76.03\n","Epoch time : 306.0s\n","Validation . . . \n","Step [50/124], Loss: 0.8999, Acc : 79.85, mIoU 56.59\n","Step [100/124], Loss: 0.6626, Acc : 80.31, mIoU 57.46\n","Epoch time : 39.2s\n","Loss 0.4718,  Acc 80.53,  IoU 57.6857\n","EPOCH 59/100\n","Step [50/362], Loss: 0.2554, Acc : 88.82, mIoU 77.35\n","Step [100/362], Loss: 0.2400, Acc : 88.37, mIoU 76.55\n","Step [150/362], Loss: 0.3195, Acc : 88.23, mIoU 76.39\n","Step [200/362], Loss: 0.2558, Acc : 88.32, mIoU 76.26\n","Step [250/362], Loss: 0.3705, Acc : 88.14, mIoU 76.09\n","Step [300/362], Loss: 0.2876, Acc : 88.06, mIoU 76.06\n","Step [350/362], Loss: 0.3126, Acc : 88.06, mIoU 76.30\n","Epoch time : 307.8s\n","Validation . . . \n","Step [50/124], Loss: 0.4470, Acc : 81.67, mIoU 59.66\n","Step [100/124], Loss: 0.7332, Acc : 81.47, mIoU 57.92\n","Epoch time : 40.4s\n","Loss 0.6009,  Acc 81.10,  IoU 57.0881\n","EPOCH 60/100\n","Step [50/362], Loss: 0.2394, Acc : 89.10, mIoU 78.93\n","Step [100/362], Loss: 0.2612, Acc : 88.77, mIoU 78.08\n","Step [150/362], Loss: 0.3858, Acc : 88.84, mIoU 78.13\n","Step [200/362], Loss: 0.2958, Acc : 88.74, mIoU 77.92\n","Step [250/362], Loss: 0.2854, Acc : 88.67, mIoU 77.33\n","Step [300/362], Loss: 0.2684, Acc : 88.52, mIoU 76.72\n","Step [350/362], Loss: 0.2917, Acc : 88.38, mIoU 76.57\n","Epoch time : 306.1s\n","Validation . . . \n","Step [50/124], Loss: 1.0500, Acc : 82.16, mIoU 58.47\n","Step [100/124], Loss: 1.0876, Acc : 80.97, mIoU 57.15\n","Epoch time : 39.1s\n","Loss 0.7019,  Acc 80.90,  IoU 57.1409\n","EPOCH 61/100\n","Step [50/362], Loss: 0.3247, Acc : 88.36, mIoU 76.68\n","Step [100/362], Loss: 0.1974, Acc : 88.63, mIoU 77.80\n","Step [150/362], Loss: 0.2879, Acc : 88.66, mIoU 77.86\n","Step [200/362], Loss: 0.2037, Acc : 88.67, mIoU 78.12\n","Step [250/362], Loss: 0.3092, Acc : 88.67, mIoU 77.78\n","Step [300/362], Loss: 0.2561, Acc : 88.63, mIoU 77.81\n","Step [350/362], Loss: 0.2661, Acc : 88.62, mIoU 77.61\n","Epoch time : 305.6s\n","Validation . . . \n","Step [50/124], Loss: 0.5697, Acc : 80.98, mIoU 56.61\n","Step [100/124], Loss: 0.7024, Acc : 81.12, mIoU 58.20\n","Epoch time : 39.4s\n","Loss 0.5917,  Acc 80.82,  IoU 57.4667\n","EPOCH 62/100\n","Step [50/362], Loss: 0.3583, Acc : 88.84, mIoU 79.47\n","Step [100/362], Loss: 0.2502, Acc : 88.83, mIoU 79.50\n","Step [150/362], Loss: 0.1859, Acc : 88.83, mIoU 78.63\n","Step [200/362], Loss: 0.3634, Acc : 88.80, mIoU 78.43\n","Step [250/362], Loss: 0.3338, Acc : 88.68, mIoU 77.69\n","Step [300/362], Loss: 0.2755, Acc : 88.67, mIoU 77.55\n","Step [350/362], Loss: 0.2647, Acc : 88.65, mIoU 77.43\n","Epoch time : 305.1s\n","Validation . . . \n","Step [50/124], Loss: 0.4599, Acc : 80.28, mIoU 58.02\n","Step [100/124], Loss: 0.4573, Acc : 81.12, mIoU 59.19\n","Epoch time : 39.7s\n","Loss 1.0604,  Acc 81.07,  IoU 58.2212\n","EPOCH 63/100\n","Step [50/362], Loss: 0.1768, Acc : 88.50, mIoU 77.86\n","Step [100/362], Loss: 0.1523, Acc : 88.71, mIoU 78.08\n","Step [150/362], Loss: 0.2353, Acc : 88.99, mIoU 78.39\n","Step [200/362], Loss: 0.3338, Acc : 88.68, mIoU 77.40\n","Step [250/362], Loss: 0.2185, Acc : 88.60, mIoU 77.22\n","Step [300/362], Loss: 0.3964, Acc : 88.59, mIoU 77.29\n","Step [350/362], Loss: 0.2806, Acc : 88.51, mIoU 77.16\n","Epoch time : 306.9s\n","Validation . . . \n","Step [50/124], Loss: 0.5245, Acc : 81.05, mIoU 57.22\n","Step [100/124], Loss: 0.8422, Acc : 80.28, mIoU 56.69\n","Epoch time : 39.7s\n","Loss 0.6928,  Acc 80.14,  IoU 56.8154\n","EPOCH 64/100\n","Step [50/362], Loss: 0.2871, Acc : 88.75, mIoU 77.06\n","Step [100/362], Loss: 0.2353, Acc : 88.86, mIoU 77.96\n","Step [150/362], Loss: 0.2907, Acc : 88.66, mIoU 77.62\n","Step [200/362], Loss: 0.2061, Acc : 88.88, mIoU 78.29\n","Step [250/362], Loss: 0.2613, Acc : 88.81, mIoU 78.06\n","Step [300/362], Loss: 0.2209, Acc : 88.78, mIoU 77.85\n","Step [350/362], Loss: 0.3446, Acc : 88.76, mIoU 77.84\n","Epoch time : 303.5s\n","Validation . . . \n","Step [50/124], Loss: 0.5900, Acc : 80.97, mIoU 57.18\n","Step [100/124], Loss: 0.7130, Acc : 80.93, mIoU 57.60\n","Epoch time : 39.9s\n","Loss 0.6293,  Acc 81.00,  IoU 57.5286\n","EPOCH 65/100\n","Step [50/362], Loss: 0.3003, Acc : 89.08, mIoU 78.82\n","Step [100/362], Loss: 0.2491, Acc : 89.38, mIoU 79.09\n","Step [150/362], Loss: 0.2772, Acc : 89.48, mIoU 79.02\n","Step [200/362], Loss: 0.2535, Acc : 89.31, mIoU 78.81\n","Step [250/362], Loss: 0.3098, Acc : 89.16, mIoU 78.53\n","Step [300/362], Loss: 0.3108, Acc : 89.05, mIoU 78.49\n","Step [350/362], Loss: 0.3572, Acc : 88.99, mIoU 78.15\n","Epoch time : 306.7s\n","Validation . . . \n","Step [50/124], Loss: 0.8819, Acc : 81.39, mIoU 59.61\n","Step [100/124], Loss: 0.7252, Acc : 80.48, mIoU 57.94\n","Epoch time : 39.1s\n","Loss 0.6704,  Acc 80.28,  IoU 57.6495\n","EPOCH 66/100\n","Step [50/362], Loss: 0.2568, Acc : 88.84, mIoU 79.26\n","Step [100/362], Loss: 0.2862, Acc : 88.97, mIoU 79.25\n","Step [150/362], Loss: 0.3932, Acc : 89.04, mIoU 79.08\n","Step [200/362], Loss: 0.1860, Acc : 89.13, mIoU 79.02\n","Step [250/362], Loss: 0.2842, Acc : 89.08, mIoU 78.83\n","Step [300/362], Loss: 0.2260, Acc : 89.15, mIoU 78.74\n","Step [350/362], Loss: 0.3130, Acc : 89.04, mIoU 78.65\n","Epoch time : 303.8s\n","Validation . . . \n","Step [50/124], Loss: 0.4177, Acc : 81.03, mIoU 57.05\n","Step [100/124], Loss: 0.5255, Acc : 80.84, mIoU 57.12\n","Epoch time : 38.9s\n","Loss 1.3902,  Acc 80.65,  IoU 56.2278\n","EPOCH 67/100\n","Step [50/362], Loss: 0.3018, Acc : 88.61, mIoU 76.09\n","Step [100/362], Loss: 0.1787, Acc : 89.22, mIoU 78.17\n","Step [150/362], Loss: 0.3339, Acc : 89.33, mIoU 78.39\n","Step [200/362], Loss: 0.1931, Acc : 89.21, mIoU 78.04\n","Step [250/362], Loss: 0.2055, Acc : 89.16, mIoU 78.40\n","Step [300/362], Loss: 0.5413, Acc : 89.02, mIoU 78.27\n","Step [350/362], Loss: 0.3257, Acc : 88.96, mIoU 78.16\n","Epoch time : 304.6s\n","Validation . . . \n","Step [50/124], Loss: 0.4271, Acc : 79.90, mIoU 56.13\n","Step [100/124], Loss: 0.4823, Acc : 80.03, mIoU 56.28\n","Epoch time : 39.9s\n","Loss 0.7436,  Acc 80.06,  IoU 56.5963\n","EPOCH 68/100\n","Step [50/362], Loss: 0.2772, Acc : 89.51, mIoU 79.18\n","Step [100/362], Loss: 0.4137, Acc : 89.29, mIoU 79.21\n","Step [150/362], Loss: 0.2226, Acc : 89.07, mIoU 79.02\n","Step [200/362], Loss: 0.3175, Acc : 88.75, mIoU 78.53\n","Step [250/362], Loss: 0.2554, Acc : 88.94, mIoU 78.60\n","Step [300/362], Loss: 0.2639, Acc : 89.03, mIoU 78.88\n","Step [350/362], Loss: 0.2933, Acc : 89.10, mIoU 79.01\n","Epoch time : 309.9s\n","Validation . . . \n","Step [50/124], Loss: 0.9188, Acc : 80.69, mIoU 53.10\n","Step [100/124], Loss: 1.5895, Acc : 79.89, mIoU 54.01\n","Epoch time : 39.6s\n","Loss 0.4054,  Acc 79.96,  IoU 54.3283\n","EPOCH 69/100\n","Step [50/362], Loss: 0.1931, Acc : 89.52, mIoU 79.48\n","Step [100/362], Loss: 0.2924, Acc : 89.75, mIoU 79.75\n","Step [150/362], Loss: 0.2976, Acc : 89.75, mIoU 79.78\n","Step [200/362], Loss: 0.2865, Acc : 89.76, mIoU 79.79\n","Step [250/362], Loss: 0.2655, Acc : 89.68, mIoU 79.87\n","Step [300/362], Loss: 0.2247, Acc : 89.57, mIoU 79.69\n","Step [350/362], Loss: 0.3338, Acc : 89.52, mIoU 79.63\n","Epoch time : 306.5s\n","Validation . . . \n","Step [50/124], Loss: 1.0019, Acc : 80.16, mIoU 56.46\n","Step [100/124], Loss: 0.4448, Acc : 80.50, mIoU 55.76\n","Epoch time : 39.8s\n","Loss 0.6688,  Acc 80.24,  IoU 55.9587\n","EPOCH 70/100\n","Step [50/362], Loss: 0.3593, Acc : 89.31, mIoU 79.77\n","Step [100/362], Loss: 0.2387, Acc : 89.36, mIoU 79.78\n","Step [150/362], Loss: 0.3346, Acc : 89.11, mIoU 78.78\n","Step [200/362], Loss: 0.2732, Acc : 88.77, mIoU 77.81\n","Step [250/362], Loss: 0.2672, Acc : 88.81, mIoU 77.95\n","Step [300/362], Loss: 0.2836, Acc : 88.98, mIoU 78.12\n","Step [350/362], Loss: 0.2613, Acc : 88.95, mIoU 78.17\n","Epoch time : 307.1s\n","Validation . . . \n","Step [50/124], Loss: 0.6967, Acc : 79.54, mIoU 53.89\n","Step [100/124], Loss: 0.5638, Acc : 80.20, mIoU 54.61\n","Epoch time : 39.1s\n","Loss 0.7170,  Acc 80.67,  IoU 55.1546\n","EPOCH 71/100\n","Step [50/362], Loss: 0.2092, Acc : 90.15, mIoU 81.48\n","Step [100/362], Loss: 0.2762, Acc : 89.86, mIoU 80.43\n","Step [150/362], Loss: 0.3405, Acc : 89.83, mIoU 80.20\n","Step [200/362], Loss: 0.2586, Acc : 89.72, mIoU 79.63\n","Step [250/362], Loss: 0.2823, Acc : 89.66, mIoU 79.66\n","Step [300/362], Loss: 0.4220, Acc : 89.57, mIoU 79.45\n","Step [350/362], Loss: 0.2385, Acc : 89.55, mIoU 79.56\n","Epoch time : 302.2s\n","Validation . . . \n","Step [50/124], Loss: 0.5745, Acc : 78.78, mIoU 54.87\n","Step [100/124], Loss: 0.7966, Acc : 79.92, mIoU 56.14\n","Epoch time : 39.2s\n","Loss 0.7827,  Acc 80.13,  IoU 56.5723\n","EPOCH 72/100\n","Step [50/362], Loss: 0.1896, Acc : 89.79, mIoU 79.89\n","Step [100/362], Loss: 0.1625, Acc : 89.92, mIoU 80.81\n","Step [150/362], Loss: 0.2118, Acc : 89.68, mIoU 80.65\n","Step [200/362], Loss: 0.2175, Acc : 89.70, mIoU 80.64\n","Step [250/362], Loss: 0.2132, Acc : 89.84, mIoU 80.59\n","Step [300/362], Loss: 0.2296, Acc : 89.88, mIoU 80.53\n","Step [350/362], Loss: 0.3267, Acc : 89.89, mIoU 80.65\n","Epoch time : 305.0s\n","Validation . . . \n","Step [50/124], Loss: 0.5092, Acc : 80.54, mIoU 56.20\n","Step [100/124], Loss: 0.3349, Acc : 80.65, mIoU 56.70\n","Epoch time : 39.6s\n","Loss 0.5663,  Acc 80.84,  IoU 56.9266\n","EPOCH 73/100\n","Step [50/362], Loss: 0.2698, Acc : 90.06, mIoU 80.69\n","Step [100/362], Loss: 0.2623, Acc : 89.97, mIoU 80.43\n","Step [150/362], Loss: 0.3314, Acc : 89.94, mIoU 80.70\n","Step [200/362], Loss: 0.2662, Acc : 89.95, mIoU 80.48\n","Step [250/362], Loss: 0.2571, Acc : 89.99, mIoU 80.82\n","Step [300/362], Loss: 0.3047, Acc : 89.94, mIoU 80.61\n","Step [350/362], Loss: 0.4020, Acc : 89.86, mIoU 80.54\n","Epoch time : 305.7s\n","Validation . . . \n","Step [50/124], Loss: 0.8422, Acc : 80.00, mIoU 54.43\n","Step [100/124], Loss: 1.0918, Acc : 80.48, mIoU 55.47\n","Epoch time : 39.8s\n","Loss 1.0247,  Acc 80.42,  IoU 55.3111\n","EPOCH 74/100\n","Step [50/362], Loss: 0.2407, Acc : 89.49, mIoU 79.73\n","Step [100/362], Loss: 0.2325, Acc : 89.58, mIoU 79.32\n","Step [150/362], Loss: 0.2222, Acc : 89.52, mIoU 79.84\n","Step [200/362], Loss: 0.2417, Acc : 89.74, mIoU 80.33\n","Step [250/362], Loss: 0.2266, Acc : 89.78, mIoU 80.39\n","Step [300/362], Loss: 0.2383, Acc : 89.87, mIoU 80.42\n","Step [350/362], Loss: 0.4488, Acc : 89.90, mIoU 80.54\n","Epoch time : 309.0s\n","Validation . . . \n","Step [50/124], Loss: 0.6919, Acc : 80.61, mIoU 56.88\n","Step [100/124], Loss: 0.7013, Acc : 80.06, mIoU 56.32\n","Epoch time : 40.2s\n","Loss 0.5679,  Acc 80.29,  IoU 56.9654\n","EPOCH 75/100\n","Step [50/362], Loss: 0.3160, Acc : 90.21, mIoU 81.12\n","Step [100/362], Loss: 0.2977, Acc : 90.18, mIoU 81.06\n","Step [150/362], Loss: 0.2686, Acc : 90.32, mIoU 81.16\n","Step [200/362], Loss: 0.3103, Acc : 90.16, mIoU 81.06\n","Step [250/362], Loss: 0.2209, Acc : 90.06, mIoU 80.67\n","Step [300/362], Loss: 0.2420, Acc : 89.91, mIoU 80.52\n","Step [350/362], Loss: 0.2771, Acc : 89.84, mIoU 80.42\n","Epoch time : 310.9s\n","Validation . . . \n","Step [50/124], Loss: 0.5420, Acc : 80.22, mIoU 54.64\n","Step [100/124], Loss: 0.5733, Acc : 80.86, mIoU 55.28\n","Epoch time : 41.5s\n","Loss 0.6453,  Acc 80.77,  IoU 55.1404\n","EPOCH 76/100\n","Step [50/362], Loss: 0.3533, Acc : 90.46, mIoU 81.34\n","Step [100/362], Loss: 0.3374, Acc : 89.98, mIoU 81.24\n","Step [150/362], Loss: 0.2799, Acc : 90.07, mIoU 81.14\n","Step [200/362], Loss: 0.3199, Acc : 89.68, mIoU 80.30\n","Step [250/362], Loss: 0.3286, Acc : 89.57, mIoU 79.73\n","Step [300/362], Loss: 0.2235, Acc : 89.65, mIoU 79.57\n","Step [350/362], Loss: 0.2855, Acc : 89.74, mIoU 79.84\n","Epoch time : 308.6s\n","Validation . . . \n","Step [50/124], Loss: 0.4833, Acc : 80.79, mIoU 54.15\n","Step [100/124], Loss: 0.7380, Acc : 80.88, mIoU 56.59\n","Epoch time : 38.9s\n","Loss 0.7692,  Acc 80.66,  IoU 56.7356\n","EPOCH 77/100\n","Step [50/362], Loss: 0.2188, Acc : 91.14, mIoU 82.09\n","Step [100/362], Loss: 0.2172, Acc : 90.76, mIoU 82.14\n","Step [150/362], Loss: 0.3296, Acc : 90.55, mIoU 81.78\n","Step [200/362], Loss: 0.2506, Acc : 90.50, mIoU 81.62\n","Step [250/362], Loss: 0.2266, Acc : 90.51, mIoU 81.73\n","Step [300/362], Loss: 0.2138, Acc : 90.47, mIoU 81.65\n","Step [350/362], Loss: 0.2442, Acc : 90.36, mIoU 81.34\n","Epoch time : 312.6s\n","Validation . . . \n","Step [50/124], Loss: 0.7269, Acc : 81.16, mIoU 56.88\n","Step [100/124], Loss: 1.0364, Acc : 80.54, mIoU 55.93\n","Epoch time : 41.1s\n","Loss 0.4811,  Acc 80.65,  IoU 56.0192\n","EPOCH 78/100\n","Step [50/362], Loss: 0.1858, Acc : 90.16, mIoU 80.32\n","Step [100/362], Loss: 0.2076, Acc : 90.47, mIoU 81.10\n","Step [150/362], Loss: 0.1854, Acc : 90.54, mIoU 81.87\n","Step [200/362], Loss: 0.2549, Acc : 90.58, mIoU 81.86\n","Step [250/362], Loss: 0.2403, Acc : 90.46, mIoU 81.65\n","Step [300/362], Loss: 0.2957, Acc : 90.33, mIoU 81.46\n","Step [350/362], Loss: 0.1864, Acc : 90.38, mIoU 81.51\n","Epoch time : 313.0s\n","Validation . . . \n","Step [50/124], Loss: 0.5855, Acc : 80.10, mIoU 58.12\n","Step [100/124], Loss: 0.6859, Acc : 80.83, mIoU 58.56\n","Epoch time : 41.3s\n","Loss 0.8952,  Acc 80.80,  IoU 58.2638\n","EPOCH 79/100\n","Step [50/362], Loss: 0.2454, Acc : 91.18, mIoU 81.09\n","Step [100/362], Loss: 0.2433, Acc : 90.94, mIoU 81.76\n","Step [150/362], Loss: 0.2655, Acc : 90.95, mIoU 81.96\n","Step [200/362], Loss: 0.3214, Acc : 90.67, mIoU 82.03\n","Step [250/362], Loss: 0.2926, Acc : 90.64, mIoU 82.02\n","Step [300/362], Loss: 0.3326, Acc : 90.47, mIoU 81.78\n","Step [350/362], Loss: 0.1902, Acc : 90.40, mIoU 81.49\n","Epoch time : 310.1s\n","Validation . . . \n","Step [50/124], Loss: 0.6909, Acc : 80.04, mIoU 56.71\n","Step [100/124], Loss: 0.7924, Acc : 80.64, mIoU 57.21\n","Epoch time : 41.0s\n","Loss 0.6032,  Acc 80.45,  IoU 56.8862\n","EPOCH 80/100\n","Step [50/362], Loss: 0.4872, Acc : 89.67, mIoU 81.65\n","Step [100/362], Loss: 0.2327, Acc : 90.06, mIoU 81.32\n","Step [150/362], Loss: 0.1799, Acc : 90.23, mIoU 81.25\n","Step [200/362], Loss: 0.1613, Acc : 90.32, mIoU 81.19\n","Step [250/362], Loss: 0.2063, Acc : 90.46, mIoU 81.61\n","Step [300/362], Loss: 0.2046, Acc : 90.46, mIoU 81.74\n","Step [350/362], Loss: 0.2625, Acc : 90.51, mIoU 81.73\n","Epoch time : 312.4s\n","Validation . . . \n","Step [50/124], Loss: 0.4703, Acc : 80.38, mIoU 55.38\n","Step [100/124], Loss: 0.5979, Acc : 80.60, mIoU 56.43\n","Epoch time : 40.1s\n","Loss 0.5913,  Acc 80.55,  IoU 56.3011\n","EPOCH 81/100\n","Step [50/362], Loss: 0.2110, Acc : 90.90, mIoU 81.51\n","Step [100/362], Loss: 0.2148, Acc : 90.99, mIoU 82.33\n","Step [150/362], Loss: 0.2824, Acc : 90.80, mIoU 81.98\n","Step [200/362], Loss: 0.2733, Acc : 90.68, mIoU 81.71\n","Step [250/362], Loss: 0.2968, Acc : 90.59, mIoU 81.61\n","Step [300/362], Loss: 0.2618, Acc : 90.50, mIoU 81.51\n","Step [350/362], Loss: 0.2189, Acc : 90.49, mIoU 81.40\n","Epoch time : 306.9s\n","Validation . . . \n","Step [50/124], Loss: 0.5262, Acc : 81.03, mIoU 57.83\n","Step [100/124], Loss: 0.6885, Acc : 80.54, mIoU 56.06\n","Epoch time : 41.2s\n","Loss 0.4101,  Acc 80.47,  IoU 56.2477\n","EPOCH 82/100\n","Step [50/362], Loss: 0.2143, Acc : 90.99, mIoU 82.50\n","Step [100/362], Loss: 0.1954, Acc : 91.25, mIoU 83.00\n","Step [150/362], Loss: 0.2806, Acc : 91.19, mIoU 82.79\n","Step [200/362], Loss: 0.2562, Acc : 90.91, mIoU 82.51\n","Step [250/362], Loss: 0.1871, Acc : 90.90, mIoU 82.56\n","Step [300/362], Loss: 0.2942, Acc : 90.73, mIoU 82.22\n","Step [350/362], Loss: 0.2958, Acc : 90.53, mIoU 81.85\n","Epoch time : 309.5s\n","Validation . . . \n","Step [50/124], Loss: 0.8787, Acc : 81.11, mIoU 56.42\n","Step [100/124], Loss: 0.8537, Acc : 81.01, mIoU 56.68\n","Epoch time : 41.6s\n","Loss 0.4588,  Acc 80.79,  IoU 56.2023\n","EPOCH 83/100\n","Step [50/362], Loss: 0.2115, Acc : 90.63, mIoU 80.53\n","Step [100/362], Loss: 0.2351, Acc : 90.67, mIoU 81.54\n","Step [150/362], Loss: 0.2820, Acc : 90.69, mIoU 81.94\n","Step [200/362], Loss: 0.1872, Acc : 90.75, mIoU 82.11\n","Step [250/362], Loss: 0.3054, Acc : 90.64, mIoU 81.98\n","Step [300/362], Loss: 0.2292, Acc : 90.61, mIoU 81.85\n","Step [350/362], Loss: 0.2213, Acc : 90.62, mIoU 81.73\n","Epoch time : 312.8s\n","Validation . . . \n","Step [50/124], Loss: 0.4991, Acc : 81.67, mIoU 58.67\n","Step [100/124], Loss: 0.7392, Acc : 80.76, mIoU 57.50\n","Epoch time : 41.2s\n","Loss 0.8100,  Acc 80.87,  IoU 57.4321\n","EPOCH 84/100\n","Step [50/362], Loss: 0.2206, Acc : 91.17, mIoU 83.47\n","Step [100/362], Loss: 0.2767, Acc : 91.07, mIoU 82.90\n","Step [150/362], Loss: 0.2422, Acc : 90.98, mIoU 82.62\n","Step [200/362], Loss: 0.3736, Acc : 90.92, mIoU 82.31\n","Step [250/362], Loss: 0.2067, Acc : 90.83, mIoU 82.24\n","Step [300/362], Loss: 0.2168, Acc : 90.80, mIoU 82.20\n","Step [350/362], Loss: 0.1859, Acc : 90.83, mIoU 82.23\n","Epoch time : 308.6s\n","Validation . . . \n","Step [50/124], Loss: 1.0457, Acc : 80.33, mIoU 56.82\n","Step [100/124], Loss: 0.4860, Acc : 80.77, mIoU 56.42\n","Epoch time : 40.7s\n","Loss 0.5008,  Acc 80.88,  IoU 56.9893\n","EPOCH 85/100\n","Step [50/362], Loss: 0.2952, Acc : 90.96, mIoU 81.33\n","Step [100/362], Loss: 0.1842, Acc : 90.78, mIoU 80.94\n","Step [150/362], Loss: 0.3333, Acc : 90.64, mIoU 81.37\n","Step [200/362], Loss: 0.2903, Acc : 90.76, mIoU 81.82\n","Step [250/362], Loss: 0.3416, Acc : 90.68, mIoU 81.97\n","Step [300/362], Loss: 0.2168, Acc : 90.74, mIoU 82.00\n","Step [350/362], Loss: 0.2423, Acc : 90.71, mIoU 81.93\n","Epoch time : 310.2s\n","Validation . . . \n","Step [50/124], Loss: 0.6224, Acc : 80.39, mIoU 55.98\n","Step [100/124], Loss: 0.6957, Acc : 80.47, mIoU 56.05\n","Epoch time : 40.6s\n","Loss 1.3540,  Acc 80.53,  IoU 56.9554\n","EPOCH 86/100\n","Step [50/362], Loss: 0.2326, Acc : 91.33, mIoU 84.43\n","Step [100/362], Loss: 0.2044, Acc : 91.15, mIoU 83.63\n","Step [150/362], Loss: 0.2551, Acc : 91.18, mIoU 83.33\n","Step [200/362], Loss: 0.1592, Acc : 91.16, mIoU 83.16\n","Step [250/362], Loss: 0.1980, Acc : 91.13, mIoU 83.08\n","Step [300/362], Loss: 0.1917, Acc : 91.09, mIoU 83.06\n","Step [350/362], Loss: 0.2961, Acc : 91.14, mIoU 83.06\n","Epoch time : 307.9s\n","Validation . . . \n","Step [50/124], Loss: 0.6772, Acc : 80.46, mIoU 55.96\n","Step [100/124], Loss: 1.1245, Acc : 80.47, mIoU 56.86\n","Epoch time : 39.7s\n","Loss 0.3090,  Acc 80.52,  IoU 56.8690\n","EPOCH 87/100\n","Step [50/362], Loss: 0.2182, Acc : 91.18, mIoU 82.85\n","Step [100/362], Loss: 0.2368, Acc : 91.29, mIoU 83.48\n","Step [150/362], Loss: 0.2448, Acc : 91.32, mIoU 83.62\n","Step [200/362], Loss: 0.1603, Acc : 91.31, mIoU 83.65\n","Step [250/362], Loss: 0.1615, Acc : 91.27, mIoU 83.60\n","Step [300/362], Loss: 0.3143, Acc : 91.23, mIoU 83.42\n","Step [350/362], Loss: 0.2849, Acc : 91.19, mIoU 83.22\n","Epoch time : 310.5s\n","Validation . . . \n","Step [50/124], Loss: 0.7006, Acc : 79.04, mIoU 54.69\n","Step [100/124], Loss: 0.5880, Acc : 78.97, mIoU 54.69\n","Epoch time : 39.3s\n","Loss 0.3716,  Acc 79.27,  IoU 55.0143\n","EPOCH 88/100\n","Step [50/362], Loss: 0.2389, Acc : 90.74, mIoU 80.93\n","Step [100/362], Loss: 0.2513, Acc : 90.34, mIoU 81.62\n","Step [150/362], Loss: 0.1991, Acc : 90.57, mIoU 81.88\n","Step [200/362], Loss: 0.2086, Acc : 90.59, mIoU 81.73\n","Step [250/362], Loss: 0.1854, Acc : 90.62, mIoU 81.52\n","Step [300/362], Loss: 0.2221, Acc : 90.68, mIoU 81.85\n","Step [350/362], Loss: 0.2457, Acc : 90.74, mIoU 81.88\n","Epoch time : 308.1s\n","Validation . . . \n","Step [50/124], Loss: 0.6667, Acc : 80.60, mIoU 54.98\n","Step [100/124], Loss: 1.0668, Acc : 80.69, mIoU 55.91\n","Epoch time : 39.1s\n","Loss 0.7283,  Acc 80.67,  IoU 56.0526\n","EPOCH 89/100\n","Step [50/362], Loss: 0.2160, Acc : 90.98, mIoU 82.62\n","Step [100/362], Loss: 0.1764, Acc : 91.29, mIoU 82.86\n","Step [150/362], Loss: 0.2600, Acc : 91.22, mIoU 82.91\n","Step [200/362], Loss: 0.1947, Acc : 91.21, mIoU 83.04\n","Step [250/362], Loss: 0.2264, Acc : 91.23, mIoU 83.10\n","Step [300/362], Loss: 0.2145, Acc : 91.20, mIoU 83.16\n","Step [350/362], Loss: 0.1862, Acc : 91.20, mIoU 83.03\n","Epoch time : 305.2s\n","Validation . . . \n","Step [50/124], Loss: 0.7222, Acc : 81.80, mIoU 56.84\n","Step [100/124], Loss: 0.6396, Acc : 81.08, mIoU 56.91\n","Epoch time : 39.1s\n","Loss 0.5479,  Acc 80.95,  IoU 56.6197\n","EPOCH 90/100\n","Step [50/362], Loss: 0.2190, Acc : 91.72, mIoU 84.84\n","Step [100/362], Loss: 0.1983, Acc : 91.65, mIoU 84.42\n","Step [150/362], Loss: 0.2440, Acc : 91.39, mIoU 83.50\n","Step [200/362], Loss: 0.2920, Acc : 91.38, mIoU 83.35\n","Step [250/362], Loss: 0.3219, Acc : 91.45, mIoU 83.61\n","Step [300/362], Loss: 0.2465, Acc : 91.37, mIoU 83.57\n","Step [350/362], Loss: 0.2252, Acc : 91.32, mIoU 83.48\n","Epoch time : 303.3s\n","Validation . . . \n","Step [50/124], Loss: 0.4327, Acc : 80.78, mIoU 57.57\n","Step [100/124], Loss: 0.4193, Acc : 80.68, mIoU 57.27\n","Epoch time : 38.7s\n","Loss 1.5145,  Acc 80.45,  IoU 57.1303\n","EPOCH 91/100\n","Step [50/362], Loss: 0.2095, Acc : 91.21, mIoU 83.32\n","Step [100/362], Loss: 0.2038, Acc : 90.97, mIoU 82.75\n","Step [150/362], Loss: 0.2641, Acc : 90.76, mIoU 82.48\n","Step [200/362], Loss: 0.1633, Acc : 90.90, mIoU 82.74\n","Step [250/362], Loss: 0.2116, Acc : 90.99, mIoU 82.70\n","Step [300/362], Loss: 0.3049, Acc : 90.99, mIoU 82.86\n","Step [350/362], Loss: 0.2081, Acc : 91.06, mIoU 83.00\n","Epoch time : 306.1s\n","Validation . . . \n","Step [50/124], Loss: 0.7956, Acc : 80.88, mIoU 53.45\n","Step [100/124], Loss: 0.4368, Acc : 80.65, mIoU 54.85\n","Epoch time : 38.8s\n","Loss 0.6630,  Acc 80.29,  IoU 55.0797\n","EPOCH 92/100\n","Step [50/362], Loss: 0.2179, Acc : 91.33, mIoU 83.85\n","Step [100/362], Loss: 0.2340, Acc : 91.02, mIoU 83.10\n","Step [150/362], Loss: 0.3454, Acc : 91.22, mIoU 83.51\n","Step [200/362], Loss: 0.2528, Acc : 91.23, mIoU 83.51\n","Step [250/362], Loss: 0.2103, Acc : 91.21, mIoU 83.62\n","Step [300/362], Loss: 0.1539, Acc : 91.28, mIoU 83.64\n","Step [350/362], Loss: 0.2657, Acc : 91.39, mIoU 83.69\n","Epoch time : 305.2s\n","Validation . . . \n","Step [50/124], Loss: 0.4742, Acc : 80.45, mIoU 57.30\n","Step [100/124], Loss: 0.5501, Acc : 80.94, mIoU 57.58\n","Epoch time : 39.0s\n","Loss 0.4456,  Acc 81.09,  IoU 57.4628\n","EPOCH 93/100\n","Step [50/362], Loss: 0.1737, Acc : 91.83, mIoU 85.31\n","Step [100/362], Loss: 0.1458, Acc : 91.86, mIoU 84.48\n","Step [150/362], Loss: 0.2331, Acc : 91.93, mIoU 84.32\n","Step [200/362], Loss: 0.2806, Acc : 91.94, mIoU 84.70\n","Step [250/362], Loss: 0.2370, Acc : 91.96, mIoU 84.69\n","Step [300/362], Loss: 0.2344, Acc : 91.86, mIoU 84.53\n","Step [350/362], Loss: 0.2097, Acc : 91.84, mIoU 84.60\n","Epoch time : 306.2s\n","Validation . . . \n","Step [50/124], Loss: 0.4486, Acc : 81.09, mIoU 57.39\n","Step [100/124], Loss: 0.6721, Acc : 80.93, mIoU 58.03\n","Epoch time : 39.3s\n","Loss 0.5396,  Acc 80.60,  IoU 57.2916\n","EPOCH 94/100\n","Step [50/362], Loss: 0.1427, Acc : 91.99, mIoU 84.58\n","Step [100/362], Loss: 0.1766, Acc : 91.49, mIoU 83.61\n","Step [150/362], Loss: 0.1551, Acc : 91.40, mIoU 83.74\n","Step [200/362], Loss: 0.2406, Acc : 91.42, mIoU 83.91\n","Step [250/362], Loss: 0.1906, Acc : 91.39, mIoU 83.74\n","Step [300/362], Loss: 0.2969, Acc : 91.40, mIoU 83.73\n","Step [350/362], Loss: 0.2133, Acc : 91.37, mIoU 83.64\n","Epoch time : 308.0s\n","Validation . . . \n","Step [50/124], Loss: 0.9633, Acc : 80.01, mIoU 56.73\n","Step [100/124], Loss: 0.5419, Acc : 80.45, mIoU 57.42\n","Epoch time : 39.5s\n","Loss 0.7822,  Acc 80.49,  IoU 57.0413\n","EPOCH 95/100\n","Step [50/362], Loss: 0.2020, Acc : 92.14, mIoU 85.06\n","Step [100/362], Loss: 0.1881, Acc : 92.00, mIoU 84.84\n","Step [150/362], Loss: 0.1947, Acc : 91.85, mIoU 84.60\n","Step [200/362], Loss: 0.2616, Acc : 91.86, mIoU 84.47\n","Step [250/362], Loss: 0.2340, Acc : 91.87, mIoU 84.55\n","Step [300/362], Loss: 0.2479, Acc : 91.64, mIoU 84.18\n","Step [350/362], Loss: 0.2440, Acc : 91.52, mIoU 83.94\n","Epoch time : 303.9s\n","Validation . . . \n","Step [50/124], Loss: 0.9993, Acc : 79.28, mIoU 55.37\n","Step [100/124], Loss: 1.4344, Acc : 80.19, mIoU 55.54\n","Epoch time : 40.2s\n","Loss 0.7212,  Acc 80.18,  IoU 55.6971\n","EPOCH 96/100\n","Step [50/362], Loss: 0.2079, Acc : 90.40, mIoU 82.25\n","Step [100/362], Loss: 0.2343, Acc : 90.83, mIoU 82.53\n","Step [150/362], Loss: 0.2527, Acc : 91.08, mIoU 82.71\n","Step [200/362], Loss: 0.1602, Acc : 91.36, mIoU 83.47\n","Step [250/362], Loss: 0.2297, Acc : 91.40, mIoU 83.62\n","Step [300/362], Loss: 0.2338, Acc : 91.52, mIoU 83.67\n","Step [350/362], Loss: 0.2382, Acc : 91.52, mIoU 83.52\n","Epoch time : 304.2s\n","Validation . . . \n","Step [50/124], Loss: 0.5858, Acc : 80.04, mIoU 52.64\n","Step [100/124], Loss: 0.5294, Acc : 80.54, mIoU 54.96\n","Epoch time : 40.2s\n","Loss 0.5987,  Acc 80.61,  IoU 55.1656\n","EPOCH 97/100\n","Step [50/362], Loss: 0.2302, Acc : 91.59, mIoU 85.05\n","Step [100/362], Loss: 0.1489, Acc : 91.93, mIoU 85.12\n","Step [150/362], Loss: 0.2171, Acc : 91.85, mIoU 85.10\n","Step [200/362], Loss: 0.1648, Acc : 91.96, mIoU 84.96\n","Step [250/362], Loss: 0.2240, Acc : 91.90, mIoU 84.69\n","Step [300/362], Loss: 0.2470, Acc : 91.67, mIoU 83.69\n","Step [350/362], Loss: 0.1891, Acc : 91.59, mIoU 83.32\n","Epoch time : 307.0s\n","Validation . . . \n","Step [50/124], Loss: 0.7263, Acc : 80.99, mIoU 59.23\n","Step [100/124], Loss: 0.4057, Acc : 80.82, mIoU 58.10\n","Epoch time : 39.4s\n","Loss 0.5191,  Acc 80.43,  IoU 57.1520\n","EPOCH 98/100\n","Step [50/362], Loss: 0.2555, Acc : 91.72, mIoU 84.90\n","Step [100/362], Loss: 0.1763, Acc : 91.82, mIoU 84.86\n","Step [150/362], Loss: 0.2190, Acc : 91.95, mIoU 84.79\n","Step [200/362], Loss: 0.3904, Acc : 92.04, mIoU 84.98\n","Step [250/362], Loss: 0.2579, Acc : 91.77, mIoU 84.57\n","Step [300/362], Loss: 0.1609, Acc : 91.76, mIoU 84.57\n","Step [350/362], Loss: 0.1421, Acc : 91.81, mIoU 84.61\n","Epoch time : 305.6s\n","Validation . . . \n","Step [50/124], Loss: 0.7024, Acc : 80.86, mIoU 55.68\n","Step [100/124], Loss: 0.7667, Acc : 80.18, mIoU 55.25\n","Epoch time : 40.9s\n","Loss 0.7199,  Acc 79.89,  IoU 54.7519\n","EPOCH 99/100\n","Step [50/362], Loss: 0.2558, Acc : 91.77, mIoU 83.23\n","Step [100/362], Loss: 0.1756, Acc : 91.87, mIoU 83.80\n","Step [150/362], Loss: 0.3002, Acc : 91.83, mIoU 84.36\n","Step [200/362], Loss: 0.1758, Acc : 91.76, mIoU 84.18\n","Step [250/362], Loss: 0.3122, Acc : 91.71, mIoU 84.02\n","Step [300/362], Loss: 0.2873, Acc : 91.70, mIoU 83.94\n","Step [350/362], Loss: 0.1674, Acc : 91.74, mIoU 84.09\n","Epoch time : 303.2s\n","Validation . . . \n","Step [50/124], Loss: 0.5639, Acc : 79.80, mIoU 56.45\n","Step [100/124], Loss: 0.6105, Acc : 80.53, mIoU 56.73\n","Epoch time : 40.0s\n","Loss 0.4299,  Acc 80.60,  IoU 56.3742\n","EPOCH 100/100\n","Step [50/362], Loss: 0.1787, Acc : 92.16, mIoU 85.10\n","Step [100/362], Loss: 0.1794, Acc : 92.32, mIoU 85.53\n","Step [150/362], Loss: 0.3654, Acc : 92.10, mIoU 84.88\n","Step [200/362], Loss: 0.2019, Acc : 92.12, mIoU 84.99\n","Step [250/362], Loss: 0.2307, Acc : 92.03, mIoU 84.88\n","Step [300/362], Loss: 0.2186, Acc : 92.06, mIoU 84.88\n","Step [350/362], Loss: 0.1494, Acc : 92.03, mIoU 84.92\n","Epoch time : 302.6s\n","Validation . . . \n","Step [50/124], Loss: 0.6871, Acc : 79.14, mIoU 53.57\n","Step [100/124], Loss: 0.6419, Acc : 79.87, mIoU 55.76\n","Epoch time : 39.7s\n","Loss 0.7319,  Acc 79.69,  IoU 56.2633\n","Testing best epoch . . .\n","Step [50/121], Loss: 0.6684, Acc : 81.67, mIoU 59.67\n","Step [100/121], Loss: 0.4550, Acc : 81.71, mIoU 58.93\n","Epoch time : 37.5s\n","Loss 0.5932,  Acc 81.78,  IoU 59.2333\n","Reading patch metadata . . .\n","Done.\n","Dataset ready.\n","Reading patch metadata . . .\n","Done.\n","Dataset ready.\n","Reading patch metadata . . .\n","Done.\n","Dataset ready.\n","Train 363, Val 121, Test 123\n","Model has 1553796 trainable params\n","UNet3D(\n","  (en3): Sequential(\n","    (0): Conv3d(10, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (pool_3): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (en4): Sequential(\n","    (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (pool_4): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (center_in): Sequential(\n","    (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (center_out): Sequential(\n","    (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): ConvTranspose3d(128, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n","  )\n","  (dc4): Sequential(\n","    (0): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (trans3): Sequential(\n","    (0): ConvTranspose3d(64, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n","    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (dc3): Sequential(\n","    (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (4): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (final): Conv3d(16, 20, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",")\n","EPOCH 1/100\n","Step [50/363], Loss: 1.3897, Acc : 48.21, mIoU 5.12\n","Step [100/363], Loss: 1.4224, Acc : 52.39, mIoU 6.24\n","Step [150/363], Loss: 0.9593, Acc : 53.90, mIoU 6.96\n","Step [200/363], Loss: 0.9667, Acc : 55.72, mIoU 8.19\n","Step [250/363], Loss: 0.8591, Acc : 57.05, mIoU 9.13\n","Step [300/363], Loss: 1.2064, Acc : 58.40, mIoU 10.39\n","Step [350/363], Loss: 0.9484, Acc : 59.56, mIoU 11.84\n","Epoch time : 306.9s\n","Validation . . . \n","Step [50/121], Loss: 1.1473, Acc : 63.78, mIoU 15.61\n","Step [100/121], Loss: 1.0154, Acc : 62.95, mIoU 14.89\n","Epoch time : 38.4s\n","Loss 1.1782,  Acc 63.17,  IoU 14.9259\n","EPOCH 2/100\n","Step [50/363], Loss: 0.9627, Acc : 68.16, mIoU 20.59\n","Step [100/363], Loss: 0.8411, Acc : 68.57, mIoU 21.44\n","Step [150/363], Loss: 0.7756, Acc : 68.75, mIoU 21.49\n","Step [200/363], Loss: 1.0181, Acc : 68.93, mIoU 21.91\n","Step [250/363], Loss: 0.8816, Acc : 69.22, mIoU 22.31\n","Step [300/363], Loss: 1.0201, Acc : 69.45, mIoU 22.77\n","Step [350/363], Loss: 0.9951, Acc : 69.56, mIoU 23.44\n","Epoch time : 307.9s\n","Validation . . . \n","Step [50/121], Loss: 0.9004, Acc : 74.30, mIoU 29.59\n","Step [100/121], Loss: 0.6589, Acc : 73.11, mIoU 29.42\n","Epoch time : 37.7s\n","Loss 0.9280,  Acc 73.17,  IoU 29.9462\n","EPOCH 3/100\n","Step [50/363], Loss: 1.0081, Acc : 71.47, mIoU 27.01\n","Step [100/363], Loss: 0.7817, Acc : 71.51, mIoU 26.65\n","Step [150/363], Loss: 0.8591, Acc : 71.74, mIoU 27.40\n","Step [200/363], Loss: 1.0560, Acc : 71.31, mIoU 27.64\n","Step [250/363], Loss: 0.8346, Acc : 71.64, mIoU 27.52\n","Step [300/363], Loss: 0.8494, Acc : 71.78, mIoU 28.05\n","Step [350/363], Loss: 0.7149, Acc : 71.74, mIoU 28.28\n","Epoch time : 304.9s\n","Validation . . . \n","Step [50/121], Loss: 0.6888, Acc : 71.93, mIoU 26.13\n","Step [100/121], Loss: 0.7052, Acc : 72.04, mIoU 26.86\n","Epoch time : 37.8s\n","Loss 1.0126,  Acc 71.95,  IoU 26.4001\n","EPOCH 4/100\n","Step [50/363], Loss: 0.8466, Acc : 72.91, mIoU 31.07\n","Step [100/363], Loss: 0.9779, Acc : 73.33, mIoU 31.37\n","Step [150/363], Loss: 0.8304, Acc : 73.31, mIoU 31.00\n","Step [200/363], Loss: 0.6102, Acc : 73.16, mIoU 31.71\n","Step [250/363], Loss: 0.5811, Acc : 73.27, mIoU 32.09\n","Step [300/363], Loss: 0.6397, Acc : 73.46, mIoU 32.26\n","Step [350/363], Loss: 0.7522, Acc : 73.59, mIoU 32.57\n","Epoch time : 307.0s\n","Validation . . . \n","Step [50/121], Loss: 0.7416, Acc : 74.44, mIoU 37.76\n","Step [100/121], Loss: 1.0284, Acc : 74.01, mIoU 37.55\n","Epoch time : 38.5s\n","Loss 0.7586,  Acc 73.90,  IoU 36.9889\n","EPOCH 5/100\n","Step [50/363], Loss: 1.0822, Acc : 71.60, mIoU 32.64\n","Step [100/363], Loss: 1.4557, Acc : 73.34, mIoU 33.76\n","Step [150/363], Loss: 0.8553, Acc : 74.41, mIoU 34.74\n","Step [200/363], Loss: 0.6250, Acc : 73.95, mIoU 35.13\n","Step [250/363], Loss: 0.8395, Acc : 74.17, mIoU 35.52\n","Step [300/363], Loss: 0.6532, Acc : 74.53, mIoU 36.14\n","Step [350/363], Loss: 0.8767, Acc : 74.78, mIoU 36.54\n","Epoch time : 305.2s\n","Validation . . . \n","Step [50/121], Loss: 0.5842, Acc : 77.72, mIoU 39.66\n","Step [100/121], Loss: 0.5040, Acc : 77.45, mIoU 40.47\n","Epoch time : 37.7s\n","Loss 0.5374,  Acc 77.73,  IoU 40.5681\n","EPOCH 6/100\n","Step [50/363], Loss: 0.7651, Acc : 75.70, mIoU 39.23\n","Step [100/363], Loss: 0.5653, Acc : 76.27, mIoU 39.67\n","Step [150/363], Loss: 0.9689, Acc : 76.30, mIoU 39.37\n","Step [200/363], Loss: 0.6084, Acc : 76.22, mIoU 38.83\n","Step [250/363], Loss: 0.6579, Acc : 76.35, mIoU 39.57\n","Step [300/363], Loss: 0.8021, Acc : 76.25, mIoU 39.77\n","Step [350/363], Loss: 0.7118, Acc : 76.08, mIoU 39.69\n","Epoch time : 308.1s\n","Validation . . . \n","Step [50/121], Loss: 0.8529, Acc : 76.98, mIoU 41.68\n","Step [100/121], Loss: 0.5222, Acc : 77.75, mIoU 42.10\n","Epoch time : 37.9s\n","Loss 0.6409,  Acc 77.77,  IoU 41.8186\n","EPOCH 7/100\n","Step [50/363], Loss: 0.5089, Acc : 77.34, mIoU 42.05\n","Step [100/363], Loss: 0.6601, Acc : 77.38, mIoU 43.42\n","Step [150/363], Loss: 0.7521, Acc : 76.87, mIoU 42.31\n","Step [200/363], Loss: 0.6344, Acc : 76.99, mIoU 42.81\n","Step [250/363], Loss: 0.7360, Acc : 76.98, mIoU 43.05\n","Step [300/363], Loss: 0.5446, Acc : 77.16, mIoU 43.13\n","Step [350/363], Loss: 0.5992, Acc : 77.17, mIoU 43.33\n","Epoch time : 306.4s\n","Validation . . . \n","Step [50/121], Loss: 0.7399, Acc : 75.22, mIoU 41.36\n","Step [100/121], Loss: 0.6637, Acc : 75.14, mIoU 40.41\n","Epoch time : 38.0s\n","Loss 0.4186,  Acc 75.74,  IoU 40.7084\n","EPOCH 8/100\n","Step [50/363], Loss: 0.6680, Acc : 76.68, mIoU 44.03\n","Step [100/363], Loss: 0.7796, Acc : 77.20, mIoU 44.10\n","Step [150/363], Loss: 0.4443, Acc : 76.95, mIoU 44.12\n","Step [200/363], Loss: 0.5952, Acc : 76.78, mIoU 44.14\n","Step [250/363], Loss: 0.7768, Acc : 77.15, mIoU 44.56\n","Step [300/363], Loss: 0.5514, Acc : 77.55, mIoU 44.76\n","Step [350/363], Loss: 0.3861, Acc : 77.53, mIoU 44.88\n","Epoch time : 307.9s\n","Validation . . . \n","Step [50/121], Loss: 0.4062, Acc : 78.83, mIoU 42.79\n","Step [100/121], Loss: 0.7068, Acc : 78.60, mIoU 42.95\n","Epoch time : 38.7s\n","Loss 0.8307,  Acc 78.17,  IoU 42.3259\n","EPOCH 9/100\n","Step [50/363], Loss: 0.5777, Acc : 76.92, mIoU 42.82\n","Step [100/363], Loss: 0.7761, Acc : 77.96, mIoU 45.29\n","Step [150/363], Loss: 0.7553, Acc : 77.74, mIoU 45.55\n","Step [200/363], Loss: 0.5734, Acc : 77.61, mIoU 45.65\n","Step [250/363], Loss: 0.6679, Acc : 77.68, mIoU 46.23\n","Step [300/363], Loss: 0.7819, Acc : 77.81, mIoU 46.45\n","Step [350/363], Loss: 0.6710, Acc : 77.85, mIoU 46.46\n","Epoch time : 307.7s\n","Validation . . . \n","Step [50/121], Loss: 0.8591, Acc : 79.20, mIoU 46.12\n","Step [100/121], Loss: 0.5125, Acc : 78.63, mIoU 44.18\n","Epoch time : 38.1s\n","Loss 0.6156,  Acc 78.68,  IoU 44.4026\n","EPOCH 10/100\n","Step [50/363], Loss: 0.5527, Acc : 78.33, mIoU 47.15\n","Step [100/363], Loss: 0.3943, Acc : 79.22, mIoU 49.75\n","Step [150/363], Loss: 0.5958, Acc : 78.89, mIoU 48.26\n","Step [200/363], Loss: 0.4296, Acc : 78.84, mIoU 48.64\n","Step [250/363], Loss: 0.3448, Acc : 78.71, mIoU 48.48\n","Step [300/363], Loss: 0.7326, Acc : 78.63, mIoU 48.49\n","Step [350/363], Loss: 0.5664, Acc : 78.51, mIoU 48.34\n","Epoch time : 307.7s\n","Validation . . . \n","Step [50/121], Loss: 0.8097, Acc : 79.29, mIoU 49.68\n","Step [100/121], Loss: 0.6777, Acc : 79.60, mIoU 49.55\n","Epoch time : 38.1s\n","Loss 0.5157,  Acc 79.79,  IoU 49.6438\n","EPOCH 11/100\n","Step [50/363], Loss: 0.5817, Acc : 78.21, mIoU 49.30\n","Step [100/363], Loss: 0.6842, Acc : 78.25, mIoU 49.72\n","Step [150/363], Loss: 0.4214, Acc : 78.83, mIoU 49.85\n","Step [200/363], Loss: 0.5524, Acc : 79.18, mIoU 50.01\n","Step [250/363], Loss: 0.5194, Acc : 79.07, mIoU 49.87\n","Step [300/363], Loss: 1.1018, Acc : 79.12, mIoU 49.80\n","Step [350/363], Loss: 0.6262, Acc : 79.26, mIoU 49.95\n","Epoch time : 309.0s\n","Validation . . . \n","Step [50/121], Loss: 0.4963, Acc : 78.90, mIoU 48.52\n","Step [100/121], Loss: 0.5091, Acc : 79.16, mIoU 47.99\n","Epoch time : 37.9s\n","Loss 0.6474,  Acc 79.12,  IoU 47.7876\n","EPOCH 12/100\n","Step [50/363], Loss: 0.6222, Acc : 78.47, mIoU 50.62\n","Step [100/363], Loss: 0.5126, Acc : 78.50, mIoU 50.75\n","Step [150/363], Loss: 0.6394, Acc : 78.68, mIoU 50.03\n","Step [200/363], Loss: 0.5421, Acc : 78.89, mIoU 50.23\n","Step [250/363], Loss: 0.4897, Acc : 79.17, mIoU 50.56\n","Step [300/363], Loss: 0.6786, Acc : 79.07, mIoU 50.70\n","Step [350/363], Loss: 0.5953, Acc : 79.08, mIoU 50.58\n","Epoch time : 307.0s\n","Validation . . . \n","Step [50/121], Loss: 0.4620, Acc : 80.28, mIoU 48.76\n","Step [100/121], Loss: 0.5546, Acc : 80.00, mIoU 48.99\n","Epoch time : 38.2s\n","Loss 0.6810,  Acc 80.28,  IoU 48.8250\n","EPOCH 13/100\n","Step [50/363], Loss: 0.5458, Acc : 79.90, mIoU 53.59\n","Step [100/363], Loss: 0.5123, Acc : 79.98, mIoU 52.67\n","Step [150/363], Loss: 0.5635, Acc : 79.58, mIoU 52.36\n","Step [200/363], Loss: 0.4101, Acc : 79.62, mIoU 51.73\n","Step [250/363], Loss: 0.4618, Acc : 79.55, mIoU 51.80\n","Step [300/363], Loss: 0.3755, Acc : 79.60, mIoU 51.90\n","Step [350/363], Loss: 0.4400, Acc : 79.51, mIoU 51.64\n","Epoch time : 306.8s\n","Validation . . . \n","Step [50/121], Loss: 0.4569, Acc : 81.37, mIoU 52.60\n","Step [100/121], Loss: 0.5901, Acc : 81.01, mIoU 51.20\n","Epoch time : 37.7s\n","Loss 0.3692,  Acc 80.70,  IoU 50.9270\n","EPOCH 14/100\n","Step [50/363], Loss: 0.4782, Acc : 79.39, mIoU 52.98\n","Step [100/363], Loss: 0.5626, Acc : 80.00, mIoU 53.43\n","Step [150/363], Loss: 0.4401, Acc : 80.36, mIoU 54.00\n","Step [200/363], Loss: 0.5529, Acc : 80.08, mIoU 52.92\n","Step [250/363], Loss: 0.6096, Acc : 79.83, mIoU 52.61\n","Step [300/363], Loss: 0.4346, Acc : 79.81, mIoU 52.59\n","Step [350/363], Loss: 0.7017, Acc : 79.72, mIoU 52.36\n","Epoch time : 306.7s\n","Validation . . . \n","Step [50/121], Loss: 0.7234, Acc : 80.32, mIoU 51.89\n","Step [100/121], Loss: 0.4296, Acc : 81.12, mIoU 52.47\n","Epoch time : 37.7s\n","Loss 0.4474,  Acc 80.76,  IoU 52.6183\n","EPOCH 15/100\n","Step [50/363], Loss: 0.3498, Acc : 79.37, mIoU 51.81\n","Step [100/363], Loss: 0.5951, Acc : 79.97, mIoU 53.90\n","Step [150/363], Loss: 0.4949, Acc : 79.70, mIoU 52.25\n","Step [200/363], Loss: 0.3740, Acc : 79.93, mIoU 52.37\n","Step [250/363], Loss: 0.9633, Acc : 79.97, mIoU 52.33\n","Step [300/363], Loss: 0.4668, Acc : 79.85, mIoU 52.26\n","Step [350/363], Loss: 0.5890, Acc : 79.89, mIoU 52.57\n","Epoch time : 305.8s\n","Validation . . . \n","Step [50/121], Loss: 0.5495, Acc : 80.38, mIoU 51.98\n","Step [100/121], Loss: 0.5758, Acc : 80.70, mIoU 52.23\n","Epoch time : 37.3s\n","Loss 0.5158,  Acc 80.80,  IoU 52.6851\n","EPOCH 16/100\n","Step [50/363], Loss: 0.4796, Acc : 81.22, mIoU 52.13\n","Step [100/363], Loss: 0.6805, Acc : 80.39, mIoU 53.28\n","Step [150/363], Loss: 0.7231, Acc : 80.06, mIoU 52.22\n","Step [200/363], Loss: 0.4677, Acc : 80.25, mIoU 52.84\n","Step [250/363], Loss: 0.6492, Acc : 80.29, mIoU 52.98\n","Step [300/363], Loss: 0.7102, Acc : 80.23, mIoU 53.25\n","Step [350/363], Loss: 0.3939, Acc : 80.20, mIoU 53.64\n","Epoch time : 309.0s\n","Validation . . . \n","Step [50/121], Loss: 0.6387, Acc : 80.97, mIoU 52.62\n","Step [100/121], Loss: 0.6361, Acc : 80.77, mIoU 52.47\n","Epoch time : 39.3s\n","Loss 0.5553,  Acc 80.68,  IoU 52.0416\n","EPOCH 17/100\n","Step [50/363], Loss: 0.6430, Acc : 80.91, mIoU 55.07\n","Step [100/363], Loss: 0.6500, Acc : 80.76, mIoU 54.05\n","Step [150/363], Loss: 0.4599, Acc : 80.91, mIoU 55.13\n","Step [200/363], Loss: 0.8166, Acc : 80.91, mIoU 55.40\n","Step [250/363], Loss: 0.6621, Acc : 80.92, mIoU 55.99\n","Step [300/363], Loss: 0.4602, Acc : 80.82, mIoU 55.72\n","Step [350/363], Loss: 0.6981, Acc : 80.64, mIoU 55.10\n","Epoch time : 314.6s\n","Validation . . . \n","Step [50/121], Loss: 0.7541, Acc : 79.12, mIoU 54.04\n","Step [100/121], Loss: 0.5317, Acc : 79.70, mIoU 53.73\n","Epoch time : 39.8s\n","Loss 0.7158,  Acc 79.98,  IoU 53.4169\n","EPOCH 18/100\n","Step [50/363], Loss: 0.5282, Acc : 80.61, mIoU 54.99\n","Step [100/363], Loss: 0.6292, Acc : 80.88, mIoU 54.70\n","Step [150/363], Loss: 0.5914, Acc : 80.76, mIoU 54.41\n","Step [200/363], Loss: 0.6067, Acc : 80.87, mIoU 54.88\n","Step [250/363], Loss: 0.4912, Acc : 80.85, mIoU 55.13\n","Step [300/363], Loss: 0.4604, Acc : 80.92, mIoU 55.08\n","Step [350/363], Loss: 0.4726, Acc : 80.74, mIoU 54.93\n","Epoch time : 309.8s\n","Validation . . . \n","Step [50/121], Loss: 0.7434, Acc : 80.71, mIoU 54.06\n","Step [100/121], Loss: 0.6083, Acc : 81.02, mIoU 55.10\n","Epoch time : 38.4s\n","Loss 0.6600,  Acc 80.92,  IoU 54.5226\n","EPOCH 19/100\n","Step [50/363], Loss: 0.6342, Acc : 80.98, mIoU 55.75\n","Step [100/363], Loss: 0.5849, Acc : 81.43, mIoU 55.45\n","Step [150/363], Loss: 0.3537, Acc : 81.34, mIoU 56.13\n","Step [200/363], Loss: 0.4643, Acc : 81.48, mIoU 56.51\n","Step [250/363], Loss: 0.4487, Acc : 81.38, mIoU 56.34\n","Step [300/363], Loss: 0.4893, Acc : 81.27, mIoU 55.90\n","Step [350/363], Loss: 0.5472, Acc : 81.13, mIoU 55.89\n","Epoch time : 307.4s\n","Validation . . . \n","Step [50/121], Loss: 0.6600, Acc : 79.85, mIoU 52.02\n","Step [100/121], Loss: 0.5907, Acc : 80.19, mIoU 53.45\n","Epoch time : 38.6s\n","Loss 0.5063,  Acc 80.31,  IoU 53.2983\n","EPOCH 20/100\n","Step [50/363], Loss: 0.4905, Acc : 82.60, mIoU 56.79\n","Step [100/363], Loss: 0.4073, Acc : 81.75, mIoU 57.03\n","Step [150/363], Loss: 0.8204, Acc : 81.58, mIoU 56.58\n","Step [200/363], Loss: 0.4725, Acc : 81.67, mIoU 56.90\n","Step [250/363], Loss: 0.5740, Acc : 81.64, mIoU 57.04\n","Step [300/363], Loss: 0.9984, Acc : 81.50, mIoU 56.61\n","Step [350/363], Loss: 0.6471, Acc : 81.27, mIoU 56.50\n","Epoch time : 308.0s\n","Validation . . . \n","Step [50/121], Loss: 0.5826, Acc : 80.98, mIoU 55.34\n","Step [100/121], Loss: 0.4217, Acc : 81.06, mIoU 54.69\n","Epoch time : 38.3s\n","Loss 0.5852,  Acc 80.97,  IoU 54.7407\n","EPOCH 21/100\n","Step [50/363], Loss: 0.5472, Acc : 80.68, mIoU 56.56\n","Step [100/363], Loss: 0.6474, Acc : 81.41, mIoU 57.11\n","Step [150/363], Loss: 0.4528, Acc : 81.33, mIoU 57.36\n","Step [200/363], Loss: 0.4006, Acc : 81.64, mIoU 57.40\n","Step [250/363], Loss: 0.5650, Acc : 81.78, mIoU 57.74\n","Step [300/363], Loss: 0.6638, Acc : 81.60, mIoU 57.31\n","Step [350/363], Loss: 0.5825, Acc : 81.48, mIoU 57.53\n","Epoch time : 305.6s\n","Validation . . . \n","Step [50/121], Loss: 0.3941, Acc : 80.97, mIoU 56.84\n","Step [100/121], Loss: 0.4622, Acc : 81.14, mIoU 57.43\n","Epoch time : 38.3s\n","Loss 0.5030,  Acc 81.34,  IoU 57.0849\n","EPOCH 22/100\n","Step [50/363], Loss: 0.4645, Acc : 81.23, mIoU 55.43\n","Step [100/363], Loss: 0.5537, Acc : 81.63, mIoU 56.81\n","Step [150/363], Loss: 0.4325, Acc : 81.86, mIoU 57.50\n","Step [200/363], Loss: 0.4367, Acc : 81.71, mIoU 58.21\n","Step [250/363], Loss: 0.5799, Acc : 81.66, mIoU 58.35\n","Step [300/363], Loss: 0.4866, Acc : 81.63, mIoU 58.15\n","Step [350/363], Loss: 0.6070, Acc : 81.71, mIoU 58.25\n","Epoch time : 308.0s\n","Validation . . . \n","Step [50/121], Loss: 0.6250, Acc : 80.75, mIoU 52.24\n","Step [100/121], Loss: 0.3644, Acc : 80.64, mIoU 52.90\n","Epoch time : 38.0s\n","Loss 0.4605,  Acc 80.82,  IoU 52.9316\n","EPOCH 23/100\n","Step [50/363], Loss: 0.5531, Acc : 81.79, mIoU 57.46\n","Step [100/363], Loss: 0.4880, Acc : 82.35, mIoU 58.74\n","Step [150/363], Loss: 0.5467, Acc : 82.27, mIoU 59.70\n","Step [200/363], Loss: 0.3196, Acc : 82.15, mIoU 59.75\n","Step [250/363], Loss: 0.5217, Acc : 82.29, mIoU 59.72\n","Step [300/363], Loss: 0.5741, Acc : 82.18, mIoU 59.75\n","Step [350/363], Loss: 0.6477, Acc : 82.05, mIoU 59.34\n","Epoch time : 309.0s\n","Validation . . . \n","Step [50/121], Loss: 0.3503, Acc : 79.62, mIoU 53.66\n","Step [100/121], Loss: 0.5492, Acc : 79.29, mIoU 53.58\n","Epoch time : 38.4s\n","Loss 0.6527,  Acc 79.07,  IoU 53.2754\n","EPOCH 24/100\n","Step [50/363], Loss: 0.4981, Acc : 82.80, mIoU 60.34\n","Step [100/363], Loss: 0.3496, Acc : 82.35, mIoU 60.80\n","Step [150/363], Loss: 0.5723, Acc : 81.95, mIoU 59.84\n","Step [200/363], Loss: 0.6126, Acc : 81.77, mIoU 59.43\n","Step [250/363], Loss: 0.5640, Acc : 81.92, mIoU 59.33\n","Step [300/363], Loss: 0.5840, Acc : 81.95, mIoU 59.11\n","Step [350/363], Loss: 0.2961, Acc : 81.99, mIoU 58.97\n","Epoch time : 309.7s\n","Validation . . . \n","Step [50/121], Loss: 0.6596, Acc : 81.33, mIoU 55.05\n","Step [100/121], Loss: 0.5145, Acc : 81.06, mIoU 56.08\n","Epoch time : 39.0s\n","Loss 0.5299,  Acc 81.35,  IoU 57.1076\n","EPOCH 25/100\n","Step [50/363], Loss: 0.3987, Acc : 82.22, mIoU 59.27\n","Step [100/363], Loss: 0.3989, Acc : 82.29, mIoU 60.36\n","Step [150/363], Loss: 0.6341, Acc : 82.29, mIoU 59.90\n","Step [200/363], Loss: 0.7146, Acc : 82.18, mIoU 60.06\n","Step [250/363], Loss: 0.4639, Acc : 82.20, mIoU 59.98\n","Step [300/363], Loss: 0.7620, Acc : 82.17, mIoU 59.87\n","Step [350/363], Loss: 0.6047, Acc : 82.26, mIoU 59.91\n","Epoch time : 308.9s\n","Validation . . . \n","Step [50/121], Loss: 0.4829, Acc : 81.53, mIoU 56.03\n","Step [100/121], Loss: 0.4419, Acc : 81.54, mIoU 54.94\n","Epoch time : 38.2s\n","Loss 0.4895,  Acc 81.34,  IoU 54.7866\n","EPOCH 26/100\n","Step [50/363], Loss: 0.4439, Acc : 82.34, mIoU 59.89\n","Step [100/363], Loss: 0.2961, Acc : 82.37, mIoU 60.33\n","Step [150/363], Loss: 0.3951, Acc : 82.26, mIoU 60.11\n","Step [200/363], Loss: 0.3751, Acc : 82.17, mIoU 59.81\n","Step [250/363], Loss: 0.5260, Acc : 82.13, mIoU 59.97\n","Step [300/363], Loss: 0.4550, Acc : 82.38, mIoU 60.41\n","Step [350/363], Loss: 0.4813, Acc : 82.52, mIoU 60.82\n","Epoch time : 308.4s\n","Validation . . . \n","Step [50/121], Loss: 0.5793, Acc : 81.22, mIoU 55.63\n","Step [100/121], Loss: 0.5527, Acc : 81.82, mIoU 56.26\n","Epoch time : 39.1s\n","Loss 0.7153,  Acc 81.88,  IoU 55.8916\n","EPOCH 27/100\n","Step [50/363], Loss: 0.3394, Acc : 82.27, mIoU 59.80\n","Step [100/363], Loss: 0.7593, Acc : 82.48, mIoU 60.46\n","Step [150/363], Loss: 0.6950, Acc : 82.43, mIoU 60.84\n","Step [200/363], Loss: 0.4229, Acc : 82.23, mIoU 60.32\n","Step [250/363], Loss: 0.3129, Acc : 82.15, mIoU 60.45\n","Step [300/363], Loss: 0.4377, Acc : 82.26, mIoU 60.51\n","Step [350/363], Loss: 0.7600, Acc : 82.31, mIoU 60.53\n","Epoch time : 312.5s\n","Validation . . . \n","Step [50/121], Loss: 0.6697, Acc : 82.61, mIoU 59.19\n","Step [100/121], Loss: 0.5211, Acc : 82.13, mIoU 58.79\n","Epoch time : 40.0s\n","Loss 0.4037,  Acc 82.19,  IoU 58.5848\n","EPOCH 28/100\n","Step [50/363], Loss: 0.3447, Acc : 82.75, mIoU 60.84\n","Step [100/363], Loss: 0.4333, Acc : 82.86, mIoU 60.46\n","Step [150/363], Loss: 0.3514, Acc : 82.62, mIoU 60.22\n","Step [200/363], Loss: 0.4311, Acc : 82.65, mIoU 60.29\n","Step [250/363], Loss: 0.7354, Acc : 82.73, mIoU 60.90\n","Step [300/363], Loss: 0.5987, Acc : 82.77, mIoU 60.93\n","Step [350/363], Loss: 0.3433, Acc : 82.78, mIoU 61.23\n","Epoch time : 310.2s\n","Validation . . . \n","Step [50/121], Loss: 0.5516, Acc : 82.34, mIoU 57.33\n","Step [100/121], Loss: 0.6506, Acc : 81.65, mIoU 56.95\n","Epoch time : 38.5s\n","Loss 0.5660,  Acc 81.33,  IoU 56.1974\n","EPOCH 29/100\n","Step [50/363], Loss: 0.3286, Acc : 83.00, mIoU 62.40\n","Step [100/363], Loss: 0.4697, Acc : 82.80, mIoU 61.93\n","Step [150/363], Loss: 0.3830, Acc : 83.07, mIoU 62.09\n","Step [200/363], Loss: 0.2980, Acc : 83.20, mIoU 62.78\n","Step [250/363], Loss: 0.6313, Acc : 83.17, mIoU 62.72\n","Step [300/363], Loss: 0.5632, Acc : 83.07, mIoU 62.23\n","Step [350/363], Loss: 0.5859, Acc : 83.00, mIoU 62.11\n","Epoch time : 311.1s\n","Validation . . . \n","Step [50/121], Loss: 0.5203, Acc : 79.29, mIoU 53.50\n","Step [100/121], Loss: 0.5456, Acc : 79.78, mIoU 54.84\n","Epoch time : 38.3s\n","Loss 0.6297,  Acc 79.68,  IoU 55.0259\n","EPOCH 30/100\n","Step [50/363], Loss: 0.5032, Acc : 82.88, mIoU 59.58\n","Step [100/363], Loss: 0.6536, Acc : 83.01, mIoU 60.57\n","Step [150/363], Loss: 0.3158, Acc : 83.34, mIoU 61.93\n","Step [200/363], Loss: 0.4743, Acc : 83.34, mIoU 62.55\n","Step [250/363], Loss: 0.4416, Acc : 83.35, mIoU 62.65\n","Step [300/363], Loss: 0.4972, Acc : 83.34, mIoU 62.66\n","Step [350/363], Loss: 0.4578, Acc : 83.27, mIoU 62.85\n","Epoch time : 308.9s\n","Validation . . . \n","Step [50/121], Loss: 0.4999, Acc : 83.36, mIoU 58.28\n","Step [100/121], Loss: 0.6044, Acc : 82.57, mIoU 59.16\n","Epoch time : 38.2s\n","Loss 0.5063,  Acc 82.30,  IoU 59.4037\n","EPOCH 31/100\n","Step [50/363], Loss: 0.5398, Acc : 83.64, mIoU 62.29\n","Step [100/363], Loss: 0.5532, Acc : 83.47, mIoU 63.98\n","Step [150/363], Loss: 0.4652, Acc : 83.30, mIoU 63.59\n","Step [200/363], Loss: 0.5127, Acc : 83.58, mIoU 64.09\n","Step [250/363], Loss: 0.3638, Acc : 83.42, mIoU 63.63\n","Step [300/363], Loss: 0.4313, Acc : 83.31, mIoU 63.44\n","Step [350/363], Loss: 0.4786, Acc : 83.30, mIoU 63.30\n","Epoch time : 307.4s\n","Validation . . . \n","Step [50/121], Loss: 0.4015, Acc : 82.07, mIoU 57.32\n","Step [100/121], Loss: 0.5923, Acc : 81.97, mIoU 56.49\n","Epoch time : 38.1s\n","Loss 0.5583,  Acc 81.77,  IoU 56.1894\n","EPOCH 32/100\n","Step [50/363], Loss: 0.3015, Acc : 84.00, mIoU 64.01\n","Step [100/363], Loss: 0.5790, Acc : 83.82, mIoU 64.00\n","Step [150/363], Loss: 0.5029, Acc : 83.83, mIoU 63.16\n","Step [200/363], Loss: 0.3502, Acc : 83.77, mIoU 63.49\n","Step [250/363], Loss: 0.5131, Acc : 83.75, mIoU 63.51\n","Step [300/363], Loss: 0.5133, Acc : 83.57, mIoU 63.75\n","Step [350/363], Loss: 0.5615, Acc : 83.50, mIoU 63.66\n","Epoch time : 309.6s\n","Validation . . . \n","Step [50/121], Loss: 0.6685, Acc : 81.09, mIoU 55.06\n","Step [100/121], Loss: 0.4600, Acc : 81.45, mIoU 56.60\n","Epoch time : 37.9s\n","Loss 0.6582,  Acc 81.55,  IoU 56.9175\n","EPOCH 33/100\n","Step [50/363], Loss: 0.3639, Acc : 85.03, mIoU 65.17\n","Step [100/363], Loss: 0.3322, Acc : 84.25, mIoU 65.13\n","Step [150/363], Loss: 0.4748, Acc : 83.94, mIoU 65.39\n","Step [200/363], Loss: 0.3410, Acc : 83.54, mIoU 64.25\n","Step [250/363], Loss: 0.2682, Acc : 83.28, mIoU 63.93\n","Step [300/363], Loss: 0.3083, Acc : 83.58, mIoU 64.36\n","Step [350/363], Loss: 0.3748, Acc : 83.53, mIoU 63.93\n","Epoch time : 304.5s\n","Validation . . . \n","Step [50/121], Loss: 0.4844, Acc : 82.69, mIoU 57.55\n","Step [100/121], Loss: 0.8191, Acc : 81.70, mIoU 57.77\n","Epoch time : 37.2s\n","Loss 0.3707,  Acc 81.99,  IoU 57.5286\n","EPOCH 34/100\n","Step [50/363], Loss: 0.3328, Acc : 83.44, mIoU 64.52\n","Step [100/363], Loss: 0.5942, Acc : 83.87, mIoU 65.17\n","Step [150/363], Loss: 0.5706, Acc : 83.99, mIoU 65.73\n","Step [200/363], Loss: 0.5478, Acc : 83.90, mIoU 65.38\n","Step [250/363], Loss: 0.4971, Acc : 83.94, mIoU 65.32\n","Step [300/363], Loss: 0.3847, Acc : 83.94, mIoU 65.08\n","Step [350/363], Loss: 0.4995, Acc : 83.93, mIoU 64.77\n","Epoch time : 308.8s\n","Validation . . . \n","Step [50/121], Loss: 0.8464, Acc : 82.05, mIoU 55.97\n","Step [100/121], Loss: 0.6632, Acc : 82.19, mIoU 58.23\n","Epoch time : 38.2s\n","Loss 0.4507,  Acc 81.95,  IoU 57.3065\n","EPOCH 35/100\n","Step [50/363], Loss: 0.2370, Acc : 83.39, mIoU 64.30\n","Step [100/363], Loss: 0.3841, Acc : 83.54, mIoU 63.73\n","Step [150/363], Loss: 0.4260, Acc : 83.72, mIoU 64.86\n","Step [200/363], Loss: 0.4612, Acc : 83.72, mIoU 65.30\n","Step [250/363], Loss: 0.2940, Acc : 83.79, mIoU 65.42\n","Step [300/363], Loss: 0.4071, Acc : 83.99, mIoU 65.35\n","Step [350/363], Loss: 0.3986, Acc : 83.90, mIoU 65.18\n","Epoch time : 312.0s\n","Validation . . . \n","Step [50/121], Loss: 0.3654, Acc : 82.19, mIoU 56.83\n","Step [100/121], Loss: 0.4843, Acc : 82.24, mIoU 57.32\n","Epoch time : 38.6s\n","Loss 0.5223,  Acc 82.09,  IoU 57.3606\n","EPOCH 36/100\n","Step [50/363], Loss: 0.3839, Acc : 83.80, mIoU 65.07\n","Step [100/363], Loss: 0.6402, Acc : 84.12, mIoU 66.13\n","Step [150/363], Loss: 0.4113, Acc : 84.19, mIoU 65.64\n","Step [200/363], Loss: 0.4671, Acc : 84.30, mIoU 65.71\n","Step [250/363], Loss: 0.4927, Acc : 84.24, mIoU 66.08\n","Step [300/363], Loss: 0.3658, Acc : 84.11, mIoU 65.79\n","Step [350/363], Loss: 0.3059, Acc : 84.10, mIoU 65.59\n","Epoch time : 313.9s\n","Validation . . . \n","Step [50/121], Loss: 0.4179, Acc : 82.01, mIoU 57.17\n","Step [100/121], Loss: 0.6270, Acc : 82.08, mIoU 58.27\n","Epoch time : 40.3s\n","Loss 0.5473,  Acc 82.07,  IoU 58.3255\n","EPOCH 37/100\n","Step [50/363], Loss: 0.3446, Acc : 84.39, mIoU 66.58\n","Step [100/363], Loss: 0.4062, Acc : 84.62, mIoU 66.55\n","Step [150/363], Loss: 0.4504, Acc : 84.59, mIoU 66.70\n","Step [200/363], Loss: 0.4357, Acc : 84.56, mIoU 66.15\n","Step [250/363], Loss: 0.5778, Acc : 84.43, mIoU 66.06\n","Step [300/363], Loss: 0.4569, Acc : 84.44, mIoU 66.16\n","Step [350/363], Loss: 0.3754, Acc : 84.42, mIoU 66.24\n","Epoch time : 310.8s\n","Validation . . . \n","Step [50/121], Loss: 0.5985, Acc : 80.98, mIoU 57.12\n","Step [100/121], Loss: 0.5257, Acc : 81.93, mIoU 57.70\n","Epoch time : 38.6s\n","Loss 0.4313,  Acc 81.83,  IoU 57.4319\n","EPOCH 38/100\n","Step [50/363], Loss: 0.4103, Acc : 85.41, mIoU 70.31\n","Step [100/363], Loss: 0.5086, Acc : 84.92, mIoU 68.12\n","Step [150/363], Loss: 0.3730, Acc : 84.45, mIoU 67.12\n","Step [200/363], Loss: 0.6045, Acc : 84.51, mIoU 66.43\n","Step [250/363], Loss: 0.3752, Acc : 84.45, mIoU 66.40\n","Step [300/363], Loss: 0.3384, Acc : 84.46, mIoU 66.44\n","Step [350/363], Loss: 0.5167, Acc : 84.43, mIoU 66.42\n","Epoch time : 307.3s\n","Validation . . . \n","Step [50/121], Loss: 0.5384, Acc : 81.42, mIoU 58.51\n","Step [100/121], Loss: 0.6659, Acc : 81.79, mIoU 58.16\n","Epoch time : 38.4s\n","Loss 0.6613,  Acc 81.81,  IoU 57.6383\n","EPOCH 39/100\n","Step [50/363], Loss: 0.3185, Acc : 85.20, mIoU 68.32\n","Step [100/363], Loss: 0.3580, Acc : 85.03, mIoU 67.72\n","Step [150/363], Loss: 0.2955, Acc : 84.93, mIoU 67.81\n","Step [200/363], Loss: 0.4907, Acc : 84.78, mIoU 67.38\n","Step [250/363], Loss: 0.3399, Acc : 84.86, mIoU 67.95\n","Step [300/363], Loss: 0.6401, Acc : 84.69, mIoU 67.43\n","Step [350/363], Loss: 0.4400, Acc : 84.67, mIoU 67.26\n","Epoch time : 320.8s\n","Validation . . . \n","Step [50/121], Loss: 0.6251, Acc : 81.82, mIoU 58.94\n","Step [100/121], Loss: 0.4232, Acc : 82.27, mIoU 59.22\n","Epoch time : 41.6s\n","Loss 0.4771,  Acc 81.95,  IoU 58.4113\n","EPOCH 40/100\n","Step [50/363], Loss: 0.3430, Acc : 85.06, mIoU 68.39\n","Step [100/363], Loss: 0.3272, Acc : 84.81, mIoU 68.17\n","Step [150/363], Loss: 0.4519, Acc : 84.72, mIoU 67.47\n","Step [200/363], Loss: 0.3676, Acc : 84.91, mIoU 67.72\n","Step [250/363], Loss: 0.4059, Acc : 84.94, mIoU 67.80\n","Step [300/363], Loss: 0.3919, Acc : 84.96, mIoU 68.06\n","Step [350/363], Loss: 0.4600, Acc : 84.88, mIoU 67.94\n","Epoch time : 340.1s\n","Validation . . . \n","Step [50/121], Loss: 0.3805, Acc : 82.95, mIoU 60.44\n","Step [100/121], Loss: 0.5873, Acc : 82.85, mIoU 60.48\n","Epoch time : 41.4s\n","Loss 0.4183,  Acc 82.43,  IoU 59.9597\n","EPOCH 41/100\n","Step [50/363], Loss: 0.3717, Acc : 85.15, mIoU 68.85\n","Step [100/363], Loss: 0.3411, Acc : 85.04, mIoU 67.52\n","Step [150/363], Loss: 0.4572, Acc : 85.11, mIoU 67.70\n","Step [200/363], Loss: 0.2986, Acc : 85.18, mIoU 68.06\n","Step [250/363], Loss: 0.3819, Acc : 85.05, mIoU 68.24\n","Step [300/363], Loss: 0.4807, Acc : 84.94, mIoU 68.03\n","Step [350/363], Loss: 0.3457, Acc : 84.86, mIoU 67.78\n","Epoch time : 337.3s\n","Validation . . . \n","Step [50/121], Loss: 0.5069, Acc : 83.01, mIoU 59.79\n","Step [100/121], Loss: 0.5235, Acc : 82.54, mIoU 58.88\n","Epoch time : 42.8s\n","Loss 0.6463,  Acc 82.33,  IoU 58.4864\n","EPOCH 42/100\n","Step [50/363], Loss: 0.3664, Acc : 85.92, mIoU 68.60\n","Step [100/363], Loss: 0.3512, Acc : 85.84, mIoU 70.20\n","Step [150/363], Loss: 0.2821, Acc : 85.31, mIoU 69.66\n","Step [200/363], Loss: 0.3097, Acc : 85.40, mIoU 69.84\n","Step [250/363], Loss: 0.4694, Acc : 85.41, mIoU 69.63\n","Step [300/363], Loss: 0.3034, Acc : 85.37, mIoU 69.52\n","Step [350/363], Loss: 0.5806, Acc : 85.43, mIoU 69.28\n","Epoch time : 345.0s\n","Validation . . . \n","Step [50/121], Loss: 0.5492, Acc : 81.71, mIoU 58.11\n","Step [100/121], Loss: 0.3958, Acc : 82.05, mIoU 58.54\n","Epoch time : 41.9s\n","Loss 0.4985,  Acc 82.15,  IoU 58.6790\n","EPOCH 43/100\n","Step [50/363], Loss: 0.4345, Acc : 86.16, mIoU 71.33\n","Step [100/363], Loss: 0.2135, Acc : 85.94, mIoU 70.81\n","Step [150/363], Loss: 0.3103, Acc : 85.94, mIoU 70.36\n","Step [200/363], Loss: 0.3916, Acc : 85.60, mIoU 69.97\n","Step [250/363], Loss: 0.4193, Acc : 85.49, mIoU 69.92\n","Step [300/363], Loss: 0.2446, Acc : 85.40, mIoU 69.76\n","Step [350/363], Loss: 0.3998, Acc : 85.43, mIoU 69.42\n","Epoch time : 342.5s\n","Validation . . . \n","Step [50/121], Loss: 0.6807, Acc : 82.18, mIoU 60.02\n","Step [100/121], Loss: 0.4896, Acc : 82.04, mIoU 59.14\n","Epoch time : 43.0s\n","Loss 0.4045,  Acc 82.30,  IoU 59.1820\n","EPOCH 44/100\n","Step [50/363], Loss: 0.3589, Acc : 85.91, mIoU 70.72\n","Step [100/363], Loss: 0.3384, Acc : 85.28, mIoU 68.59\n","Step [150/363], Loss: 0.3635, Acc : 85.42, mIoU 69.06\n","Step [200/363], Loss: 0.3702, Acc : 85.43, mIoU 68.75\n","Step [250/363], Loss: 0.3687, Acc : 85.36, mIoU 68.92\n","Step [300/363], Loss: 0.3368, Acc : 85.36, mIoU 69.06\n","Step [350/363], Loss: 0.3758, Acc : 85.20, mIoU 68.73\n","Epoch time : 340.9s\n","Validation . . . \n","Step [50/121], Loss: 0.5795, Acc : 82.06, mIoU 58.15\n","Step [100/121], Loss: 0.4806, Acc : 81.56, mIoU 57.75\n","Epoch time : 43.1s\n","Loss 0.3770,  Acc 81.80,  IoU 57.8690\n","EPOCH 45/100\n","Step [50/363], Loss: 0.2937, Acc : 85.78, mIoU 71.05\n","Step [100/363], Loss: 0.4210, Acc : 85.86, mIoU 71.67\n","Step [150/363], Loss: 0.4108, Acc : 85.79, mIoU 71.21\n","Step [200/363], Loss: 0.4049, Acc : 85.72, mIoU 71.21\n","Step [250/363], Loss: 0.4231, Acc : 85.68, mIoU 71.10\n","Step [300/363], Loss: 0.2466, Acc : 85.73, mIoU 71.25\n","Step [350/363], Loss: 0.2971, Acc : 85.77, mIoU 70.82\n","Epoch time : 341.8s\n","Validation . . . \n","Step [50/121], Loss: 0.4720, Acc : 82.47, mIoU 59.17\n","Step [100/121], Loss: 0.4635, Acc : 82.18, mIoU 58.88\n","Epoch time : 42.8s\n","Loss 0.5896,  Acc 82.03,  IoU 58.7078\n","EPOCH 46/100\n","Step [50/363], Loss: 0.3313, Acc : 86.50, mIoU 71.65\n","Step [100/363], Loss: 0.4741, Acc : 86.24, mIoU 71.28\n","Step [150/363], Loss: 0.4730, Acc : 86.22, mIoU 71.25\n","Step [200/363], Loss: 0.3285, Acc : 85.98, mIoU 71.27\n","Step [250/363], Loss: 0.3818, Acc : 86.00, mIoU 71.47\n","Step [300/363], Loss: 0.2726, Acc : 85.89, mIoU 71.36\n","Step [350/363], Loss: 0.5783, Acc : 85.90, mIoU 71.02\n","Epoch time : 345.1s\n","Validation . . . \n","Step [50/121], Loss: 0.5046, Acc : 82.08, mIoU 56.46\n","Step [100/121], Loss: 0.8649, Acc : 82.11, mIoU 56.78\n","Epoch time : 41.7s\n","Loss 0.6992,  Acc 81.83,  IoU 56.5614\n","EPOCH 47/100\n","Step [50/363], Loss: 0.3478, Acc : 86.17, mIoU 70.92\n","Step [100/363], Loss: 0.3540, Acc : 86.56, mIoU 71.00\n","Step [150/363], Loss: 0.3341, Acc : 86.41, mIoU 71.16\n","Step [200/363], Loss: 0.3621, Acc : 86.31, mIoU 70.58\n","Step [250/363], Loss: 0.3843, Acc : 86.30, mIoU 70.78\n","Step [300/363], Loss: 0.3971, Acc : 86.06, mIoU 70.67\n","Step [350/363], Loss: 0.3105, Acc : 86.08, mIoU 70.79\n","Epoch time : 342.7s\n","Validation . . . \n","Step [50/121], Loss: 0.6122, Acc : 82.18, mIoU 58.41\n","Step [100/121], Loss: 0.7415, Acc : 81.59, mIoU 59.00\n","Epoch time : 40.8s\n","Loss 0.4206,  Acc 81.56,  IoU 58.8034\n","EPOCH 48/100\n","Step [50/363], Loss: 0.4510, Acc : 86.54, mIoU 71.88\n","Step [100/363], Loss: 0.3564, Acc : 86.54, mIoU 72.64\n","Step [150/363], Loss: 0.3826, Acc : 86.59, mIoU 72.53\n","Step [200/363], Loss: 0.4040, Acc : 86.48, mIoU 72.16\n","Step [250/363], Loss: 0.4242, Acc : 86.51, mIoU 71.99\n","Step [300/363], Loss: 0.4629, Acc : 86.29, mIoU 71.95\n","Step [350/363], Loss: 0.4249, Acc : 86.27, mIoU 71.90\n","Epoch time : 347.4s\n","Validation . . . \n","Step [50/121], Loss: 0.4278, Acc : 82.81, mIoU 59.51\n","Step [100/121], Loss: 0.5181, Acc : 82.21, mIoU 58.51\n","Epoch time : 43.7s\n","Loss 0.3642,  Acc 82.16,  IoU 58.7822\n","EPOCH 49/100\n","Step [50/363], Loss: 0.2440, Acc : 85.91, mIoU 70.16\n","Step [100/363], Loss: 0.3467, Acc : 86.42, mIoU 71.81\n","Step [150/363], Loss: 0.4192, Acc : 86.49, mIoU 72.11\n","Step [200/363], Loss: 0.2954, Acc : 86.76, mIoU 73.05\n","Step [250/363], Loss: 0.5061, Acc : 86.63, mIoU 72.54\n","Step [300/363], Loss: 0.2561, Acc : 86.59, mIoU 72.36\n","Step [350/363], Loss: 0.3207, Acc : 86.50, mIoU 72.26\n","Epoch time : 344.7s\n","Validation . . . \n","Step [50/121], Loss: 0.9595, Acc : 82.12, mIoU 58.32\n","Step [100/121], Loss: 0.4275, Acc : 81.63, mIoU 58.82\n","Epoch time : 42.2s\n","Loss 0.4845,  Acc 81.73,  IoU 58.5539\n","EPOCH 50/100\n","Step [50/363], Loss: 0.4650, Acc : 87.16, mIoU 72.70\n","Step [100/363], Loss: 0.2097, Acc : 86.79, mIoU 72.33\n","Step [150/363], Loss: 0.1790, Acc : 86.65, mIoU 72.09\n","Step [200/363], Loss: 0.3978, Acc : 86.72, mIoU 72.38\n","Step [250/363], Loss: 0.2578, Acc : 86.67, mIoU 72.44\n","Step [300/363], Loss: 0.3638, Acc : 86.59, mIoU 72.64\n","Step [350/363], Loss: 0.4073, Acc : 86.51, mIoU 72.53\n","Epoch time : 347.0s\n","Validation . . . \n","Step [50/121], Loss: 0.3560, Acc : 81.50, mIoU 57.70\n","Step [100/121], Loss: 0.4552, Acc : 82.01, mIoU 58.56\n","Epoch time : 42.7s\n","Loss 0.4436,  Acc 82.15,  IoU 59.0091\n","EPOCH 51/100\n","Step [50/363], Loss: 0.5682, Acc : 86.65, mIoU 72.98\n","Step [100/363], Loss: 0.3271, Acc : 86.98, mIoU 73.50\n","Step [150/363], Loss: 0.4097, Acc : 86.76, mIoU 73.61\n","Step [200/363], Loss: 0.2938, Acc : 86.75, mIoU 73.18\n","Step [250/363], Loss: 0.4015, Acc : 86.64, mIoU 72.74\n","Step [300/363], Loss: 0.4717, Acc : 86.49, mIoU 72.49\n","Step [350/363], Loss: 0.3265, Acc : 86.51, mIoU 72.47\n","Epoch time : 340.1s\n","Validation . . . \n","Step [50/121], Loss: 0.4274, Acc : 81.59, mIoU 56.11\n","Step [100/121], Loss: 0.7228, Acc : 81.65, mIoU 57.11\n","Epoch time : 41.9s\n","Loss 0.4691,  Acc 81.91,  IoU 57.8633\n","EPOCH 52/100\n","Step [50/363], Loss: 0.4960, Acc : 86.34, mIoU 71.76\n","Step [100/363], Loss: 0.3309, Acc : 86.31, mIoU 73.18\n","Step [150/363], Loss: 0.2407, Acc : 86.63, mIoU 73.64\n","Step [200/363], Loss: 0.3464, Acc : 86.61, mIoU 73.47\n","Step [250/363], Loss: 0.2725, Acc : 86.61, mIoU 73.55\n","Step [300/363], Loss: 0.4472, Acc : 86.61, mIoU 73.31\n","Step [350/363], Loss: 0.4626, Acc : 86.57, mIoU 73.13\n","Epoch time : 346.3s\n","Validation . . . \n","Step [50/121], Loss: 0.4319, Acc : 82.19, mIoU 57.94\n","Step [100/121], Loss: 0.6640, Acc : 81.67, mIoU 58.52\n","Epoch time : 42.5s\n","Loss 0.5959,  Acc 82.00,  IoU 58.9161\n","EPOCH 53/100\n","Step [50/363], Loss: 0.3439, Acc : 85.54, mIoU 70.05\n","Step [100/363], Loss: 0.4184, Acc : 86.45, mIoU 72.61\n","Step [150/363], Loss: 0.4045, Acc : 86.75, mIoU 73.41\n","Step [200/363], Loss: 0.3845, Acc : 86.89, mIoU 73.48\n","Step [250/363], Loss: 0.3520, Acc : 86.76, mIoU 73.33\n","Step [300/363], Loss: 0.3333, Acc : 86.75, mIoU 73.26\n","Step [350/363], Loss: 0.4043, Acc : 86.77, mIoU 73.30\n","Epoch time : 340.2s\n","Validation . . . \n","Step [50/121], Loss: 0.4824, Acc : 82.03, mIoU 57.19\n","Step [100/121], Loss: 0.7502, Acc : 82.19, mIoU 58.19\n","Epoch time : 42.7s\n","Loss 0.4205,  Acc 82.08,  IoU 57.9290\n","EPOCH 54/100\n","Step [50/363], Loss: 0.3633, Acc : 87.14, mIoU 73.90\n","Step [100/363], Loss: 0.2757, Acc : 87.34, mIoU 73.77\n","Step [150/363], Loss: 0.3545, Acc : 87.63, mIoU 74.54\n","Step [200/363], Loss: 0.3285, Acc : 87.41, mIoU 74.94\n","Step [250/363], Loss: 0.2245, Acc : 87.32, mIoU 74.70\n","Step [300/363], Loss: 0.3217, Acc : 87.28, mIoU 74.66\n","Step [350/363], Loss: 0.4549, Acc : 87.17, mIoU 74.43\n","Epoch time : 343.4s\n","Validation . . . \n","Step [50/121], Loss: 0.5008, Acc : 81.72, mIoU 57.61\n","Step [100/121], Loss: 0.5138, Acc : 81.95, mIoU 58.67\n","Epoch time : 42.4s\n","Loss 0.8473,  Acc 81.88,  IoU 58.4829\n","EPOCH 55/100\n","Step [50/363], Loss: 0.4284, Acc : 86.91, mIoU 72.73\n","Step [100/363], Loss: 0.4397, Acc : 86.85, mIoU 74.10\n","Step [150/363], Loss: 0.4027, Acc : 86.91, mIoU 73.96\n","Step [200/363], Loss: 0.3740, Acc : 86.89, mIoU 74.21\n","Step [250/363], Loss: 0.4882, Acc : 87.04, mIoU 74.23\n","Step [300/363], Loss: 0.3274, Acc : 87.00, mIoU 74.13\n","Step [350/363], Loss: 0.2248, Acc : 86.98, mIoU 74.00\n","Epoch time : 341.9s\n","Validation . . . \n","Step [50/121], Loss: 0.4763, Acc : 81.97, mIoU 57.35\n","Step [100/121], Loss: 0.7245, Acc : 82.13, mIoU 57.92\n","Epoch time : 42.5s\n","Loss 0.5557,  Acc 82.23,  IoU 58.0384\n","EPOCH 56/100\n","Step [50/363], Loss: 0.3058, Acc : 87.32, mIoU 73.54\n","Step [100/363], Loss: 0.3889, Acc : 87.54, mIoU 74.00\n","Step [150/363], Loss: 0.3475, Acc : 87.51, mIoU 74.15\n","Step [200/363], Loss: 0.3065, Acc : 87.22, mIoU 74.02\n","Step [250/363], Loss: 0.4448, Acc : 87.24, mIoU 74.26\n","Step [300/363], Loss: 0.3520, Acc : 87.19, mIoU 74.06\n","Step [350/363], Loss: 0.3334, Acc : 87.02, mIoU 73.55\n","Epoch time : 341.4s\n","Validation . . . \n","Step [50/121], Loss: 0.3692, Acc : 81.54, mIoU 58.61\n","Step [100/121], Loss: 0.4950, Acc : 81.87, mIoU 58.48\n","Epoch time : 42.4s\n","Loss 0.9875,  Acc 81.59,  IoU 57.8652\n","EPOCH 57/100\n","Step [50/363], Loss: 0.2648, Acc : 86.96, mIoU 75.03\n","Step [100/363], Loss: 0.3385, Acc : 87.14, mIoU 74.17\n","Step [150/363], Loss: 0.4043, Acc : 87.44, mIoU 74.55\n","Step [200/363], Loss: 0.2835, Acc : 87.53, mIoU 74.57\n","Step [250/363], Loss: 0.3874, Acc : 87.46, mIoU 74.60\n","Step [300/363], Loss: 0.3816, Acc : 87.41, mIoU 74.81\n","Step [350/363], Loss: 0.4539, Acc : 87.38, mIoU 74.87\n","Epoch time : 346.1s\n","Validation . . . \n","Step [50/121], Loss: 0.8587, Acc : 81.26, mIoU 57.77\n","Step [100/121], Loss: 0.7081, Acc : 82.23, mIoU 59.58\n","Epoch time : 42.6s\n","Loss 0.3588,  Acc 81.87,  IoU 59.2906\n","EPOCH 58/100\n","Step [50/363], Loss: 0.3423, Acc : 87.79, mIoU 75.18\n","Step [100/363], Loss: 0.4498, Acc : 86.71, mIoU 73.63\n","Step [150/363], Loss: 0.3378, Acc : 86.95, mIoU 73.89\n","Step [200/363], Loss: 0.3060, Acc : 87.19, mIoU 74.42\n","Step [250/363], Loss: 0.2461, Acc : 87.26, mIoU 74.66\n","Step [300/363], Loss: 0.4376, Acc : 87.36, mIoU 74.78\n","Step [350/363], Loss: 0.3518, Acc : 87.30, mIoU 74.66\n","Epoch time : 340.5s\n","Validation . . . \n","Step [50/121], Loss: 0.5647, Acc : 82.64, mIoU 59.14\n","Step [100/121], Loss: 0.4230, Acc : 82.60, mIoU 59.44\n","Epoch time : 41.4s\n","Loss 0.8412,  Acc 82.21,  IoU 59.6941\n","EPOCH 59/100\n","Step [50/363], Loss: 0.3423, Acc : 88.09, mIoU 76.67\n","Step [100/363], Loss: 0.3699, Acc : 87.99, mIoU 76.62\n","Step [150/363], Loss: 0.3537, Acc : 87.84, mIoU 76.45\n","Step [200/363], Loss: 0.3669, Acc : 87.81, mIoU 75.86\n","Step [250/363], Loss: 0.4001, Acc : 87.72, mIoU 75.60\n","Step [300/363], Loss: 0.3449, Acc : 87.70, mIoU 75.59\n","Step [350/363], Loss: 0.6053, Acc : 87.61, mIoU 75.52\n","Epoch time : 345.2s\n","Validation . . . \n","Step [50/121], Loss: 0.5999, Acc : 81.51, mIoU 57.59\n","Step [100/121], Loss: 0.4575, Acc : 81.86, mIoU 58.13\n","Epoch time : 42.4s\n","Loss 0.5553,  Acc 81.99,  IoU 57.9818\n","EPOCH 60/100\n","Step [50/363], Loss: 0.2816, Acc : 88.21, mIoU 77.66\n","Step [100/363], Loss: 0.3435, Acc : 88.23, mIoU 77.37\n","Step [150/363], Loss: 0.2635, Acc : 88.20, mIoU 77.31\n","Step [200/363], Loss: 0.3480, Acc : 88.10, mIoU 76.91\n","Step [250/363], Loss: 0.1913, Acc : 88.07, mIoU 76.53\n","Step [300/363], Loss: 0.4075, Acc : 88.06, mIoU 76.59\n","Step [350/363], Loss: 0.4295, Acc : 87.95, mIoU 76.44\n","Epoch time : 343.5s\n","Validation . . . \n","Step [50/121], Loss: 0.5810, Acc : 81.91, mIoU 58.49\n","Step [100/121], Loss: 0.5997, Acc : 81.73, mIoU 58.71\n","Epoch time : 41.8s\n","Loss 0.6254,  Acc 81.60,  IoU 58.8772\n","EPOCH 61/100\n","Step [50/363], Loss: 0.2736, Acc : 87.88, mIoU 77.75\n","Step [100/363], Loss: 0.3573, Acc : 87.73, mIoU 77.07\n","Step [150/363], Loss: 0.3616, Acc : 87.98, mIoU 77.45\n","Step [200/363], Loss: 0.2400, Acc : 87.97, mIoU 77.34\n","Step [250/363], Loss: 0.2648, Acc : 87.97, mIoU 76.98\n","Step [300/363], Loss: 0.2931, Acc : 88.04, mIoU 76.77\n","Step [350/363], Loss: 0.4184, Acc : 88.03, mIoU 76.60\n","Epoch time : 348.0s\n","Validation . . . \n","Step [50/121], Loss: 0.4745, Acc : 82.15, mIoU 58.95\n","Step [100/121], Loss: 0.4910, Acc : 82.07, mIoU 59.20\n","Epoch time : 41.7s\n","Loss 0.3564,  Acc 81.93,  IoU 59.0327\n","EPOCH 62/100\n","Step [50/363], Loss: 0.2202, Acc : 88.26, mIoU 76.28\n","Step [100/363], Loss: 0.3178, Acc : 88.50, mIoU 77.52\n","Step [150/363], Loss: 0.3164, Acc : 88.19, mIoU 77.22\n","Step [200/363], Loss: 0.2465, Acc : 88.30, mIoU 77.21\n","Step [250/363], Loss: 0.5546, Acc : 88.26, mIoU 76.89\n","Step [300/363], Loss: 0.2741, Acc : 88.05, mIoU 76.10\n","Step [350/363], Loss: 0.4422, Acc : 87.78, mIoU 75.83\n","Epoch time : 342.4s\n","Validation . . . \n","Step [50/121], Loss: 0.5128, Acc : 82.18, mIoU 58.42\n","Step [100/121], Loss: 0.3908, Acc : 81.48, mIoU 58.27\n","Epoch time : 42.6s\n","Loss 0.5370,  Acc 81.63,  IoU 59.1103\n","EPOCH 63/100\n","Step [50/363], Loss: 0.4586, Acc : 88.17, mIoU 76.99\n","Step [100/363], Loss: 0.2880, Acc : 88.15, mIoU 76.53\n","Step [150/363], Loss: 0.3268, Acc : 88.18, mIoU 76.63\n","Step [200/363], Loss: 0.3280, Acc : 88.27, mIoU 76.96\n","Step [250/363], Loss: 0.2920, Acc : 88.27, mIoU 76.95\n","Step [300/363], Loss: 0.4139, Acc : 88.24, mIoU 77.05\n","Step [350/363], Loss: 0.2638, Acc : 88.25, mIoU 77.01\n","Epoch time : 343.9s\n","Validation . . . \n","Step [50/121], Loss: 0.3616, Acc : 81.87, mIoU 57.77\n","Step [100/121], Loss: 0.6661, Acc : 82.05, mIoU 58.36\n","Epoch time : 41.8s\n","Loss 0.5678,  Acc 82.01,  IoU 59.0245\n","EPOCH 64/100\n","Step [50/363], Loss: 0.4786, Acc : 88.32, mIoU 77.71\n","Step [100/363], Loss: 0.2368, Acc : 88.50, mIoU 78.01\n","Step [150/363], Loss: 0.3415, Acc : 88.54, mIoU 78.46\n","Step [200/363], Loss: 0.1788, Acc : 88.70, mIoU 78.66\n","Step [250/363], Loss: 0.3573, Acc : 88.75, mIoU 78.75\n","Step [300/363], Loss: 0.3169, Acc : 88.78, mIoU 78.61\n","Step [350/363], Loss: 0.2953, Acc : 88.68, mIoU 78.34\n","Epoch time : 341.4s\n","Validation . . . \n","Step [50/121], Loss: 0.4862, Acc : 82.06, mIoU 58.51\n","Step [100/121], Loss: 0.5639, Acc : 82.18, mIoU 58.80\n","Epoch time : 42.3s\n","Loss 0.3683,  Acc 81.88,  IoU 58.4625\n","EPOCH 65/100\n","Step [50/363], Loss: 0.2197, Acc : 88.72, mIoU 78.21\n","Step [100/363], Loss: 0.2700, Acc : 88.80, mIoU 78.52\n","Step [150/363], Loss: 0.3046, Acc : 88.52, mIoU 78.11\n","Step [200/363], Loss: 0.2844, Acc : 88.50, mIoU 77.95\n","Step [250/363], Loss: 0.2126, Acc : 88.46, mIoU 77.68\n","Step [300/363], Loss: 0.3036, Acc : 88.46, mIoU 77.52\n","Step [350/363], Loss: 0.2639, Acc : 88.48, mIoU 77.51\n","Epoch time : 342.2s\n","Validation . . . \n","Step [50/121], Loss: 0.6719, Acc : 81.29, mIoU 59.17\n","Step [100/121], Loss: 0.5427, Acc : 82.05, mIoU 59.80\n","Epoch time : 40.5s\n","Loss 0.6562,  Acc 82.26,  IoU 59.6674\n","EPOCH 66/100\n","Step [50/363], Loss: 0.3379, Acc : 89.23, mIoU 77.47\n","Step [100/363], Loss: 0.3523, Acc : 88.91, mIoU 78.11\n","Step [150/363], Loss: 0.4110, Acc : 88.81, mIoU 78.02\n","Step [200/363], Loss: 0.3955, Acc : 88.84, mIoU 78.04\n","Step [250/363], Loss: 0.2936, Acc : 88.74, mIoU 77.81\n","Step [300/363], Loss: 0.2684, Acc : 88.68, mIoU 77.73\n","Step [350/363], Loss: 0.2360, Acc : 88.41, mIoU 77.48\n","Epoch time : 344.0s\n","Validation . . . \n","Step [50/121], Loss: 0.3420, Acc : 81.19, mIoU 58.44\n","Step [100/121], Loss: 0.7463, Acc : 80.74, mIoU 58.38\n","Epoch time : 43.0s\n","Loss 0.6891,  Acc 80.89,  IoU 58.3016\n","EPOCH 67/100\n","Step [50/363], Loss: 0.2600, Acc : 88.39, mIoU 77.02\n","Step [100/363], Loss: 0.3441, Acc : 88.53, mIoU 77.84\n","Step [150/363], Loss: 0.2473, Acc : 88.69, mIoU 78.04\n","Step [200/363], Loss: 0.3581, Acc : 88.78, mIoU 78.37\n","Step [250/363], Loss: 0.3092, Acc : 88.67, mIoU 78.23\n","Step [300/363], Loss: 0.3158, Acc : 88.59, mIoU 78.07\n","Step [350/363], Loss: 0.2447, Acc : 88.59, mIoU 77.95\n","Epoch time : 342.3s\n","Validation . . . \n","Step [50/121], Loss: 0.5790, Acc : 82.38, mIoU 58.81\n","Step [100/121], Loss: 0.7035, Acc : 82.36, mIoU 59.81\n","Epoch time : 43.2s\n","Loss 0.3825,  Acc 82.25,  IoU 59.7715\n","EPOCH 68/100\n","Step [50/363], Loss: 0.2412, Acc : 88.99, mIoU 77.59\n","Step [100/363], Loss: 0.2580, Acc : 89.08, mIoU 78.86\n","Step [150/363], Loss: 0.3496, Acc : 88.96, mIoU 78.73\n","Step [200/363], Loss: 0.3261, Acc : 88.83, mIoU 78.30\n","Step [250/363], Loss: 0.2686, Acc : 88.96, mIoU 78.53\n","Step [300/363], Loss: 0.2511, Acc : 89.00, mIoU 78.58\n","Step [350/363], Loss: 0.2589, Acc : 89.06, mIoU 78.72\n","Epoch time : 343.2s\n","Validation . . . \n","Step [50/121], Loss: 0.5561, Acc : 82.25, mIoU 57.59\n","Step [100/121], Loss: 0.4800, Acc : 81.63, mIoU 57.89\n","Epoch time : 41.9s\n","Loss 0.5275,  Acc 81.58,  IoU 58.1782\n","EPOCH 69/100\n","Step [50/363], Loss: 0.2149, Acc : 90.02, mIoU 79.73\n","Step [100/363], Loss: 0.3001, Acc : 89.39, mIoU 79.77\n","Step [150/363], Loss: 0.3722, Acc : 89.14, mIoU 79.04\n","Step [200/363], Loss: 0.2464, Acc : 88.96, mIoU 78.66\n","Step [250/363], Loss: 0.2325, Acc : 88.98, mIoU 78.58\n","Step [300/363], Loss: 0.2795, Acc : 89.00, mIoU 78.62\n","Step [350/363], Loss: 0.3337, Acc : 88.87, mIoU 78.32\n","Epoch time : 345.2s\n","Validation . . . \n","Step [50/121], Loss: 0.5787, Acc : 81.74, mIoU 57.71\n","Step [100/121], Loss: 0.5402, Acc : 81.33, mIoU 56.83\n","Epoch time : 42.6s\n","Loss 0.4197,  Acc 81.54,  IoU 57.5549\n","EPOCH 70/100\n","Step [50/363], Loss: 0.2592, Acc : 89.17, mIoU 78.20\n","Step [100/363], Loss: 0.3487, Acc : 89.30, mIoU 79.21\n","Step [150/363], Loss: 0.3256, Acc : 88.99, mIoU 78.53\n","Step [200/363], Loss: 0.2557, Acc : 88.84, mIoU 78.28\n","Step [250/363], Loss: 0.2589, Acc : 88.85, mIoU 78.24\n","Step [300/363], Loss: 0.2758, Acc : 88.87, mIoU 78.32\n","Step [350/363], Loss: 0.3467, Acc : 88.94, mIoU 78.51\n","Epoch time : 343.2s\n","Validation . . . \n","Step [50/121], Loss: 0.7617, Acc : 81.31, mIoU 57.36\n","Step [100/121], Loss: 0.4561, Acc : 81.57, mIoU 58.61\n","Epoch time : 42.1s\n","Loss 0.4740,  Acc 81.89,  IoU 58.6372\n","EPOCH 71/100\n","Step [50/363], Loss: 0.2586, Acc : 89.19, mIoU 79.74\n","Step [100/363], Loss: 0.3603, Acc : 89.46, mIoU 80.31\n","Step [150/363], Loss: 0.2751, Acc : 88.95, mIoU 78.85\n","Step [200/363], Loss: 0.3165, Acc : 88.82, mIoU 78.66\n","Step [250/363], Loss: 0.3216, Acc : 89.01, mIoU 78.87\n","Step [300/363], Loss: 0.2346, Acc : 89.00, mIoU 78.81\n","Step [350/363], Loss: 0.3187, Acc : 88.93, mIoU 78.49\n","Epoch time : 344.0s\n","Validation . . . \n","Step [50/121], Loss: 0.5495, Acc : 81.20, mIoU 58.06\n","Step [100/121], Loss: 0.8138, Acc : 81.48, mIoU 59.07\n","Epoch time : 42.3s\n","Loss 0.5676,  Acc 81.47,  IoU 58.8053\n","EPOCH 72/100\n","Step [50/363], Loss: 0.2885, Acc : 89.65, mIoU 79.34\n","Step [100/363], Loss: 0.2486, Acc : 89.26, mIoU 78.73\n","Step [150/363], Loss: 0.2587, Acc : 89.16, mIoU 79.29\n","Step [200/363], Loss: 0.2324, Acc : 89.29, mIoU 79.85\n","Step [250/363], Loss: 0.1849, Acc : 89.34, mIoU 79.96\n","Step [300/363], Loss: 0.2281, Acc : 89.23, mIoU 79.80\n","Step [350/363], Loss: 0.2480, Acc : 89.32, mIoU 79.91\n","Epoch time : 345.0s\n","Validation . . . \n","Step [50/121], Loss: 0.7844, Acc : 82.49, mIoU 59.10\n","Step [100/121], Loss: 0.4616, Acc : 82.11, mIoU 59.73\n","Epoch time : 41.3s\n","Loss 0.6436,  Acc 81.77,  IoU 59.2765\n","EPOCH 73/100\n","Step [50/363], Loss: 0.3777, Acc : 89.24, mIoU 79.14\n","Step [100/363], Loss: 0.2342, Acc : 89.71, mIoU 80.76\n","Step [150/363], Loss: 0.2839, Acc : 89.81, mIoU 80.72\n","Step [200/363], Loss: 0.2913, Acc : 89.65, mIoU 80.34\n","Step [250/363], Loss: 0.2444, Acc : 89.53, mIoU 79.86\n","Step [300/363], Loss: 0.2638, Acc : 89.44, mIoU 79.53\n","Step [350/363], Loss: 0.2315, Acc : 89.41, mIoU 79.59\n","Epoch time : 338.9s\n","Validation . . . \n","Step [50/121], Loss: 0.5768, Acc : 81.09, mIoU 56.72\n","Step [100/121], Loss: 0.7257, Acc : 81.22, mIoU 57.23\n","Epoch time : 41.6s\n","Loss 0.5838,  Acc 81.21,  IoU 57.3846\n","EPOCH 74/100\n","Step [50/363], Loss: 0.3014, Acc : 88.86, mIoU 77.60\n","Step [100/363], Loss: 0.2805, Acc : 89.05, mIoU 77.94\n","Step [150/363], Loss: 0.2506, Acc : 89.26, mIoU 78.76\n","Step [200/363], Loss: 0.3039, Acc : 89.33, mIoU 79.32\n","Step [250/363], Loss: 0.2342, Acc : 89.26, mIoU 79.36\n","Step [300/363], Loss: 0.2631, Acc : 89.29, mIoU 79.52\n","Step [350/363], Loss: 0.3140, Acc : 89.38, mIoU 79.54\n","Epoch time : 345.1s\n","Validation . . . \n","Step [50/121], Loss: 0.5859, Acc : 81.35, mIoU 58.62\n","Step [100/121], Loss: 0.6147, Acc : 81.03, mIoU 57.87\n","Epoch time : 42.2s\n","Loss 0.4569,  Acc 81.32,  IoU 57.9620\n","EPOCH 75/100\n","Step [50/363], Loss: 0.3414, Acc : 89.91, mIoU 79.30\n","Step [100/363], Loss: 0.2201, Acc : 89.82, mIoU 79.77\n","Step [150/363], Loss: 0.2307, Acc : 89.63, mIoU 79.76\n","Step [200/363], Loss: 0.2638, Acc : 89.61, mIoU 80.30\n","Step [250/363], Loss: 0.2587, Acc : 89.70, mIoU 80.45\n","Step [300/363], Loss: 0.2788, Acc : 89.70, mIoU 80.34\n","Step [350/363], Loss: 0.3203, Acc : 89.72, mIoU 80.28\n","Epoch time : 343.2s\n","Validation . . . \n","Step [50/121], Loss: 0.5722, Acc : 80.99, mIoU 58.61\n","Step [100/121], Loss: 0.4698, Acc : 81.42, mIoU 58.64\n","Epoch time : 42.3s\n","Loss 0.7220,  Acc 81.36,  IoU 58.8575\n","EPOCH 76/100\n","Step [50/363], Loss: 0.2545, Acc : 90.27, mIoU 80.56\n","Step [100/363], Loss: 0.2305, Acc : 90.14, mIoU 80.65\n","Step [150/363], Loss: 0.2009, Acc : 89.93, mIoU 80.77\n","Step [200/363], Loss: 0.1704, Acc : 89.75, mIoU 80.48\n","Step [250/363], Loss: 0.2269, Acc : 89.70, mIoU 80.21\n","Step [300/363], Loss: 0.1812, Acc : 89.68, mIoU 80.16\n","Step [350/363], Loss: 0.2407, Acc : 89.65, mIoU 80.10\n","Epoch time : 345.7s\n","Validation . . . \n","Step [50/121], Loss: 0.5434, Acc : 82.42, mIoU 60.03\n","Step [100/121], Loss: 0.9613, Acc : 81.77, mIoU 59.08\n","Epoch time : 42.4s\n","Loss 0.3570,  Acc 81.94,  IoU 59.4136\n","EPOCH 77/100\n","Step [50/363], Loss: 0.2769, Acc : 89.42, mIoU 80.63\n","Step [100/363], Loss: 0.3498, Acc : 89.35, mIoU 80.22\n","Step [150/363], Loss: 0.2800, Acc : 89.70, mIoU 80.87\n","Step [200/363], Loss: 0.2457, Acc : 89.82, mIoU 81.03\n","Step [250/363], Loss: 0.3117, Acc : 89.72, mIoU 80.78\n","Step [300/363], Loss: 0.1915, Acc : 89.62, mIoU 80.67\n","Step [350/363], Loss: 0.3499, Acc : 89.58, mIoU 80.49\n","Epoch time : 340.6s\n","Validation . . . \n","Step [50/121], Loss: 0.6910, Acc : 80.41, mIoU 56.33\n","Step [100/121], Loss: 0.4932, Acc : 81.01, mIoU 57.14\n","Epoch time : 41.3s\n","Loss 0.7018,  Acc 81.03,  IoU 56.8035\n","EPOCH 78/100\n","Step [50/363], Loss: 0.1831, Acc : 89.98, mIoU 80.03\n","Step [100/363], Loss: 0.2134, Acc : 90.42, mIoU 81.12\n","Step [150/363], Loss: 0.2698, Acc : 90.01, mIoU 80.78\n","Step [200/363], Loss: 0.3327, Acc : 89.97, mIoU 80.66\n","Step [250/363], Loss: 0.2440, Acc : 89.80, mIoU 80.46\n","Step [300/363], Loss: 0.2668, Acc : 89.78, mIoU 80.31\n","Step [350/363], Loss: 0.2635, Acc : 89.77, mIoU 80.51\n","Epoch time : 346.4s\n","Validation . . . \n","Step [50/121], Loss: 0.6807, Acc : 82.70, mIoU 59.92\n","Step [100/121], Loss: 0.9356, Acc : 82.08, mIoU 58.79\n","Epoch time : 42.5s\n","Loss 0.9430,  Acc 81.80,  IoU 58.5250\n","EPOCH 79/100\n","Step [50/363], Loss: 0.2524, Acc : 90.12, mIoU 82.02\n","Step [100/363], Loss: 0.2552, Acc : 90.12, mIoU 82.20\n","Step [150/363], Loss: 0.3192, Acc : 90.09, mIoU 81.85\n","Step [200/363], Loss: 0.3421, Acc : 89.83, mIoU 80.78\n","Step [250/363], Loss: 0.3072, Acc : 89.46, mIoU 79.43\n","Step [300/363], Loss: 0.2304, Acc : 89.55, mIoU 79.57\n","Step [350/363], Loss: 0.3828, Acc : 89.49, mIoU 79.46\n","Epoch time : 344.1s\n","Validation . . . \n","Step [50/121], Loss: 0.6115, Acc : 81.92, mIoU 59.24\n","Step [100/121], Loss: 0.6215, Acc : 81.88, mIoU 58.73\n","Epoch time : 41.2s\n","Loss 0.4574,  Acc 81.78,  IoU 58.9419\n","EPOCH 80/100\n","Step [50/363], Loss: 0.3018, Acc : 90.07, mIoU 80.81\n","Step [100/363], Loss: 0.3473, Acc : 90.01, mIoU 81.18\n","Step [150/363], Loss: 0.3193, Acc : 89.96, mIoU 80.99\n","Step [200/363], Loss: 0.2270, Acc : 89.98, mIoU 81.03\n","Step [250/363], Loss: 0.2732, Acc : 90.11, mIoU 81.09\n","Step [300/363], Loss: 0.3334, Acc : 89.96, mIoU 80.86\n","Step [350/363], Loss: 0.4592, Acc : 89.90, mIoU 80.81\n","Epoch time : 341.2s\n","Validation . . . \n","Step [50/121], Loss: 0.8119, Acc : 81.61, mIoU 58.66\n","Step [100/121], Loss: 0.3416, Acc : 81.77, mIoU 59.01\n","Epoch time : 42.1s\n","Loss 0.4968,  Acc 81.59,  IoU 58.3962\n","EPOCH 81/100\n","Step [50/363], Loss: 0.2351, Acc : 90.06, mIoU 81.91\n","Step [100/363], Loss: 0.1672, Acc : 89.98, mIoU 81.29\n","Step [150/363], Loss: 0.2840, Acc : 90.24, mIoU 81.47\n","Step [200/363], Loss: 0.1986, Acc : 90.32, mIoU 81.69\n","Step [250/363], Loss: 0.2493, Acc : 90.37, mIoU 81.72\n","Step [300/363], Loss: 0.2447, Acc : 90.31, mIoU 81.75\n","Step [350/363], Loss: 0.2270, Acc : 90.31, mIoU 81.78\n","Epoch time : 344.8s\n","Validation . . . \n","Step [50/121], Loss: 0.6390, Acc : 81.89, mIoU 57.42\n","Step [100/121], Loss: 0.4799, Acc : 81.89, mIoU 58.57\n","Epoch time : 41.4s\n","Loss 0.8053,  Acc 81.83,  IoU 58.7982\n","EPOCH 82/100\n","Step [50/363], Loss: 0.2445, Acc : 90.80, mIoU 82.94\n","Step [100/363], Loss: 0.2351, Acc : 90.63, mIoU 82.86\n","Step [150/363], Loss: 0.2810, Acc : 90.58, mIoU 82.57\n","Step [200/363], Loss: 0.2226, Acc : 90.53, mIoU 82.60\n","Step [250/363], Loss: 0.2224, Acc : 90.54, mIoU 82.48\n","Step [300/363], Loss: 0.2466, Acc : 90.52, mIoU 82.23\n","Step [350/363], Loss: 0.2527, Acc : 90.41, mIoU 82.15\n","Epoch time : 342.5s\n","Validation . . . \n","Step [50/121], Loss: 0.7539, Acc : 81.67, mIoU 57.81\n","Step [100/121], Loss: 0.4267, Acc : 81.57, mIoU 57.69\n","Epoch time : 42.5s\n","Loss 0.7157,  Acc 81.57,  IoU 57.9601\n","EPOCH 83/100\n","Step [50/363], Loss: 0.1687, Acc : 90.45, mIoU 82.64\n","Step [100/363], Loss: 0.2220, Acc : 90.87, mIoU 82.55\n","Step [150/363], Loss: 0.2886, Acc : 90.64, mIoU 82.16\n","Step [200/363], Loss: 0.2311, Acc : 90.65, mIoU 82.41\n","Step [250/363], Loss: 0.2123, Acc : 90.63, mIoU 82.15\n","Step [300/363], Loss: 0.1881, Acc : 90.63, mIoU 82.34\n","Step [350/363], Loss: 0.3499, Acc : 90.43, mIoU 81.67\n","Epoch time : 342.4s\n","Validation . . . \n","Step [50/121], Loss: 0.4979, Acc : 80.86, mIoU 56.69\n","Step [100/121], Loss: 0.4587, Acc : 80.32, mIoU 55.86\n","Epoch time : 41.9s\n","Loss 0.6203,  Acc 80.55,  IoU 56.8492\n","EPOCH 84/100\n","Step [50/363], Loss: 0.2192, Acc : 90.86, mIoU 81.17\n","Step [100/363], Loss: 0.2233, Acc : 90.74, mIoU 81.84\n","Step [150/363], Loss: 0.2895, Acc : 90.58, mIoU 81.66\n","Step [200/363], Loss: 0.2319, Acc : 90.16, mIoU 80.83\n","Step [250/363], Loss: 0.2900, Acc : 90.06, mIoU 80.84\n","Step [300/363], Loss: 0.2490, Acc : 89.96, mIoU 80.75\n","Step [350/363], Loss: 0.2573, Acc : 89.90, mIoU 80.76\n","Epoch time : 346.0s\n","Validation . . . \n","Step [50/121], Loss: 0.6321, Acc : 81.44, mIoU 54.96\n","Step [100/121], Loss: 0.5052, Acc : 81.86, mIoU 56.96\n","Epoch time : 42.8s\n","Loss 0.8889,  Acc 81.90,  IoU 57.0973\n","EPOCH 85/100\n","Step [50/363], Loss: 0.2057, Acc : 90.40, mIoU 81.89\n","Step [100/363], Loss: 0.1916, Acc : 90.31, mIoU 81.76\n","Step [150/363], Loss: 0.3241, Acc : 90.35, mIoU 81.76\n","Step [200/363], Loss: 0.2819, Acc : 90.47, mIoU 82.11\n","Step [250/363], Loss: 0.2806, Acc : 90.38, mIoU 81.87\n","Step [300/363], Loss: 0.2020, Acc : 90.41, mIoU 82.02\n","Step [350/363], Loss: 0.2113, Acc : 90.41, mIoU 82.05\n","Epoch time : 340.7s\n","Validation . . . \n","Step [50/121], Loss: 0.4337, Acc : 81.67, mIoU 58.05\n","Step [100/121], Loss: 0.7609, Acc : 80.66, mIoU 58.74\n","Epoch time : 41.0s\n","Loss 0.3219,  Acc 80.87,  IoU 58.5971\n","EPOCH 86/100\n","Step [50/363], Loss: 0.3332, Acc : 90.26, mIoU 81.08\n","Step [100/363], Loss: 0.2034, Acc : 90.18, mIoU 81.45\n","Step [150/363], Loss: 0.2199, Acc : 90.32, mIoU 81.77\n","Step [200/363], Loss: 0.2548, Acc : 90.37, mIoU 81.72\n","Step [250/363], Loss: 0.2243, Acc : 90.47, mIoU 81.88\n","Step [300/363], Loss: 0.2285, Acc : 90.49, mIoU 82.00\n","Step [350/363], Loss: 0.2749, Acc : 90.51, mIoU 81.92\n","Epoch time : 347.2s\n","Validation . . . \n","Step [50/121], Loss: 0.7599, Acc : 80.68, mIoU 57.26\n","Step [100/121], Loss: 0.9838, Acc : 81.21, mIoU 57.95\n","Epoch time : 43.5s\n","Loss 0.7819,  Acc 80.68,  IoU 57.1840\n","EPOCH 87/100\n","Step [50/363], Loss: 0.3153, Acc : 90.05, mIoU 80.40\n","Step [100/363], Loss: 0.2748, Acc : 90.19, mIoU 80.37\n","Step [150/363], Loss: 0.1963, Acc : 90.19, mIoU 80.68\n","Step [200/363], Loss: 0.2897, Acc : 90.21, mIoU 80.98\n","Step [250/363], Loss: 0.2519, Acc : 90.18, mIoU 80.93\n","Step [300/363], Loss: 0.2109, Acc : 90.27, mIoU 81.18\n","Step [350/363], Loss: 0.1658, Acc : 90.39, mIoU 81.38\n","Epoch time : 341.6s\n","Validation . . . \n","Step [50/121], Loss: 0.4610, Acc : 81.55, mIoU 56.43\n","Step [100/121], Loss: 0.6057, Acc : 81.99, mIoU 58.33\n","Epoch time : 42.8s\n","Loss 0.5086,  Acc 81.82,  IoU 58.1687\n","EPOCH 88/100\n","Step [50/363], Loss: 0.2565, Acc : 90.80, mIoU 83.24\n","Step [100/363], Loss: 0.2439, Acc : 91.00, mIoU 83.46\n","Step [150/363], Loss: 0.2407, Acc : 91.10, mIoU 83.41\n","Step [200/363], Loss: 0.2396, Acc : 91.04, mIoU 83.41\n","Step [250/363], Loss: 0.1950, Acc : 90.99, mIoU 83.55\n","Step [300/363], Loss: 0.3223, Acc : 90.78, mIoU 83.13\n","Step [350/363], Loss: 0.2962, Acc : 90.79, mIoU 82.97\n","Epoch time : 346.3s\n","Validation . . . \n","Step [50/121], Loss: 0.6269, Acc : 81.18, mIoU 56.96\n","Step [100/121], Loss: 0.7069, Acc : 81.47, mIoU 57.16\n","Epoch time : 42.0s\n","Loss 0.6655,  Acc 81.22,  IoU 57.1622\n","EPOCH 89/100\n","Step [50/363], Loss: 0.3332, Acc : 89.67, mIoU 81.04\n","Step [100/363], Loss: 0.2092, Acc : 90.39, mIoU 82.03\n","Step [150/363], Loss: 0.2214, Acc : 90.27, mIoU 81.84\n","Step [200/363], Loss: 0.2402, Acc : 90.27, mIoU 81.71\n","Step [250/363], Loss: 0.2671, Acc : 90.36, mIoU 81.63\n","Step [300/363], Loss: 0.2211, Acc : 90.46, mIoU 81.93\n","Step [350/363], Loss: 0.2507, Acc : 90.50, mIoU 82.15\n","Epoch time : 345.7s\n","Validation . . . \n","Step [50/121], Loss: 0.3137, Acc : 81.37, mIoU 56.66\n","Step [100/121], Loss: 0.7801, Acc : 81.16, mIoU 57.19\n","Epoch time : 42.5s\n","Loss 0.3794,  Acc 81.09,  IoU 57.5074\n","EPOCH 90/100\n","Step [50/363], Loss: 0.2645, Acc : 90.56, mIoU 82.22\n","Step [100/363], Loss: 0.3114, Acc : 90.60, mIoU 82.60\n","Step [150/363], Loss: 0.2221, Acc : 90.78, mIoU 82.63\n","Step [200/363], Loss: 0.2449, Acc : 90.54, mIoU 82.67\n","Step [250/363], Loss: 0.2136, Acc : 90.48, mIoU 82.43\n","Step [300/363], Loss: 0.2405, Acc : 90.53, mIoU 82.30\n","Step [350/363], Loss: 0.2100, Acc : 90.59, mIoU 82.45\n","Epoch time : 341.9s\n","Validation . . . \n","Step [50/121], Loss: 0.3979, Acc : 81.43, mIoU 59.12\n","Step [100/121], Loss: 0.5853, Acc : 81.79, mIoU 59.01\n","Epoch time : 42.8s\n","Loss 0.6973,  Acc 81.58,  IoU 58.4325\n","EPOCH 91/100\n","Step [50/363], Loss: 0.1832, Acc : 90.92, mIoU 82.82\n","Step [100/363], Loss: 0.1776, Acc : 91.03, mIoU 83.36\n","Step [150/363], Loss: 0.2229, Acc : 90.97, mIoU 83.04\n","Step [200/363], Loss: 0.2678, Acc : 91.02, mIoU 83.04\n","Step [250/363], Loss: 0.1690, Acc : 91.01, mIoU 82.99\n","Step [300/363], Loss: 0.3422, Acc : 91.00, mIoU 83.02\n","Step [350/363], Loss: 0.2139, Acc : 91.08, mIoU 83.19\n","Epoch time : 343.5s\n","Validation . . . \n","Step [50/121], Loss: 0.6910, Acc : 81.37, mIoU 56.19\n","Step [100/121], Loss: 0.7743, Acc : 81.16, mIoU 56.97\n","Epoch time : 41.7s\n","Loss 0.6103,  Acc 81.26,  IoU 57.6234\n","EPOCH 92/100\n","Step [50/363], Loss: 0.2607, Acc : 91.11, mIoU 83.18\n","Step [100/363], Loss: 0.2379, Acc : 90.74, mIoU 83.48\n","Step [150/363], Loss: 0.2675, Acc : 90.57, mIoU 82.45\n","Step [200/363], Loss: 0.2579, Acc : 90.70, mIoU 82.67\n","Step [250/363], Loss: 0.2110, Acc : 90.83, mIoU 82.60\n","Step [300/363], Loss: 0.1744, Acc : 90.91, mIoU 82.86\n","Step [350/363], Loss: 0.1926, Acc : 90.92, mIoU 82.78\n","Epoch time : 347.1s\n","Validation . . . \n","Step [50/121], Loss: 1.0567, Acc : 81.12, mIoU 56.57\n","Step [100/121], Loss: 0.6752, Acc : 81.72, mIoU 57.72\n","Epoch time : 43.0s\n","Loss 0.8880,  Acc 81.64,  IoU 58.2899\n","EPOCH 93/100\n","Step [50/363], Loss: 0.2106, Acc : 90.99, mIoU 83.16\n","Step [100/363], Loss: 0.2529, Acc : 91.04, mIoU 83.31\n","Step [150/363], Loss: 0.2138, Acc : 91.24, mIoU 83.66\n","Step [200/363], Loss: 0.2564, Acc : 91.24, mIoU 83.52\n","Step [250/363], Loss: 0.2522, Acc : 91.26, mIoU 83.72\n","Step [300/363], Loss: 0.1715, Acc : 91.22, mIoU 83.52\n","Step [350/363], Loss: 0.1790, Acc : 91.13, mIoU 83.32\n","Epoch time : 345.4s\n","Validation . . . \n","Step [50/121], Loss: 0.5575, Acc : 81.91, mIoU 58.64\n","Step [100/121], Loss: 1.0713, Acc : 81.82, mIoU 59.40\n","Epoch time : 42.7s\n","Loss 0.5212,  Acc 81.81,  IoU 59.3529\n","EPOCH 94/100\n","Step [50/363], Loss: 0.2653, Acc : 90.92, mIoU 83.78\n","Step [100/363], Loss: 0.1852, Acc : 90.85, mIoU 83.70\n","Step [150/363], Loss: 0.2480, Acc : 90.89, mIoU 83.27\n","Step [200/363], Loss: 0.3355, Acc : 91.06, mIoU 83.46\n","Step [250/363], Loss: 0.2395, Acc : 91.09, mIoU 83.39\n","Step [300/363], Loss: 0.3456, Acc : 90.99, mIoU 83.24\n","Step [350/363], Loss: 0.2827, Acc : 90.98, mIoU 83.06\n","Epoch time : 341.5s\n","Validation . . . \n","Step [50/121], Loss: 0.8876, Acc : 81.41, mIoU 57.48\n","Step [100/121], Loss: 0.8515, Acc : 81.60, mIoU 57.48\n","Epoch time : 42.4s\n","Loss 0.9713,  Acc 81.57,  IoU 57.3481\n","EPOCH 95/100\n","Step [50/363], Loss: 0.2183, Acc : 91.50, mIoU 83.53\n","Step [100/363], Loss: 0.2612, Acc : 91.25, mIoU 83.11\n","Step [150/363], Loss: 0.2940, Acc : 91.36, mIoU 83.35\n","Step [200/363], Loss: 0.1580, Acc : 91.22, mIoU 83.38\n","Step [250/363], Loss: 0.2329, Acc : 91.10, mIoU 83.23\n","Step [300/363], Loss: 0.1795, Acc : 91.14, mIoU 83.41\n","Step [350/363], Loss: 0.2303, Acc : 91.12, mIoU 83.40\n","Epoch time : 345.7s\n","Validation . . . \n","Step [50/121], Loss: 0.5116, Acc : 81.56, mIoU 57.13\n","Step [100/121], Loss: 0.7756, Acc : 81.61, mIoU 57.26\n","Epoch time : 42.3s\n","Loss 1.0487,  Acc 81.43,  IoU 57.5672\n","EPOCH 96/100\n","Step [50/363], Loss: 0.1907, Acc : 91.96, mIoU 84.44\n","Step [100/363], Loss: 0.2640, Acc : 91.53, mIoU 83.57\n","Step [150/363], Loss: 0.3267, Acc : 91.40, mIoU 83.47\n","Step [200/363], Loss: 0.2106, Acc : 91.46, mIoU 83.59\n","Step [250/363], Loss: 0.2742, Acc : 91.37, mIoU 83.68\n","Step [300/363], Loss: 0.1946, Acc : 91.37, mIoU 83.69\n","Step [350/363], Loss: 0.2311, Acc : 91.30, mIoU 83.58\n","Epoch time : 346.9s\n","Validation . . . \n","Step [50/121], Loss: 0.4936, Acc : 80.99, mIoU 57.09\n","Step [100/121], Loss: 0.6687, Acc : 81.27, mIoU 57.15\n","Epoch time : 42.1s\n","Loss 0.5349,  Acc 81.33,  IoU 57.2155\n","EPOCH 97/100\n","Step [50/363], Loss: 0.2631, Acc : 91.09, mIoU 83.86\n","Step [100/363], Loss: 0.2267, Acc : 91.16, mIoU 83.61\n","Step [150/363], Loss: 0.2519, Acc : 91.37, mIoU 84.09\n","Step [200/363], Loss: 0.2322, Acc : 91.39, mIoU 83.87\n","Step [250/363], Loss: 0.2311, Acc : 91.39, mIoU 83.75\n","Step [300/363], Loss: 0.1721, Acc : 91.42, mIoU 83.83\n","Step [350/363], Loss: 0.2913, Acc : 91.39, mIoU 83.91\n","Epoch time : 344.2s\n","Validation . . . \n","Step [50/121], Loss: 1.0107, Acc : 81.28, mIoU 57.20\n","Step [100/121], Loss: 0.5226, Acc : 81.65, mIoU 57.93\n","Epoch time : 42.1s\n","Loss 0.7879,  Acc 81.80,  IoU 58.3331\n","EPOCH 98/100\n","Step [50/363], Loss: 0.1751, Acc : 91.89, mIoU 84.08\n","Step [100/363], Loss: 0.1623, Acc : 91.97, mIoU 84.82\n","Step [150/363], Loss: 0.2432, Acc : 91.80, mIoU 84.77\n","Step [200/363], Loss: 0.2052, Acc : 91.71, mIoU 84.46\n","Step [250/363], Loss: 0.2596, Acc : 91.65, mIoU 84.32\n","Step [300/363], Loss: 0.2197, Acc : 91.52, mIoU 84.03\n","Step [350/363], Loss: 0.2991, Acc : 91.43, mIoU 84.03\n","Epoch time : 343.5s\n","Validation . . . \n","Step [50/121], Loss: 0.6758, Acc : 81.50, mIoU 56.81\n","Step [100/121], Loss: 0.6340, Acc : 81.88, mIoU 58.22\n","Epoch time : 42.6s\n","Loss 0.4632,  Acc 81.69,  IoU 58.0474\n","EPOCH 99/100\n","Step [50/363], Loss: 0.2191, Acc : 91.57, mIoU 85.16\n","Step [100/363], Loss: 0.2003, Acc : 91.74, mIoU 84.88\n","Step [150/363], Loss: 0.2070, Acc : 91.73, mIoU 84.91\n","Step [200/363], Loss: 0.2209, Acc : 91.78, mIoU 84.79\n","Step [250/363], Loss: 0.1876, Acc : 91.75, mIoU 84.61\n","Step [300/363], Loss: 0.2525, Acc : 91.70, mIoU 84.37\n","Step [350/363], Loss: 0.3374, Acc : 91.65, mIoU 84.42\n","Epoch time : 343.6s\n","Validation . . . \n","Step [50/121], Loss: 0.5351, Acc : 81.42, mIoU 56.21\n","Step [100/121], Loss: 0.5609, Acc : 81.42, mIoU 56.22\n","Epoch time : 41.6s\n","Loss 0.6659,  Acc 81.26,  IoU 56.3665\n","EPOCH 100/100\n","Step [50/363], Loss: 0.2313, Acc : 91.60, mIoU 84.48\n","Step [100/363], Loss: 0.1914, Acc : 91.70, mIoU 84.98\n","Step [150/363], Loss: 0.2325, Acc : 91.62, mIoU 84.51\n","Step [200/363], Loss: 0.2353, Acc : 91.40, mIoU 84.06\n","Step [250/363], Loss: 0.2905, Acc : 91.31, mIoU 83.29\n","Step [300/363], Loss: 0.2093, Acc : 91.21, mIoU 82.87\n","Step [350/363], Loss: 0.1915, Acc : 91.25, mIoU 82.99\n","Epoch time : 345.0s\n","Validation . . . \n","Step [50/121], Loss: 0.5403, Acc : 81.64, mIoU 59.41\n","Step [100/121], Loss: 0.6763, Acc : 81.30, mIoU 58.76\n","Epoch time : 42.7s\n","Loss 0.7180,  Acc 81.18,  IoU 59.0509\n","Testing best epoch . . .\n","Step [50/123], Loss: 0.4681, Acc : 81.48, mIoU 59.85\n","Step [100/123], Loss: 0.4995, Acc : 81.71, mIoU 59.60\n","Epoch time : 43.7s\n","Loss 0.3730,  Acc 81.80,  IoU 58.8474\n","Reading patch metadata . . .\n","Done.\n","Dataset ready.\n","Reading patch metadata . . .\n","Done.\n","Dataset ready.\n","Reading patch metadata . . .\n","Done.\n","Dataset ready.\n","Train 366, Val 123, Test 118\n","Model has 1553796 trainable params\n","UNet3D(\n","  (en3): Sequential(\n","    (0): Conv3d(10, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (pool_3): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (en4): Sequential(\n","    (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (pool_4): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (center_in): Sequential(\n","    (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (center_out): Sequential(\n","    (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): ConvTranspose3d(128, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n","  )\n","  (dc4): Sequential(\n","    (0): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (trans3): Sequential(\n","    (0): ConvTranspose3d(64, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n","    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (dc3): Sequential(\n","    (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (4): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (final): Conv3d(16, 20, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",")\n","EPOCH 1/100\n","Step [50/366], Loss: 1.3178, Acc : 42.69, mIoU 4.29\n","Step [100/366], Loss: 1.1587, Acc : 49.22, mIoU 5.90\n","Step [150/366], Loss: 1.6375, Acc : 52.41, mIoU 7.11\n","Step [200/366], Loss: 1.0738, Acc : 55.44, mIoU 8.16\n","Step [250/366], Loss: 1.1098, Acc : 56.85, mIoU 9.12\n","Step [300/366], Loss: 0.9162, Acc : 57.62, mIoU 9.95\n","Step [350/366], Loss: 1.5196, Acc : 58.78, mIoU 11.15\n","Epoch time : 350.7s\n","Validation . . . \n","Step [50/123], Loss: 0.7270, Acc : 68.16, mIoU 19.54\n","Step [100/123], Loss: 0.7557, Acc : 68.74, mIoU 20.18\n","Epoch time : 41.9s\n","Loss 1.1840,  Acc 68.84,  IoU 20.4514\n","EPOCH 2/100\n","Step [50/366], Loss: 0.8137, Acc : 67.55, mIoU 21.22\n","Step [100/366], Loss: 1.1239, Acc : 69.00, mIoU 22.31\n","Step [150/366], Loss: 1.0438, Acc : 69.50, mIoU 22.83\n","Step [200/366], Loss: 1.0045, Acc : 69.66, mIoU 22.79\n","Step [250/366], Loss: 0.9199, Acc : 69.24, mIoU 23.37\n","Step [300/366], Loss: 1.1471, Acc : 69.44, mIoU 24.14\n","Step [350/366], Loss: 1.1570, Acc : 69.64, mIoU 24.34\n","Epoch time : 350.9s\n","Validation . . . \n","Step [50/123], Loss: 0.9691, Acc : 72.02, mIoU 29.36\n","Step [100/123], Loss: 0.8269, Acc : 72.55, mIoU 29.67\n","Epoch time : 42.7s\n","Loss 1.0251,  Acc 72.77,  IoU 29.6160\n","EPOCH 3/100\n","Step [50/366], Loss: 0.7081, Acc : 71.58, mIoU 28.34\n","Step [100/366], Loss: 0.6473, Acc : 72.16, mIoU 28.39\n","Step [150/366], Loss: 0.9705, Acc : 72.21, mIoU 29.09\n","Step [200/366], Loss: 0.6216, Acc : 72.41, mIoU 29.50\n","Step [250/366], Loss: 0.5481, Acc : 72.50, mIoU 30.34\n","Step [300/366], Loss: 0.5733, Acc : 72.88, mIoU 31.09\n","Step [350/366], Loss: 0.6731, Acc : 73.10, mIoU 31.45\n","Epoch time : 351.4s\n","Validation . . . \n","Step [50/123], Loss: 0.7466, Acc : 75.67, mIoU 34.01\n","Step [100/123], Loss: 0.6965, Acc : 74.91, mIoU 33.80\n","Epoch time : 43.0s\n","Loss 0.4662,  Acc 74.81,  IoU 33.5387\n","EPOCH 4/100\n","Step [50/366], Loss: 0.7493, Acc : 74.08, mIoU 34.95\n","Step [100/366], Loss: 0.6580, Acc : 74.60, mIoU 34.89\n","Step [150/366], Loss: 0.5392, Acc : 74.38, mIoU 34.84\n","Step [200/366], Loss: 0.7976, Acc : 74.42, mIoU 34.97\n","Step [250/366], Loss: 1.1684, Acc : 74.29, mIoU 34.86\n","Step [300/366], Loss: 0.8473, Acc : 74.23, mIoU 34.97\n","Step [350/366], Loss: 0.9032, Acc : 74.20, mIoU 34.80\n","Epoch time : 349.3s\n","Validation . . . \n","Step [50/123], Loss: 0.5225, Acc : 76.28, mIoU 37.00\n","Step [100/123], Loss: 0.8397, Acc : 75.30, mIoU 36.32\n","Epoch time : 42.0s\n","Loss 0.8942,  Acc 75.25,  IoU 36.1607\n","EPOCH 5/100\n","Step [50/366], Loss: 0.6593, Acc : 74.39, mIoU 35.77\n","Step [100/366], Loss: 0.6541, Acc : 75.24, mIoU 36.94\n","Step [150/366], Loss: 0.8461, Acc : 75.01, mIoU 37.25\n","Step [200/366], Loss: 0.6888, Acc : 75.15, mIoU 37.44\n","Step [250/366], Loss: 0.7451, Acc : 75.12, mIoU 37.62\n","Step [300/366], Loss: 0.9848, Acc : 75.38, mIoU 37.78\n","Step [350/366], Loss: 0.6750, Acc : 75.43, mIoU 37.66\n","Epoch time : 345.7s\n","Validation . . . \n","Step [50/123], Loss: 0.5528, Acc : 76.03, mIoU 39.99\n","Step [100/123], Loss: 0.5254, Acc : 76.68, mIoU 39.52\n","Epoch time : 42.9s\n","Loss 0.7615,  Acc 76.54,  IoU 39.7914\n","EPOCH 6/100\n","Step [50/366], Loss: 0.6501, Acc : 76.67, mIoU 40.41\n","Step [100/366], Loss: 0.5456, Acc : 76.88, mIoU 41.17\n","Step [150/366], Loss: 0.8526, Acc : 76.82, mIoU 40.95\n","Step [200/366], Loss: 0.7649, Acc : 76.74, mIoU 41.29\n","Step [250/366], Loss: 0.5487, Acc : 76.72, mIoU 41.22\n","Step [300/366], Loss: 0.7100, Acc : 76.60, mIoU 40.98\n","Step [350/366], Loss: 0.5196, Acc : 76.41, mIoU 41.09\n","Epoch time : 349.6s\n","Validation . . . \n","Step [50/123], Loss: 0.9148, Acc : 77.58, mIoU 42.11\n","Step [100/123], Loss: 0.7762, Acc : 77.24, mIoU 41.85\n","Epoch time : 42.4s\n","Loss 0.8655,  Acc 77.25,  IoU 41.5800\n","EPOCH 7/100\n","Step [50/366], Loss: 0.8839, Acc : 76.29, mIoU 41.75\n","Step [100/366], Loss: 0.4937, Acc : 76.55, mIoU 41.42\n","Step [150/366], Loss: 0.4697, Acc : 76.78, mIoU 41.87\n","Step [200/366], Loss: 0.8153, Acc : 77.03, mIoU 42.47\n","Step [250/366], Loss: 0.4359, Acc : 77.16, mIoU 42.20\n","Step [300/366], Loss: 0.6659, Acc : 77.34, mIoU 42.97\n","Step [350/366], Loss: 0.6501, Acc : 77.40, mIoU 43.14\n","Epoch time : 355.7s\n","Validation . . . \n","Step [50/123], Loss: 0.4500, Acc : 78.46, mIoU 44.88\n","Step [100/123], Loss: 0.5908, Acc : 78.64, mIoU 45.20\n","Epoch time : 42.7s\n","Loss 0.4585,  Acc 78.26,  IoU 44.5681\n","EPOCH 8/100\n","Step [50/366], Loss: 0.8017, Acc : 77.30, mIoU 45.63\n","Step [100/366], Loss: 0.6898, Acc : 77.78, mIoU 45.76\n","Step [150/366], Loss: 0.7928, Acc : 78.21, mIoU 45.57\n","Step [200/366], Loss: 0.4644, Acc : 78.20, mIoU 45.69\n","Step [250/366], Loss: 0.5831, Acc : 78.17, mIoU 46.03\n","Step [300/366], Loss: 0.6807, Acc : 78.06, mIoU 45.92\n","Step [350/366], Loss: 0.6663, Acc : 78.08, mIoU 46.14\n","Epoch time : 349.0s\n","Validation . . . \n","Step [50/123], Loss: 0.6214, Acc : 77.44, mIoU 43.03\n","Step [100/123], Loss: 0.3415, Acc : 77.60, mIoU 42.50\n","Epoch time : 42.2s\n","Loss 0.6100,  Acc 77.49,  IoU 42.5243\n","EPOCH 9/100\n","Step [50/366], Loss: 0.5938, Acc : 77.69, mIoU 46.18\n","Step [100/366], Loss: 0.6067, Acc : 78.03, mIoU 45.51\n","Step [150/366], Loss: 0.7460, Acc : 77.83, mIoU 45.89\n","Step [200/366], Loss: 0.4809, Acc : 78.28, mIoU 46.10\n","Step [250/366], Loss: 0.8451, Acc : 78.28, mIoU 46.15\n","Step [300/366], Loss: 0.8271, Acc : 78.32, mIoU 46.38\n","Step [350/366], Loss: 0.5574, Acc : 78.38, mIoU 46.70\n","Epoch time : 349.9s\n","Validation . . . \n","Step [50/123], Loss: 0.6476, Acc : 80.09, mIoU 50.20\n","Step [100/123], Loss: 0.7608, Acc : 78.87, mIoU 48.36\n","Epoch time : 41.3s\n","Loss 0.5000,  Acc 78.58,  IoU 48.6881\n","EPOCH 10/100\n","Step [50/366], Loss: 0.6265, Acc : 78.45, mIoU 48.82\n","Step [100/366], Loss: 0.9535, Acc : 77.75, mIoU 48.37\n","Step [150/366], Loss: 0.4608, Acc : 78.53, mIoU 48.48\n","Step [200/366], Loss: 0.6664, Acc : 78.56, mIoU 48.14\n","Step [250/366], Loss: 0.5980, Acc : 78.41, mIoU 48.14\n","Step [300/366], Loss: 0.9966, Acc : 78.52, mIoU 48.17\n","Step [350/366], Loss: 0.4636, Acc : 78.60, mIoU 48.42\n","Epoch time : 350.1s\n","Validation . . . \n","Step [50/123], Loss: 0.6464, Acc : 78.98, mIoU 48.46\n","Step [100/123], Loss: 0.5462, Acc : 79.31, mIoU 49.21\n","Epoch time : 42.4s\n","Loss 0.5458,  Acc 79.72,  IoU 49.2057\n","EPOCH 11/100\n","Step [50/366], Loss: 0.8500, Acc : 78.40, mIoU 45.63\n","Step [100/366], Loss: 0.6086, Acc : 78.72, mIoU 48.90\n","Step [150/366], Loss: 0.5394, Acc : 78.81, mIoU 49.47\n","Step [200/366], Loss: 0.4839, Acc : 78.94, mIoU 49.08\n","Step [250/366], Loss: 0.4579, Acc : 79.14, mIoU 49.27\n","Step [300/366], Loss: 0.5325, Acc : 79.36, mIoU 49.80\n","Step [350/366], Loss: 0.5556, Acc : 79.22, mIoU 49.92\n","Epoch time : 349.0s\n","Validation . . . \n","Step [50/123], Loss: 0.5732, Acc : 79.60, mIoU 49.58\n","Step [100/123], Loss: 0.5786, Acc : 79.26, mIoU 49.19\n","Epoch time : 42.1s\n","Loss 1.1208,  Acc 79.32,  IoU 49.9185\n","EPOCH 12/100\n","Step [50/366], Loss: 0.5630, Acc : 80.53, mIoU 50.31\n","Step [100/366], Loss: 0.7472, Acc : 79.24, mIoU 49.89\n","Step [150/366], Loss: 0.4814, Acc : 79.56, mIoU 49.64\n","Step [200/366], Loss: 0.5821, Acc : 79.53, mIoU 49.54\n","Step [250/366], Loss: 0.5075, Acc : 79.55, mIoU 50.10\n","Step [300/366], Loss: 0.6136, Acc : 79.51, mIoU 50.81\n","Step [350/366], Loss: 0.5238, Acc : 79.58, mIoU 51.07\n","Epoch time : 350.2s\n","Validation . . . \n","Step [50/123], Loss: 1.1562, Acc : 80.05, mIoU 49.25\n","Step [100/123], Loss: 0.6687, Acc : 79.81, mIoU 49.38\n","Epoch time : 43.0s\n","Loss 0.6746,  Acc 79.76,  IoU 49.2837\n","EPOCH 13/100\n","Step [50/366], Loss: 0.4813, Acc : 80.44, mIoU 50.84\n","Step [100/366], Loss: 0.5718, Acc : 80.08, mIoU 52.81\n","Step [150/366], Loss: 0.5489, Acc : 80.13, mIoU 51.83\n","Step [200/366], Loss: 0.4585, Acc : 80.08, mIoU 51.95\n","Step [250/366], Loss: 0.4790, Acc : 79.96, mIoU 51.90\n","Step [300/366], Loss: 0.4678, Acc : 79.82, mIoU 51.68\n","Step [350/366], Loss: 0.4573, Acc : 79.79, mIoU 51.61\n","Epoch time : 348.8s\n","Validation . . . \n","Step [50/123], Loss: 0.6017, Acc : 79.77, mIoU 51.42\n","Step [100/123], Loss: 0.4252, Acc : 79.15, mIoU 50.42\n","Epoch time : 42.7s\n","Loss 0.6419,  Acc 79.34,  IoU 51.2675\n","EPOCH 14/100\n","Step [50/366], Loss: 0.3930, Acc : 79.88, mIoU 51.83\n","Step [100/366], Loss: 0.5055, Acc : 79.95, mIoU 52.03\n","Step [150/366], Loss: 0.4564, Acc : 80.09, mIoU 52.69\n","Step [200/366], Loss: 0.3993, Acc : 80.04, mIoU 52.92\n","Step [250/366], Loss: 0.5720, Acc : 80.25, mIoU 53.16\n","Step [300/366], Loss: 0.6745, Acc : 80.12, mIoU 52.99\n","Step [350/366], Loss: 0.6134, Acc : 80.08, mIoU 52.77\n","Epoch time : 345.9s\n","Validation . . . \n","Step [50/123], Loss: 0.8887, Acc : 79.03, mIoU 53.12\n","Step [100/123], Loss: 0.7683, Acc : 80.59, mIoU 52.69\n","Epoch time : 41.3s\n","Loss 0.3744,  Acc 80.39,  IoU 52.4837\n","EPOCH 15/100\n","Step [50/366], Loss: 0.7731, Acc : 80.17, mIoU 53.54\n","Step [100/366], Loss: 0.6385, Acc : 79.99, mIoU 53.46\n","Step [150/366], Loss: 0.3491, Acc : 80.54, mIoU 53.08\n","Step [200/366], Loss: 0.4245, Acc : 80.93, mIoU 53.66\n","Step [250/366], Loss: 0.5581, Acc : 80.66, mIoU 53.51\n","Step [300/366], Loss: 0.5986, Acc : 80.59, mIoU 53.76\n","Step [350/366], Loss: 0.4648, Acc : 80.58, mIoU 53.91\n","Epoch time : 350.2s\n","Validation . . . \n","Step [50/123], Loss: 0.6503, Acc : 80.40, mIoU 51.65\n","Step [100/123], Loss: 0.5490, Acc : 80.06, mIoU 52.34\n","Epoch time : 42.8s\n","Loss 0.3843,  Acc 79.95,  IoU 52.5641\n","EPOCH 16/100\n","Step [50/366], Loss: 0.7867, Acc : 80.42, mIoU 56.51\n","Step [100/366], Loss: 0.8110, Acc : 80.54, mIoU 54.57\n","Step [150/366], Loss: 0.3977, Acc : 80.78, mIoU 55.06\n","Step [200/366], Loss: 0.4963, Acc : 81.05, mIoU 55.46\n","Step [250/366], Loss: 0.4523, Acc : 81.01, mIoU 55.22\n","Step [300/366], Loss: 0.6561, Acc : 80.87, mIoU 54.96\n","Step [350/366], Loss: 0.4837, Acc : 80.84, mIoU 54.85\n","Epoch time : 348.6s\n","Validation . . . \n","Step [50/123], Loss: 0.5592, Acc : 77.08, mIoU 49.06\n","Step [100/123], Loss: 0.8872, Acc : 77.91, mIoU 49.97\n","Epoch time : 43.0s\n","Loss 0.4988,  Acc 78.28,  IoU 50.6987\n","EPOCH 17/100\n","Step [50/366], Loss: 0.5927, Acc : 81.22, mIoU 52.92\n","Step [100/366], Loss: 0.5337, Acc : 81.31, mIoU 54.68\n","Step [150/366], Loss: 0.4696, Acc : 81.12, mIoU 55.46\n","Step [200/366], Loss: 0.6786, Acc : 80.85, mIoU 54.99\n","Step [250/366], Loss: 0.4719, Acc : 81.04, mIoU 55.43\n","Step [300/366], Loss: 0.6698, Acc : 80.93, mIoU 55.44\n","Step [350/366], Loss: 0.5299, Acc : 80.82, mIoU 55.37\n","Epoch time : 351.7s\n","Validation . . . \n","Step [50/123], Loss: 0.4033, Acc : 80.91, mIoU 51.68\n","Step [100/123], Loss: 0.5009, Acc : 80.89, mIoU 53.22\n","Epoch time : 42.5s\n","Loss 0.4795,  Acc 80.65,  IoU 52.8793\n","EPOCH 18/100\n","Step [50/366], Loss: 0.4461, Acc : 81.19, mIoU 56.59\n","Step [100/366], Loss: 0.5666, Acc : 81.76, mIoU 57.01\n","Step [150/366], Loss: 0.5278, Acc : 81.34, mIoU 56.43\n","Step [200/366], Loss: 0.4482, Acc : 81.33, mIoU 56.73\n","Step [250/366], Loss: 0.4732, Acc : 81.20, mIoU 56.44\n","Step [300/366], Loss: 0.4923, Acc : 81.07, mIoU 56.14\n","Step [350/366], Loss: 0.4724, Acc : 81.04, mIoU 55.76\n","Epoch time : 350.9s\n","Validation . . . \n","Step [50/123], Loss: 0.7193, Acc : 79.72, mIoU 54.18\n","Step [100/123], Loss: 0.5779, Acc : 80.36, mIoU 54.64\n","Epoch time : 42.7s\n","Loss 0.5141,  Acc 80.51,  IoU 55.0097\n","EPOCH 19/100\n","Step [50/366], Loss: 0.3685, Acc : 80.91, mIoU 56.70\n","Step [100/366], Loss: 0.4190, Acc : 81.43, mIoU 56.11\n","Step [150/366], Loss: 0.4544, Acc : 81.14, mIoU 56.08\n","Step [200/366], Loss: 0.3871, Acc : 81.31, mIoU 56.86\n","Step [250/366], Loss: 0.4716, Acc : 81.38, mIoU 57.22\n","Step [300/366], Loss: 0.6616, Acc : 81.22, mIoU 57.00\n","Step [350/366], Loss: 0.6928, Acc : 81.22, mIoU 56.67\n","Epoch time : 348.2s\n","Validation . . . \n","Step [50/123], Loss: 0.6374, Acc : 80.52, mIoU 53.93\n","Step [100/123], Loss: 0.5752, Acc : 79.64, mIoU 51.89\n","Epoch time : 42.5s\n","Loss 0.3951,  Acc 79.81,  IoU 51.9722\n","EPOCH 20/100\n","Step [50/366], Loss: 0.4733, Acc : 82.11, mIoU 55.12\n","Step [100/366], Loss: 0.6475, Acc : 81.72, mIoU 56.75\n","Step [150/366], Loss: 0.6266, Acc : 81.55, mIoU 57.24\n","Step [200/366], Loss: 0.4877, Acc : 81.41, mIoU 57.29\n","Step [250/366], Loss: 0.4270, Acc : 81.26, mIoU 57.16\n","Step [300/366], Loss: 0.4889, Acc : 81.60, mIoU 57.84\n","Step [350/366], Loss: 0.5088, Acc : 81.50, mIoU 57.65\n","Epoch time : 351.1s\n","Validation . . . \n","Step [50/123], Loss: 0.5298, Acc : 80.00, mIoU 54.28\n","Step [100/123], Loss: 0.4444, Acc : 80.32, mIoU 54.58\n","Epoch time : 42.7s\n","Loss 0.5037,  Acc 80.74,  IoU 54.8664\n","EPOCH 21/100\n","Step [50/366], Loss: 0.5435, Acc : 80.82, mIoU 58.73\n","Step [100/366], Loss: 0.4159, Acc : 81.33, mIoU 58.89\n","Step [150/366], Loss: 0.4900, Acc : 81.48, mIoU 58.89\n","Step [200/366], Loss: 0.4286, Acc : 81.51, mIoU 58.87\n","Step [250/366], Loss: 0.3882, Acc : 81.68, mIoU 58.19\n","Step [300/366], Loss: 0.4252, Acc : 81.71, mIoU 57.98\n","Step [350/366], Loss: 0.5714, Acc : 81.74, mIoU 58.19\n","Epoch time : 347.6s\n","Validation . . . \n","Step [50/123], Loss: 0.7105, Acc : 80.92, mIoU 56.73\n","Step [100/123], Loss: 0.4271, Acc : 80.79, mIoU 55.41\n","Epoch time : 43.0s\n","Loss 0.6039,  Acc 80.37,  IoU 55.4795\n","EPOCH 22/100\n","Step [50/366], Loss: 0.4048, Acc : 81.65, mIoU 59.94\n","Step [100/366], Loss: 0.3608, Acc : 81.82, mIoU 60.22\n","Step [150/366], Loss: 0.4565, Acc : 81.89, mIoU 59.15\n","Step [200/366], Loss: 0.4377, Acc : 81.93, mIoU 59.78\n","Step [250/366], Loss: 0.7012, Acc : 81.88, mIoU 59.42\n","Step [300/366], Loss: 0.4913, Acc : 81.75, mIoU 58.94\n","Step [350/366], Loss: 0.4547, Acc : 81.94, mIoU 59.14\n","Epoch time : 350.9s\n","Validation . . . \n","Step [50/123], Loss: 0.4597, Acc : 81.03, mIoU 55.74\n","Step [100/123], Loss: 0.6400, Acc : 80.98, mIoU 55.83\n","Epoch time : 43.0s\n","Loss 0.5020,  Acc 80.98,  IoU 55.9454\n","EPOCH 23/100\n","Step [50/366], Loss: 0.4247, Acc : 82.76, mIoU 58.39\n","Step [100/366], Loss: 0.4534, Acc : 83.08, mIoU 60.53\n","Step [150/366], Loss: 0.3378, Acc : 82.64, mIoU 60.37\n","Step [200/366], Loss: 0.4664, Acc : 82.46, mIoU 60.11\n","Step [250/366], Loss: 0.4288, Acc : 82.45, mIoU 60.11\n","Step [300/366], Loss: 0.5254, Acc : 82.34, mIoU 59.77\n","Step [350/366], Loss: 0.5385, Acc : 82.03, mIoU 59.33\n","Epoch time : 353.0s\n","Validation . . . \n","Step [50/123], Loss: 0.8395, Acc : 81.92, mIoU 55.87\n","Step [100/123], Loss: 0.6360, Acc : 81.10, mIoU 55.02\n","Epoch time : 42.5s\n","Loss 0.6484,  Acc 80.79,  IoU 55.0533\n","EPOCH 24/100\n","Step [50/366], Loss: 0.4262, Acc : 81.99, mIoU 58.18\n","Step [100/366], Loss: 0.4590, Acc : 82.40, mIoU 60.18\n","Step [150/366], Loss: 0.5933, Acc : 82.25, mIoU 60.17\n","Step [200/366], Loss: 0.4738, Acc : 81.89, mIoU 59.17\n","Step [250/366], Loss: 0.4907, Acc : 82.03, mIoU 59.30\n","Step [300/366], Loss: 0.5506, Acc : 82.19, mIoU 59.52\n","Step [350/366], Loss: 0.3615, Acc : 82.16, mIoU 59.82\n","Epoch time : 351.6s\n","Validation . . . \n","Step [50/123], Loss: 0.4752, Acc : 81.79, mIoU 55.74\n","Step [100/123], Loss: 0.5322, Acc : 81.36, mIoU 56.14\n","Epoch time : 42.3s\n","Loss 0.8628,  Acc 81.19,  IoU 56.1575\n","EPOCH 25/100\n","Step [50/366], Loss: 0.5471, Acc : 82.40, mIoU 60.28\n","Step [100/366], Loss: 0.6362, Acc : 82.16, mIoU 59.28\n","Step [150/366], Loss: 0.5961, Acc : 82.41, mIoU 59.85\n","Step [200/366], Loss: 0.6447, Acc : 82.50, mIoU 60.29\n","Step [250/366], Loss: 0.3770, Acc : 82.31, mIoU 59.81\n","Step [300/366], Loss: 0.4534, Acc : 82.34, mIoU 60.17\n","Step [350/366], Loss: 0.5358, Acc : 82.30, mIoU 60.27\n","Epoch time : 346.2s\n","Validation . . . \n","Step [50/123], Loss: 0.6015, Acc : 79.95, mIoU 54.42\n","Step [100/123], Loss: 0.8268, Acc : 80.27, mIoU 55.56\n","Epoch time : 42.3s\n","Loss 0.4191,  Acc 79.92,  IoU 55.2879\n","EPOCH 26/100\n","Step [50/366], Loss: 0.6134, Acc : 82.47, mIoU 62.05\n","Step [100/366], Loss: 0.5825, Acc : 82.79, mIoU 61.82\n","Step [150/366], Loss: 0.4272, Acc : 82.52, mIoU 60.71\n","Step [200/366], Loss: 0.4088, Acc : 82.70, mIoU 60.98\n","Step [250/366], Loss: 0.4753, Acc : 82.55, mIoU 60.75\n","Step [300/366], Loss: 0.3926, Acc : 82.59, mIoU 60.68\n","Step [350/366], Loss: 0.4802, Acc : 82.68, mIoU 61.02\n","Epoch time : 346.4s\n","Validation . . . \n","Step [50/123], Loss: 0.7339, Acc : 81.45, mIoU 55.37\n","Step [100/123], Loss: 0.7248, Acc : 81.29, mIoU 56.05\n","Epoch time : 41.9s\n","Loss 0.4402,  Acc 81.51,  IoU 56.4165\n","EPOCH 27/100\n","Step [50/366], Loss: 0.3378, Acc : 83.39, mIoU 62.82\n","Step [100/366], Loss: 0.5709, Acc : 83.50, mIoU 63.42\n","Step [150/366], Loss: 0.4011, Acc : 83.21, mIoU 62.50\n","Step [200/366], Loss: 0.6142, Acc : 82.81, mIoU 62.33\n","Step [250/366], Loss: 0.4214, Acc : 82.90, mIoU 62.66\n","Step [300/366], Loss: 0.4325, Acc : 83.07, mIoU 62.82\n","Step [350/366], Loss: 0.6016, Acc : 83.09, mIoU 62.82\n","Epoch time : 348.8s\n","Validation . . . \n","Step [50/123], Loss: 0.4697, Acc : 79.73, mIoU 51.13\n","Step [100/123], Loss: 0.4034, Acc : 80.53, mIoU 53.52\n","Epoch time : 41.9s\n","Loss 0.6707,  Acc 80.41,  IoU 54.4457\n","EPOCH 28/100\n","Step [50/366], Loss: 0.3767, Acc : 83.34, mIoU 62.84\n","Step [100/366], Loss: 0.5818, Acc : 83.12, mIoU 62.34\n","Step [150/366], Loss: 0.4262, Acc : 83.08, mIoU 62.63\n","Step [200/366], Loss: 0.4745, Acc : 82.86, mIoU 62.77\n","Step [250/366], Loss: 0.4635, Acc : 83.05, mIoU 62.78\n","Step [300/366], Loss: 0.5223, Acc : 83.07, mIoU 62.66\n","Step [350/366], Loss: 0.4481, Acc : 82.98, mIoU 62.62\n","Epoch time : 347.2s\n","Validation . . . \n","Step [50/123], Loss: 0.6420, Acc : 80.35, mIoU 54.10\n","Step [100/123], Loss: 0.7163, Acc : 80.22, mIoU 54.29\n","Epoch time : 42.2s\n","Loss 0.6587,  Acc 80.37,  IoU 54.5940\n","EPOCH 29/100\n","Step [50/366], Loss: 0.4127, Acc : 82.86, mIoU 63.89\n","Step [100/366], Loss: 0.5940, Acc : 83.10, mIoU 65.08\n","Step [150/366], Loss: 0.5001, Acc : 83.52, mIoU 64.75\n","Step [200/366], Loss: 0.3425, Acc : 83.48, mIoU 64.36\n","Step [250/366], Loss: 0.3155, Acc : 83.36, mIoU 63.54\n","Step [300/366], Loss: 0.3732, Acc : 83.47, mIoU 63.64\n","Step [350/366], Loss: 0.7332, Acc : 83.17, mIoU 63.21\n","Epoch time : 348.0s\n","Validation . . . \n","Step [50/123], Loss: 0.8386, Acc : 80.14, mIoU 56.69\n","Step [100/123], Loss: 0.5236, Acc : 80.55, mIoU 56.36\n","Epoch time : 43.2s\n","Loss 0.4788,  Acc 80.77,  IoU 56.5023\n","EPOCH 30/100\n","Step [50/366], Loss: 0.4242, Acc : 83.91, mIoU 61.95\n","Step [100/366], Loss: 0.5132, Acc : 83.60, mIoU 63.62\n","Step [150/366], Loss: 0.4010, Acc : 83.46, mIoU 62.87\n","Step [200/366], Loss: 0.3097, Acc : 83.23, mIoU 62.23\n","Step [250/366], Loss: 0.4657, Acc : 83.08, mIoU 62.27\n","Step [300/366], Loss: 0.4084, Acc : 83.03, mIoU 62.31\n","Step [350/366], Loss: 0.4134, Acc : 83.09, mIoU 62.66\n","Epoch time : 346.6s\n","Validation . . . \n","Step [50/123], Loss: 0.6088, Acc : 80.25, mIoU 56.25\n","Step [100/123], Loss: 0.4571, Acc : 80.33, mIoU 56.50\n","Epoch time : 41.4s\n","Loss 0.9752,  Acc 80.31,  IoU 56.0595\n","EPOCH 31/100\n","Step [50/366], Loss: 0.2636, Acc : 83.76, mIoU 64.22\n","Step [100/366], Loss: 0.4737, Acc : 83.77, mIoU 64.50\n","Step [150/366], Loss: 0.5085, Acc : 83.60, mIoU 64.50\n","Step [200/366], Loss: 0.2827, Acc : 83.63, mIoU 64.52\n","Step [250/366], Loss: 0.4596, Acc : 83.66, mIoU 64.48\n","Step [300/366], Loss: 0.3420, Acc : 83.61, mIoU 64.26\n","Step [350/366], Loss: 0.4790, Acc : 83.65, mIoU 64.34\n","Epoch time : 342.5s\n","Validation . . . \n","Step [50/123], Loss: 0.4263, Acc : 81.05, mIoU 55.24\n","Step [100/123], Loss: 0.6859, Acc : 80.77, mIoU 54.98\n","Epoch time : 43.2s\n","Loss 0.5002,  Acc 80.95,  IoU 55.6452\n","EPOCH 32/100\n","Step [50/366], Loss: 0.4914, Acc : 83.97, mIoU 65.49\n","Step [100/366], Loss: 0.4609, Acc : 83.86, mIoU 64.95\n","Step [150/366], Loss: 0.5009, Acc : 83.46, mIoU 64.69\n","Step [200/366], Loss: 0.4752, Acc : 83.47, mIoU 65.10\n","Step [250/366], Loss: 0.3231, Acc : 83.56, mIoU 64.80\n","Step [300/366], Loss: 0.4375, Acc : 83.69, mIoU 64.65\n","Step [350/366], Loss: 0.3243, Acc : 83.72, mIoU 64.85\n","Epoch time : 350.5s\n","Validation . . . \n","Step [50/123], Loss: 0.2134, Acc : 81.30, mIoU 56.82\n","Step [100/123], Loss: 0.6703, Acc : 81.26, mIoU 55.06\n","Epoch time : 43.2s\n","Loss 0.7566,  Acc 81.37,  IoU 55.0752\n","EPOCH 33/100\n","Step [50/366], Loss: 0.3914, Acc : 83.47, mIoU 64.25\n","Step [100/366], Loss: 0.4076, Acc : 83.93, mIoU 65.05\n","Step [150/366], Loss: 0.4625, Acc : 84.01, mIoU 65.29\n","Step [200/366], Loss: 0.4589, Acc : 83.78, mIoU 65.27\n","Step [250/366], Loss: 0.5187, Acc : 83.69, mIoU 65.28\n","Step [300/366], Loss: 0.3781, Acc : 83.91, mIoU 65.37\n","Step [350/366], Loss: 0.6378, Acc : 83.91, mIoU 65.39\n","Epoch time : 346.4s\n","Validation . . . \n","Step [50/123], Loss: 0.5137, Acc : 80.61, mIoU 55.48\n","Step [100/123], Loss: 0.4643, Acc : 81.01, mIoU 56.17\n","Epoch time : 42.3s\n","Loss 0.4396,  Acc 81.15,  IoU 56.4167\n","EPOCH 34/100\n","Step [50/366], Loss: 0.5388, Acc : 85.17, mIoU 68.05\n","Step [100/366], Loss: 0.4402, Acc : 84.72, mIoU 67.43\n","Step [150/366], Loss: 0.3553, Acc : 84.24, mIoU 66.62\n","Step [200/366], Loss: 0.4753, Acc : 84.09, mIoU 66.24\n","Step [250/366], Loss: 0.3295, Acc : 84.06, mIoU 65.76\n","Step [300/366], Loss: 0.6416, Acc : 84.02, mIoU 65.58\n","Step [350/366], Loss: 0.6406, Acc : 83.99, mIoU 65.42\n","Epoch time : 351.2s\n","Validation . . . \n","Step [50/123], Loss: 0.6235, Acc : 81.19, mIoU 56.55\n","Step [100/123], Loss: 0.5953, Acc : 81.63, mIoU 56.57\n","Epoch time : 41.7s\n","Loss 0.3716,  Acc 81.45,  IoU 56.6857\n","EPOCH 35/100\n","Step [50/366], Loss: 0.4989, Acc : 85.55, mIoU 68.16\n","Step [100/366], Loss: 0.4478, Acc : 85.13, mIoU 67.69\n","Step [150/366], Loss: 0.4142, Acc : 84.76, mIoU 67.08\n","Step [200/366], Loss: 0.4705, Acc : 84.67, mIoU 67.32\n","Step [250/366], Loss: 0.3681, Acc : 84.56, mIoU 67.04\n","Step [300/366], Loss: 0.4923, Acc : 84.54, mIoU 66.76\n","Step [350/366], Loss: 0.2999, Acc : 84.46, mIoU 66.78\n","Epoch time : 348.7s\n","Validation . . . \n","Step [50/123], Loss: 0.4866, Acc : 80.95, mIoU 55.54\n","Step [100/123], Loss: 0.5233, Acc : 81.24, mIoU 55.84\n","Epoch time : 43.0s\n","Loss 0.5284,  Acc 81.35,  IoU 55.7959\n","EPOCH 36/100\n","Step [50/366], Loss: 0.3289, Acc : 85.02, mIoU 67.03\n","Step [100/366], Loss: 0.5846, Acc : 84.88, mIoU 67.30\n","Step [150/366], Loss: 0.4221, Acc : 84.78, mIoU 66.76\n","Step [200/366], Loss: 0.5468, Acc : 84.62, mIoU 66.55\n","Step [250/366], Loss: 0.3277, Acc : 84.49, mIoU 66.60\n","Step [300/366], Loss: 0.4050, Acc : 84.62, mIoU 67.17\n","Step [350/366], Loss: 0.4412, Acc : 84.53, mIoU 66.99\n","Epoch time : 346.5s\n","Validation . . . \n","Step [50/123], Loss: 0.6494, Acc : 80.61, mIoU 57.05\n","Step [100/123], Loss: 0.4408, Acc : 81.18, mIoU 56.18\n","Epoch time : 42.1s\n","Loss 0.4973,  Acc 81.04,  IoU 55.7271\n","EPOCH 37/100\n","Step [50/366], Loss: 0.4719, Acc : 84.78, mIoU 69.09\n","Step [100/366], Loss: 0.4107, Acc : 84.53, mIoU 68.68\n","Step [150/366], Loss: 0.2475, Acc : 84.51, mIoU 68.24\n","Step [200/366], Loss: 0.4160, Acc : 84.53, mIoU 68.15\n","Step [250/366], Loss: 0.4281, Acc : 84.77, mIoU 68.72\n","Step [300/366], Loss: 0.3547, Acc : 84.85, mIoU 68.55\n","Step [350/366], Loss: 0.6728, Acc : 84.81, mIoU 68.11\n","Epoch time : 355.4s\n","Validation . . . \n","Step [50/123], Loss: 0.3758, Acc : 80.53, mIoU 55.60\n","Step [100/123], Loss: 0.5763, Acc : 81.22, mIoU 56.79\n","Epoch time : 43.3s\n","Loss 0.3039,  Acc 81.33,  IoU 57.2674\n","EPOCH 38/100\n","Step [50/366], Loss: 0.5058, Acc : 84.93, mIoU 68.61\n","Step [100/366], Loss: 0.4701, Acc : 84.86, mIoU 68.52\n","Step [150/366], Loss: 0.8156, Acc : 84.78, mIoU 67.83\n","Step [200/366], Loss: 0.4829, Acc : 84.82, mIoU 67.82\n","Step [250/366], Loss: 0.4546, Acc : 84.72, mIoU 67.81\n","Step [300/366], Loss: 0.3518, Acc : 84.82, mIoU 67.80\n","Step [350/366], Loss: 0.5406, Acc : 84.67, mIoU 67.83\n","Epoch time : 347.5s\n","Validation . . . \n","Step [50/123], Loss: 0.5408, Acc : 81.81, mIoU 56.49\n","Step [100/123], Loss: 0.5017, Acc : 81.66, mIoU 57.09\n","Epoch time : 42.5s\n","Loss 0.5229,  Acc 81.67,  IoU 56.8796\n","EPOCH 39/100\n","Step [50/366], Loss: 0.4012, Acc : 84.85, mIoU 67.18\n","Step [100/366], Loss: 0.4386, Acc : 85.24, mIoU 69.05\n","Step [150/366], Loss: 0.4339, Acc : 85.32, mIoU 69.01\n","Step [200/366], Loss: 0.3947, Acc : 85.30, mIoU 68.69\n","Step [250/366], Loss: 0.3340, Acc : 85.09, mIoU 68.43\n","Step [300/366], Loss: 0.3969, Acc : 84.93, mIoU 68.34\n","Step [350/366], Loss: 0.3670, Acc : 84.79, mIoU 68.28\n","Epoch time : 354.1s\n","Validation . . . \n","Step [50/123], Loss: 0.5551, Acc : 81.12, mIoU 57.08\n","Step [100/123], Loss: 0.4834, Acc : 81.45, mIoU 56.74\n","Epoch time : 43.8s\n","Loss 0.5650,  Acc 81.22,  IoU 56.3077\n","EPOCH 40/100\n","Step [50/366], Loss: 0.4743, Acc : 84.33, mIoU 66.69\n","Step [100/366], Loss: 0.3553, Acc : 85.17, mIoU 69.13\n","Step [150/366], Loss: 0.5673, Acc : 85.02, mIoU 69.11\n","Step [200/366], Loss: 0.4630, Acc : 84.99, mIoU 68.99\n","Step [250/366], Loss: 0.5414, Acc : 84.87, mIoU 68.73\n","Step [300/366], Loss: 0.5104, Acc : 84.97, mIoU 68.70\n","Step [350/366], Loss: 0.3404, Acc : 85.03, mIoU 68.88\n","Epoch time : 350.1s\n","Validation . . . \n","Step [50/123], Loss: 0.3992, Acc : 80.94, mIoU 56.20\n","Step [100/123], Loss: 0.3732, Acc : 81.33, mIoU 56.61\n","Epoch time : 43.3s\n","Loss 0.7147,  Acc 81.36,  IoU 56.7355\n","EPOCH 41/100\n","Step [50/366], Loss: 0.3670, Acc : 85.79, mIoU 68.46\n","Step [100/366], Loss: 0.3738, Acc : 85.55, mIoU 68.44\n","Step [150/366], Loss: 0.3974, Acc : 85.60, mIoU 68.78\n","Step [200/366], Loss: 0.3169, Acc : 85.38, mIoU 69.16\n","Step [250/366], Loss: 0.5107, Acc : 85.08, mIoU 68.72\n","Step [300/366], Loss: 0.3764, Acc : 85.03, mIoU 68.84\n","Step [350/366], Loss: 0.5436, Acc : 85.04, mIoU 69.02\n","Epoch time : 349.3s\n","Validation . . . \n","Step [50/123], Loss: 0.5176, Acc : 80.82, mIoU 54.48\n","Step [100/123], Loss: 0.4104, Acc : 81.78, mIoU 57.55\n","Epoch time : 43.1s\n","Loss 0.8139,  Acc 81.65,  IoU 57.6925\n","EPOCH 42/100\n","Step [50/366], Loss: 0.4999, Acc : 85.88, mIoU 69.45\n","Step [100/366], Loss: 0.3775, Acc : 85.61, mIoU 69.92\n","Step [150/366], Loss: 0.3251, Acc : 85.50, mIoU 69.80\n","Step [200/366], Loss: 0.3141, Acc : 85.52, mIoU 69.95\n","Step [250/366], Loss: 0.3513, Acc : 85.69, mIoU 70.27\n","Step [300/366], Loss: 0.5156, Acc : 85.70, mIoU 70.02\n","Step [350/366], Loss: 0.3734, Acc : 85.69, mIoU 70.01\n","Epoch time : 348.3s\n","Validation . . . \n","Step [50/123], Loss: 0.3753, Acc : 81.53, mIoU 55.75\n","Step [100/123], Loss: 0.4348, Acc : 81.33, mIoU 55.51\n","Epoch time : 42.9s\n","Loss 0.6229,  Acc 81.15,  IoU 55.7914\n","EPOCH 43/100\n","Step [50/366], Loss: 0.3420, Acc : 86.51, mIoU 71.21\n","Step [100/366], Loss: 0.3289, Acc : 86.06, mIoU 71.40\n","Step [150/366], Loss: 0.2578, Acc : 85.97, mIoU 71.51\n","Step [200/366], Loss: 0.3662, Acc : 86.06, mIoU 71.70\n","Step [250/366], Loss: 0.5432, Acc : 85.95, mIoU 71.60\n","Step [300/366], Loss: 0.3671, Acc : 85.81, mIoU 71.11\n","Step [350/366], Loss: 0.3900, Acc : 85.80, mIoU 70.94\n","Epoch time : 348.5s\n","Validation . . . \n","Step [50/123], Loss: 0.5657, Acc : 82.16, mIoU 56.22\n","Step [100/123], Loss: 0.5438, Acc : 82.00, mIoU 57.16\n","Epoch time : 43.0s\n","Loss 0.4714,  Acc 81.49,  IoU 57.0913\n","EPOCH 44/100\n","Step [50/366], Loss: 0.3050, Acc : 86.71, mIoU 71.01\n","Step [100/366], Loss: 0.4388, Acc : 86.43, mIoU 71.79\n","Step [150/366], Loss: 0.3460, Acc : 86.06, mIoU 72.12\n","Step [200/366], Loss: 0.3414, Acc : 86.04, mIoU 71.81\n","Step [250/366], Loss: 0.3354, Acc : 85.91, mIoU 71.78\n","Step [300/366], Loss: 0.3538, Acc : 85.68, mIoU 71.31\n","Step [350/366], Loss: 0.4087, Acc : 85.81, mIoU 71.12\n","Epoch time : 343.6s\n","Validation . . . \n","Step [50/123], Loss: 0.8767, Acc : 81.05, mIoU 56.45\n","Step [100/123], Loss: 0.5162, Acc : 81.62, mIoU 57.17\n","Epoch time : 43.0s\n","Loss 0.8312,  Acc 81.16,  IoU 56.8192\n","EPOCH 45/100\n","Step [50/366], Loss: 0.4552, Acc : 85.79, mIoU 69.09\n","Step [100/366], Loss: 0.2627, Acc : 86.08, mIoU 71.22\n","Step [150/366], Loss: 0.8167, Acc : 86.16, mIoU 71.03\n","Step [200/366], Loss: 0.4339, Acc : 86.00, mIoU 70.94\n","Step [250/366], Loss: 0.3978, Acc : 86.01, mIoU 71.14\n","Step [300/366], Loss: 0.3582, Acc : 86.07, mIoU 71.28\n","Step [350/366], Loss: 0.3224, Acc : 85.86, mIoU 70.99\n","Epoch time : 352.8s\n","Validation . . . \n","Step [50/123], Loss: 0.5991, Acc : 81.73, mIoU 57.67\n","Step [100/123], Loss: 0.3327, Acc : 81.58, mIoU 57.76\n","Epoch time : 42.7s\n","Loss 0.5198,  Acc 81.57,  IoU 57.4330\n","EPOCH 46/100\n","Step [50/366], Loss: 0.2965, Acc : 86.40, mIoU 72.39\n","Step [100/366], Loss: 0.3230, Acc : 86.19, mIoU 72.42\n","Step [150/366], Loss: 0.2682, Acc : 86.00, mIoU 72.03\n","Step [200/366], Loss: 0.2809, Acc : 86.09, mIoU 71.84\n","Step [250/366], Loss: 0.4174, Acc : 86.08, mIoU 71.75\n","Step [300/366], Loss: 0.4104, Acc : 86.12, mIoU 71.67\n","Step [350/366], Loss: 0.4090, Acc : 86.13, mIoU 71.68\n","Epoch time : 346.5s\n","Validation . . . \n","Step [50/123], Loss: 0.5997, Acc : 81.66, mIoU 55.63\n","Step [100/123], Loss: 0.6065, Acc : 80.99, mIoU 56.45\n","Epoch time : 43.8s\n","Loss 0.4975,  Acc 81.24,  IoU 57.1510\n","EPOCH 47/100\n","Step [50/366], Loss: 0.3110, Acc : 86.04, mIoU 73.73\n","Step [100/366], Loss: 0.3560, Acc : 86.27, mIoU 73.77\n","Step [150/366], Loss: 0.4385, Acc : 86.52, mIoU 73.79\n","Step [200/366], Loss: 0.3793, Acc : 86.53, mIoU 73.58\n","Step [250/366], Loss: 0.2885, Acc : 86.52, mIoU 72.95\n","Step [300/366], Loss: 0.4261, Acc : 86.54, mIoU 72.88\n","Step [350/366], Loss: 0.4053, Acc : 86.39, mIoU 72.85\n","Epoch time : 345.1s\n","Validation . . . \n","Step [50/123], Loss: 0.4165, Acc : 81.91, mIoU 57.83\n","Step [100/123], Loss: 1.0070, Acc : 81.89, mIoU 57.92\n","Epoch time : 42.0s\n","Loss 0.4667,  Acc 81.76,  IoU 57.8129\n","EPOCH 48/100\n","Step [50/366], Loss: 0.3480, Acc : 87.06, mIoU 71.62\n","Step [100/366], Loss: 0.4886, Acc : 86.60, mIoU 73.56\n","Step [150/366], Loss: 0.3953, Acc : 86.82, mIoU 74.00\n","Step [200/366], Loss: 0.3737, Acc : 86.86, mIoU 73.66\n","Step [250/366], Loss: 0.4323, Acc : 86.61, mIoU 73.04\n","Step [300/366], Loss: 0.3898, Acc : 86.55, mIoU 73.28\n","Step [350/366], Loss: 0.3682, Acc : 86.54, mIoU 73.17\n","Epoch time : 350.5s\n","Validation . . . \n","Step [50/123], Loss: 1.1911, Acc : 81.09, mIoU 54.51\n","Step [100/123], Loss: 0.6227, Acc : 81.14, mIoU 56.73\n","Epoch time : 43.1s\n","Loss 0.7352,  Acc 81.30,  IoU 57.2878\n","EPOCH 49/100\n","Step [50/366], Loss: 0.2565, Acc : 87.04, mIoU 74.59\n","Step [100/366], Loss: 0.2513, Acc : 87.13, mIoU 74.26\n","Step [150/366], Loss: 0.3069, Acc : 87.19, mIoU 74.45\n","Step [200/366], Loss: 0.3020, Acc : 86.95, mIoU 73.97\n","Step [250/366], Loss: 0.2498, Acc : 86.93, mIoU 73.72\n","Step [300/366], Loss: 0.3036, Acc : 86.80, mIoU 73.53\n","Step [350/366], Loss: 0.3850, Acc : 86.66, mIoU 73.37\n","Epoch time : 347.8s\n","Validation . . . \n","Step [50/123], Loss: 0.5886, Acc : 81.49, mIoU 55.50\n","Step [100/123], Loss: 0.6362, Acc : 81.32, mIoU 55.93\n","Epoch time : 43.3s\n","Loss 0.4612,  Acc 81.30,  IoU 56.3627\n","EPOCH 50/100\n","Step [50/366], Loss: 0.2242, Acc : 85.93, mIoU 72.51\n","Step [100/366], Loss: 0.2617, Acc : 85.94, mIoU 72.34\n","Step [150/366], Loss: 0.2173, Acc : 86.14, mIoU 72.73\n","Step [200/366], Loss: 0.5209, Acc : 86.46, mIoU 73.04\n","Step [250/366], Loss: 0.3552, Acc : 86.51, mIoU 73.11\n","Step [300/366], Loss: 0.5066, Acc : 86.42, mIoU 73.07\n","Step [350/366], Loss: 0.5032, Acc : 86.47, mIoU 73.12\n","Epoch time : 349.5s\n","Validation . . . \n","Step [50/123], Loss: 0.3465, Acc : 81.90, mIoU 57.27\n","Step [100/123], Loss: 0.5541, Acc : 81.47, mIoU 56.64\n","Epoch time : 43.3s\n","Loss 0.7446,  Acc 81.43,  IoU 56.5208\n","EPOCH 51/100\n","Step [50/366], Loss: 0.3203, Acc : 86.71, mIoU 72.57\n","Step [100/366], Loss: 0.4499, Acc : 86.72, mIoU 73.07\n","Step [150/366], Loss: 0.4053, Acc : 86.86, mIoU 73.87\n","Step [200/366], Loss: 0.4119, Acc : 86.85, mIoU 73.90\n","Step [250/366], Loss: 0.3053, Acc : 86.76, mIoU 73.90\n","Step [300/366], Loss: 0.3717, Acc : 86.78, mIoU 73.88\n","Step [350/366], Loss: 0.2720, Acc : 86.85, mIoU 73.95\n","Epoch time : 346.8s\n","Validation . . . \n","Step [50/123], Loss: 0.6964, Acc : 80.79, mIoU 56.24\n","Step [100/123], Loss: 0.6091, Acc : 81.12, mIoU 57.63\n","Epoch time : 43.1s\n","Loss 0.3856,  Acc 80.93,  IoU 57.4714\n","EPOCH 52/100\n","Step [50/366], Loss: 0.2676, Acc : 87.00, mIoU 74.13\n","Step [100/366], Loss: 0.3579, Acc : 87.24, mIoU 74.45\n","Step [150/366], Loss: 0.3078, Acc : 87.17, mIoU 74.61\n","Step [200/366], Loss: 0.4268, Acc : 87.16, mIoU 75.02\n","Step [250/366], Loss: 0.3138, Acc : 87.13, mIoU 74.96\n","Step [300/366], Loss: 0.4468, Acc : 87.12, mIoU 74.64\n","Step [350/366], Loss: 0.4557, Acc : 87.08, mIoU 74.41\n","Epoch time : 348.8s\n","Validation . . . \n","Step [50/123], Loss: 0.7406, Acc : 81.38, mIoU 57.23\n","Step [100/123], Loss: 0.4815, Acc : 80.80, mIoU 55.71\n","Epoch time : 42.3s\n","Loss 0.6223,  Acc 80.48,  IoU 55.5208\n","EPOCH 53/100\n","Step [50/366], Loss: 0.4362, Acc : 86.22, mIoU 73.56\n","Step [100/366], Loss: 0.4003, Acc : 86.85, mIoU 74.34\n","Step [150/366], Loss: 0.3894, Acc : 87.00, mIoU 74.64\n","Step [200/366], Loss: 0.4484, Acc : 86.92, mIoU 74.34\n","Step [250/366], Loss: 0.4760, Acc : 86.90, mIoU 74.06\n","Step [300/366], Loss: 0.4234, Acc : 86.98, mIoU 74.18\n","Step [350/366], Loss: 0.3521, Acc : 86.92, mIoU 73.87\n","Epoch time : 343.7s\n","Validation . . . \n","Step [50/123], Loss: 0.6976, Acc : 81.39, mIoU 58.69\n","Step [100/123], Loss: 1.2044, Acc : 81.45, mIoU 57.73\n","Epoch time : 42.6s\n","Loss 0.6357,  Acc 81.37,  IoU 57.5031\n","EPOCH 54/100\n","Step [50/366], Loss: 0.2855, Acc : 86.98, mIoU 76.62\n","Step [100/366], Loss: 0.3510, Acc : 87.46, mIoU 75.96\n","Step [150/366], Loss: 0.3759, Acc : 87.20, mIoU 75.25\n","Step [200/366], Loss: 0.2632, Acc : 87.27, mIoU 74.72\n","Step [250/366], Loss: 0.2731, Acc : 87.15, mIoU 74.78\n","Step [300/366], Loss: 0.3087, Acc : 87.19, mIoU 74.80\n","Step [350/366], Loss: 0.4108, Acc : 87.31, mIoU 74.93\n","Epoch time : 348.5s\n","Validation . . . \n","Step [50/123], Loss: 0.7362, Acc : 81.30, mIoU 59.30\n","Step [100/123], Loss: 0.4793, Acc : 81.54, mIoU 58.46\n","Epoch time : 42.5s\n","Loss 0.5482,  Acc 81.60,  IoU 58.4764\n","EPOCH 55/100\n","Step [50/366], Loss: 0.2064, Acc : 88.40, mIoU 77.48\n","Step [100/366], Loss: 0.2718, Acc : 87.93, mIoU 76.05\n","Step [150/366], Loss: 0.3927, Acc : 87.62, mIoU 75.72\n","Step [200/366], Loss: 0.5626, Acc : 87.46, mIoU 75.29\n","Step [250/366], Loss: 0.2477, Acc : 87.47, mIoU 75.58\n","Step [300/366], Loss: 0.2974, Acc : 87.44, mIoU 75.64\n","Step [350/366], Loss: 0.2596, Acc : 87.43, mIoU 75.67\n","Epoch time : 345.3s\n","Validation . . . \n","Step [50/123], Loss: 0.3450, Acc : 81.17, mIoU 55.78\n","Step [100/123], Loss: 0.7987, Acc : 81.47, mIoU 56.09\n","Epoch time : 42.2s\n","Loss 0.6580,  Acc 81.49,  IoU 56.2882\n","EPOCH 56/100\n","Step [50/366], Loss: 0.3066, Acc : 87.71, mIoU 77.57\n","Step [100/366], Loss: 0.2987, Acc : 87.29, mIoU 76.30\n","Step [150/366], Loss: 0.2224, Acc : 87.50, mIoU 75.99\n","Step [200/366], Loss: 0.3240, Acc : 87.65, mIoU 76.03\n","Step [250/366], Loss: 0.3346, Acc : 87.75, mIoU 75.95\n","Step [300/366], Loss: 0.2488, Acc : 87.59, mIoU 75.73\n","Step [350/366], Loss: 0.4269, Acc : 87.55, mIoU 75.57\n","Epoch time : 346.4s\n","Validation . . . \n","Step [50/123], Loss: 0.4730, Acc : 80.24, mIoU 57.53\n","Step [100/123], Loss: 0.5122, Acc : 80.51, mIoU 57.32\n","Epoch time : 42.5s\n","Loss 0.5739,  Acc 80.32,  IoU 57.8370\n","EPOCH 57/100\n","Step [50/366], Loss: 0.3512, Acc : 88.01, mIoU 75.47\n","Step [100/366], Loss: 0.3462, Acc : 87.75, mIoU 75.70\n","Step [150/366], Loss: 0.2987, Acc : 87.71, mIoU 76.02\n","Step [200/366], Loss: 0.2515, Acc : 87.59, mIoU 75.68\n","Step [250/366], Loss: 0.3062, Acc : 87.78, mIoU 76.34\n","Step [300/366], Loss: 0.3744, Acc : 87.73, mIoU 76.03\n","Step [350/366], Loss: 0.4081, Acc : 87.60, mIoU 75.81\n","Epoch time : 346.9s\n","Validation . . . \n","Step [50/123], Loss: 0.4368, Acc : 80.21, mIoU 54.66\n","Step [100/123], Loss: 0.6527, Acc : 80.74, mIoU 55.36\n","Epoch time : 41.7s\n","Loss 0.5357,  Acc 81.07,  IoU 56.6241\n","EPOCH 58/100\n","Step [50/366], Loss: 0.3330, Acc : 87.81, mIoU 76.98\n","Step [100/366], Loss: 0.2734, Acc : 87.88, mIoU 77.00\n","Step [150/366], Loss: 0.4772, Acc : 87.74, mIoU 76.56\n","Step [200/366], Loss: 0.3050, Acc : 87.62, mIoU 76.52\n","Step [250/366], Loss: 0.3392, Acc : 87.56, mIoU 76.37\n","Step [300/366], Loss: 0.2473, Acc : 87.55, mIoU 76.28\n","Step [350/366], Loss: 0.3076, Acc : 87.53, mIoU 76.00\n","Epoch time : 348.0s\n","Validation . . . \n","Step [50/123], Loss: 0.7735, Acc : 80.87, mIoU 57.86\n","Step [100/123], Loss: 0.5335, Acc : 81.00, mIoU 57.77\n","Epoch time : 43.3s\n","Loss 0.4098,  Acc 81.05,  IoU 57.6147\n","EPOCH 59/100\n","Step [50/366], Loss: 0.2120, Acc : 88.40, mIoU 78.12\n","Step [100/366], Loss: 0.3531, Acc : 88.18, mIoU 78.05\n","Step [150/366], Loss: 0.2521, Acc : 88.29, mIoU 77.94\n","Step [200/366], Loss: 0.3886, Acc : 88.22, mIoU 77.57\n","Step [250/366], Loss: 0.2471, Acc : 88.09, mIoU 77.58\n","Step [300/366], Loss: 0.2226, Acc : 88.08, mIoU 77.17\n","Step [350/366], Loss: 0.2740, Acc : 88.05, mIoU 77.03\n","Epoch time : 345.2s\n","Validation . . . \n","Step [50/123], Loss: 0.6630, Acc : 80.87, mIoU 55.74\n","Step [100/123], Loss: 0.6403, Acc : 80.43, mIoU 55.45\n","Epoch time : 41.2s\n","Loss 0.3219,  Acc 80.66,  IoU 56.2322\n","EPOCH 60/100\n","Step [50/366], Loss: 0.2754, Acc : 88.30, mIoU 77.58\n","Step [100/366], Loss: 0.2577, Acc : 88.30, mIoU 77.72\n","Step [150/366], Loss: 0.2286, Acc : 88.44, mIoU 77.95\n","Step [200/366], Loss: 0.3268, Acc : 88.33, mIoU 77.55\n","Step [250/366], Loss: 0.1774, Acc : 88.45, mIoU 77.44\n","Step [300/366], Loss: 0.4198, Acc : 88.34, mIoU 77.38\n","Step [350/366], Loss: 0.3915, Acc : 88.19, mIoU 77.28\n","Epoch time : 350.7s\n","Validation . . . \n","Step [50/123], Loss: 0.2306, Acc : 81.65, mIoU 56.29\n","Step [100/123], Loss: 0.4071, Acc : 80.98, mIoU 57.26\n","Epoch time : 43.0s\n","Loss 0.5420,  Acc 81.30,  IoU 57.3529\n","EPOCH 61/100\n","Step [50/366], Loss: 0.2073, Acc : 87.69, mIoU 76.09\n","Step [100/366], Loss: 0.3545, Acc : 88.12, mIoU 77.04\n","Step [150/366], Loss: 0.3356, Acc : 88.11, mIoU 77.61\n","Step [200/366], Loss: 0.3875, Acc : 88.21, mIoU 77.69\n","Step [250/366], Loss: 0.3340, Acc : 88.39, mIoU 77.72\n","Step [300/366], Loss: 0.2455, Acc : 88.31, mIoU 77.78\n","Step [350/366], Loss: 0.3072, Acc : 88.25, mIoU 77.55\n","Epoch time : 346.7s\n","Validation . . . \n","Step [50/123], Loss: 0.5447, Acc : 81.51, mIoU 56.78\n","Step [100/123], Loss: 0.4867, Acc : 80.84, mIoU 58.10\n","Epoch time : 42.9s\n","Loss 0.3772,  Acc 81.10,  IoU 58.1663\n","EPOCH 62/100\n","Step [50/366], Loss: 0.3541, Acc : 88.31, mIoU 78.43\n","Step [100/366], Loss: 0.3034, Acc : 88.59, mIoU 78.42\n","Step [150/366], Loss: 0.2525, Acc : 88.30, mIoU 77.77\n","Step [200/366], Loss: 0.3404, Acc : 88.40, mIoU 77.86\n","Step [250/366], Loss: 0.2875, Acc : 88.31, mIoU 77.71\n","Step [300/366], Loss: 0.2477, Acc : 88.25, mIoU 77.51\n","Step [350/366], Loss: 0.2869, Acc : 88.27, mIoU 77.49\n","Epoch time : 350.0s\n","Validation . . . \n","Step [50/123], Loss: 0.5132, Acc : 80.52, mIoU 58.11\n","Step [100/123], Loss: 0.5280, Acc : 80.84, mIoU 58.15\n","Epoch time : 41.9s\n","Loss 0.5136,  Acc 81.27,  IoU 58.2309\n","EPOCH 63/100\n","Step [50/366], Loss: 0.2536, Acc : 89.10, mIoU 79.34\n","Step [100/366], Loss: 0.2784, Acc : 88.64, mIoU 77.99\n","Step [150/366], Loss: 0.2110, Acc : 88.60, mIoU 77.74\n","Step [200/366], Loss: 0.2208, Acc : 88.53, mIoU 77.88\n","Step [250/366], Loss: 0.3358, Acc : 88.50, mIoU 77.92\n","Step [300/366], Loss: 0.2293, Acc : 88.45, mIoU 77.83\n","Step [350/366], Loss: 0.2815, Acc : 88.51, mIoU 78.02\n","Epoch time : 348.0s\n","Validation . . . \n","Step [50/123], Loss: 0.7297, Acc : 81.82, mIoU 58.53\n","Step [100/123], Loss: 0.4770, Acc : 81.50, mIoU 58.53\n","Epoch time : 42.2s\n","Loss 0.4799,  Acc 81.34,  IoU 58.4302\n","EPOCH 64/100\n","Step [50/366], Loss: 0.3485, Acc : 88.80, mIoU 79.01\n","Step [100/366], Loss: 0.2514, Acc : 88.49, mIoU 78.28\n","Step [150/366], Loss: 0.2557, Acc : 88.66, mIoU 78.92\n","Step [200/366], Loss: 0.4344, Acc : 88.55, mIoU 78.22\n","Step [250/366], Loss: 0.3972, Acc : 88.51, mIoU 77.84\n","Step [300/366], Loss: 0.3182, Acc : 88.46, mIoU 77.58\n","Step [350/366], Loss: 0.2502, Acc : 88.44, mIoU 77.49\n","Epoch time : 346.6s\n","Validation . . . \n","Step [50/123], Loss: 0.8046, Acc : 81.18, mIoU 57.05\n","Step [100/123], Loss: 0.4439, Acc : 81.23, mIoU 56.52\n","Epoch time : 42.6s\n","Loss 0.5081,  Acc 81.14,  IoU 56.2162\n","EPOCH 65/100\n","Step [50/366], Loss: 0.3603, Acc : 87.88, mIoU 74.90\n","Step [100/366], Loss: 0.2167, Acc : 88.16, mIoU 75.86\n","Step [150/366], Loss: 0.2272, Acc : 88.51, mIoU 76.46\n","Step [200/366], Loss: 0.2131, Acc : 88.48, mIoU 76.96\n","Step [250/366], Loss: 0.3569, Acc : 88.48, mIoU 77.18\n","Step [300/366], Loss: 0.2123, Acc : 88.46, mIoU 77.19\n","Step [350/366], Loss: 0.2982, Acc : 88.44, mIoU 77.10\n","Epoch time : 347.9s\n","Validation . . . \n","Step [50/123], Loss: 0.7908, Acc : 81.02, mIoU 55.14\n","Step [100/123], Loss: 0.4776, Acc : 81.15, mIoU 56.69\n","Epoch time : 43.2s\n","Loss 0.8257,  Acc 81.12,  IoU 56.7781\n","EPOCH 66/100\n","Step [50/366], Loss: 0.3119, Acc : 89.02, mIoU 80.03\n","Step [100/366], Loss: 0.2159, Acc : 89.27, mIoU 79.80\n","Step [150/366], Loss: 0.2785, Acc : 88.97, mIoU 79.13\n","Step [200/366], Loss: 0.2832, Acc : 88.93, mIoU 79.01\n","Step [250/366], Loss: 0.2739, Acc : 88.92, mIoU 79.19\n","Step [300/366], Loss: 0.3731, Acc : 88.85, mIoU 79.15\n","Step [350/366], Loss: 0.2535, Acc : 88.88, mIoU 79.14\n","Epoch time : 344.8s\n","Validation . . . \n","Step [50/123], Loss: 0.7631, Acc : 80.71, mIoU 56.62\n","Step [100/123], Loss: 0.6231, Acc : 81.30, mIoU 58.49\n","Epoch time : 42.4s\n","Loss 0.6418,  Acc 81.38,  IoU 58.5599\n","EPOCH 67/100\n","Step [50/366], Loss: 0.2987, Acc : 89.17, mIoU 79.94\n","Step [100/366], Loss: 0.2685, Acc : 89.02, mIoU 79.60\n","Step [150/366], Loss: 0.2865, Acc : 89.24, mIoU 79.66\n","Step [200/366], Loss: 0.3096, Acc : 89.28, mIoU 80.07\n","Step [250/366], Loss: 0.3038, Acc : 89.22, mIoU 79.86\n","Step [300/366], Loss: 0.3823, Acc : 89.13, mIoU 79.52\n","Step [350/366], Loss: 0.2201, Acc : 88.99, mIoU 79.06\n","Epoch time : 349.9s\n","Validation . . . \n","Step [50/123], Loss: 1.0384, Acc : 81.79, mIoU 59.85\n","Step [100/123], Loss: 0.4716, Acc : 81.53, mIoU 58.81\n","Epoch time : 41.4s\n","Loss 0.7498,  Acc 81.44,  IoU 58.3393\n","EPOCH 68/100\n","Step [50/366], Loss: 0.3213, Acc : 89.28, mIoU 79.95\n","Step [100/366], Loss: 0.1635, Acc : 89.12, mIoU 79.68\n","Step [150/366], Loss: 0.3695, Acc : 89.04, mIoU 79.61\n","Step [200/366], Loss: 0.2443, Acc : 89.01, mIoU 79.62\n","Step [250/366], Loss: 0.4264, Acc : 88.95, mIoU 79.43\n","Step [300/366], Loss: 0.3634, Acc : 88.95, mIoU 79.42\n","Step [350/366], Loss: 0.3342, Acc : 88.86, mIoU 79.17\n","Epoch time : 344.4s\n","Validation . . . \n","Step [50/123], Loss: 0.8397, Acc : 79.29, mIoU 53.92\n","Step [100/123], Loss: 0.5661, Acc : 79.32, mIoU 54.10\n","Epoch time : 42.9s\n","Loss 0.4291,  Acc 79.50,  IoU 54.6242\n","EPOCH 69/100\n","Step [50/366], Loss: 0.2315, Acc : 89.52, mIoU 81.06\n","Step [100/366], Loss: 0.3868, Acc : 89.41, mIoU 80.84\n","Step [150/366], Loss: 0.2333, Acc : 89.41, mIoU 80.27\n","Step [200/366], Loss: 0.3163, Acc : 89.10, mIoU 79.77\n","Step [250/366], Loss: 0.2927, Acc : 88.95, mIoU 79.21\n","Step [300/366], Loss: 0.3392, Acc : 88.94, mIoU 79.01\n","Step [350/366], Loss: 0.3082, Acc : 89.10, mIoU 79.13\n","Epoch time : 349.8s\n","Validation . . . \n","Step [50/123], Loss: 0.4588, Acc : 81.18, mIoU 56.44\n","Step [100/123], Loss: 0.5659, Acc : 80.73, mIoU 56.64\n","Epoch time : 42.5s\n","Loss 0.7138,  Acc 80.74,  IoU 56.8811\n","EPOCH 70/100\n","Step [50/366], Loss: 0.2861, Acc : 89.66, mIoU 78.65\n","Step [100/366], Loss: 0.2654, Acc : 89.59, mIoU 79.12\n","Step [150/366], Loss: 0.3163, Acc : 89.54, mIoU 79.50\n","Step [200/366], Loss: 0.2897, Acc : 89.24, mIoU 79.10\n","Step [250/366], Loss: 0.1934, Acc : 89.34, mIoU 79.75\n","Step [300/366], Loss: 0.2980, Acc : 89.35, mIoU 79.56\n","Step [350/366], Loss: 0.3739, Acc : 89.20, mIoU 79.51\n","Epoch time : 350.2s\n","Validation . . . \n","Step [50/123], Loss: 0.4456, Acc : 81.85, mIoU 58.02\n","Step [100/123], Loss: 0.9590, Acc : 81.35, mIoU 58.16\n","Epoch time : 42.3s\n","Loss 0.4578,  Acc 81.12,  IoU 58.0357\n","EPOCH 71/100\n","Step [50/366], Loss: 0.2487, Acc : 89.42, mIoU 79.94\n","Step [100/366], Loss: 0.2080, Acc : 89.55, mIoU 80.74\n","Step [150/366], Loss: 0.3870, Acc : 89.28, mIoU 80.51\n","Step [200/366], Loss: 0.4537, Acc : 88.97, mIoU 79.89\n","Step [250/366], Loss: 0.2686, Acc : 89.02, mIoU 79.62\n","Step [300/366], Loss: 0.2142, Acc : 89.14, mIoU 79.68\n","Step [350/366], Loss: 0.2644, Acc : 89.11, mIoU 79.70\n","Epoch time : 342.2s\n","Validation . . . \n","Step [50/123], Loss: 1.0300, Acc : 80.47, mIoU 56.26\n","Step [100/123], Loss: 0.7111, Acc : 81.15, mIoU 57.30\n","Epoch time : 43.0s\n","Loss 0.5994,  Acc 81.15,  IoU 57.1003\n","EPOCH 72/100\n","Step [50/366], Loss: 0.2435, Acc : 89.67, mIoU 82.35\n","Step [100/366], Loss: 0.2303, Acc : 89.63, mIoU 81.87\n","Step [150/366], Loss: 0.3197, Acc : 89.27, mIoU 80.56\n","Step [200/366], Loss: 0.2580, Acc : 89.28, mIoU 80.23\n","Step [250/366], Loss: 0.2717, Acc : 89.32, mIoU 80.33\n","Step [300/366], Loss: 0.3772, Acc : 89.28, mIoU 80.29\n","Step [350/366], Loss: 0.2735, Acc : 89.36, mIoU 80.26\n","Epoch time : 346.7s\n","Validation . . . \n","Step [50/123], Loss: 0.4548, Acc : 80.99, mIoU 57.01\n","Step [100/123], Loss: 0.4701, Acc : 80.72, mIoU 57.06\n","Epoch time : 41.8s\n","Loss 0.5631,  Acc 80.63,  IoU 56.7459\n","EPOCH 73/100\n","Step [50/366], Loss: 0.2688, Acc : 89.56, mIoU 80.92\n","Step [100/366], Loss: 0.1923, Acc : 89.82, mIoU 81.34\n","Step [150/366], Loss: 0.4325, Acc : 89.45, mIoU 80.35\n","Step [200/366], Loss: 0.2722, Acc : 89.45, mIoU 79.97\n","Step [250/366], Loss: 0.2963, Acc : 89.41, mIoU 80.14\n","Step [300/366], Loss: 0.2676, Acc : 89.43, mIoU 80.31\n","Step [350/366], Loss: 0.3047, Acc : 89.40, mIoU 80.14\n","Epoch time : 350.7s\n","Validation . . . \n","Step [50/123], Loss: 0.7390, Acc : 80.89, mIoU 57.89\n","Step [100/123], Loss: 0.7024, Acc : 80.88, mIoU 57.35\n","Epoch time : 43.0s\n","Loss 0.3955,  Acc 80.98,  IoU 57.0250\n","EPOCH 74/100\n","Step [50/366], Loss: 0.3557, Acc : 89.44, mIoU 79.04\n","Step [100/366], Loss: 0.1742, Acc : 89.65, mIoU 80.22\n","Step [150/366], Loss: 0.2981, Acc : 89.45, mIoU 80.29\n","Step [200/366], Loss: 0.2624, Acc : 89.57, mIoU 80.32\n","Step [250/366], Loss: 0.3930, Acc : 89.52, mIoU 80.28\n","Step [300/366], Loss: 0.2305, Acc : 89.59, mIoU 80.26\n","Step [350/366], Loss: 0.2524, Acc : 89.65, mIoU 80.43\n","Epoch time : 348.7s\n","Validation . . . \n","Step [50/123], Loss: 1.0500, Acc : 81.38, mIoU 57.24\n","Step [100/123], Loss: 0.5539, Acc : 81.19, mIoU 57.65\n","Epoch time : 42.6s\n","Loss 0.6477,  Acc 81.02,  IoU 57.4277\n","EPOCH 75/100\n","Step [50/366], Loss: 0.2371, Acc : 90.07, mIoU 81.84\n","Step [100/366], Loss: 0.2073, Acc : 90.10, mIoU 81.50\n","Step [150/366], Loss: 0.2139, Acc : 90.10, mIoU 81.58\n","Step [200/366], Loss: 0.2293, Acc : 90.14, mIoU 81.71\n","Step [250/366], Loss: 0.3183, Acc : 90.05, mIoU 81.78\n","Step [300/366], Loss: 0.2412, Acc : 89.94, mIoU 81.50\n","Step [350/366], Loss: 0.2235, Acc : 89.91, mIoU 81.34\n","Epoch time : 349.6s\n","Validation . . . \n","Step [50/123], Loss: 0.7012, Acc : 81.76, mIoU 58.27\n","Step [100/123], Loss: 0.5897, Acc : 81.58, mIoU 57.85\n","Epoch time : 42.5s\n","Loss 0.3229,  Acc 81.45,  IoU 57.6977\n","EPOCH 76/100\n","Step [50/366], Loss: 0.3903, Acc : 90.08, mIoU 81.84\n","Step [100/366], Loss: 0.2905, Acc : 89.84, mIoU 80.76\n","Step [150/366], Loss: 0.3868, Acc : 89.90, mIoU 81.01\n","Step [200/366], Loss: 0.3669, Acc : 89.95, mIoU 81.23\n","Step [250/366], Loss: 0.2529, Acc : 89.81, mIoU 81.20\n","Step [300/366], Loss: 0.3131, Acc : 89.73, mIoU 80.87\n","Step [350/366], Loss: 0.3436, Acc : 89.75, mIoU 80.69\n","Epoch time : 342.6s\n","Validation . . . \n","Step [50/123], Loss: 0.5499, Acc : 81.96, mIoU 57.35\n","Step [100/123], Loss: 1.2766, Acc : 81.19, mIoU 57.56\n","Epoch time : 43.4s\n","Loss 0.5892,  Acc 81.26,  IoU 57.4357\n","EPOCH 77/100\n","Step [50/366], Loss: 0.2695, Acc : 89.25, mIoU 79.86\n","Step [100/366], Loss: 0.2856, Acc : 89.46, mIoU 80.01\n","Step [150/366], Loss: 0.3573, Acc : 89.49, mIoU 80.11\n","Step [200/366], Loss: 0.3704, Acc : 89.47, mIoU 79.83\n","Step [250/366], Loss: 0.1697, Acc : 89.42, mIoU 79.82\n","Step [300/366], Loss: 0.3767, Acc : 89.43, mIoU 79.94\n","Step [350/366], Loss: 0.2540, Acc : 89.54, mIoU 80.19\n","Epoch time : 352.3s\n","Validation . . . \n","Step [50/123], Loss: 0.4057, Acc : 82.13, mIoU 58.89\n","Step [100/123], Loss: 0.5991, Acc : 81.20, mIoU 57.63\n","Epoch time : 43.1s\n","Loss 0.5022,  Acc 81.15,  IoU 57.9097\n","EPOCH 78/100\n","Step [50/366], Loss: 0.1816, Acc : 90.24, mIoU 83.15\n","Step [100/366], Loss: 0.2501, Acc : 90.59, mIoU 83.78\n","Step [150/366], Loss: 0.2429, Acc : 90.37, mIoU 83.08\n","Step [200/366], Loss: 0.3167, Acc : 90.28, mIoU 82.51\n","Step [250/366], Loss: 0.1952, Acc : 90.23, mIoU 82.30\n","Step [300/366], Loss: 0.2282, Acc : 90.20, mIoU 82.05\n","Step [350/366], Loss: 0.2641, Acc : 90.16, mIoU 81.86\n","Epoch time : 330.6s\n","Validation . . . \n","Step [50/123], Loss: 0.4593, Acc : 80.48, mIoU 55.71\n","Step [100/123], Loss: 0.5528, Acc : 80.91, mIoU 57.27\n","Epoch time : 41.0s\n","Loss 0.3918,  Acc 80.93,  IoU 57.3892\n","EPOCH 79/100\n","Step [50/366], Loss: 0.4200, Acc : 89.72, mIoU 77.60\n","Step [100/366], Loss: 0.2006, Acc : 89.82, mIoU 78.31\n","Step [150/366], Loss: 0.2735, Acc : 89.82, mIoU 78.96\n","Step [200/366], Loss: 0.2220, Acc : 89.69, mIoU 79.62\n","Step [250/366], Loss: 0.4105, Acc : 89.77, mIoU 79.82\n","Step [300/366], Loss: 0.2033, Acc : 89.87, mIoU 80.06\n","Step [350/366], Loss: 0.2198, Acc : 89.93, mIoU 80.36\n","Epoch time : 331.4s\n","Validation . . . \n","Step [50/123], Loss: 0.6707, Acc : 80.97, mIoU 56.34\n","Step [100/123], Loss: 0.4947, Acc : 80.91, mIoU 56.13\n","Epoch time : 40.7s\n","Loss 0.6617,  Acc 80.81,  IoU 56.4341\n","EPOCH 80/100\n","Step [50/366], Loss: 0.3011, Acc : 90.53, mIoU 82.31\n","Step [100/366], Loss: 0.3349, Acc : 90.66, mIoU 82.25\n","Step [150/366], Loss: 0.2151, Acc : 90.40, mIoU 81.99\n","Step [200/366], Loss: 0.1884, Acc : 90.36, mIoU 82.21\n","Step [250/366], Loss: 0.2319, Acc : 90.38, mIoU 82.16\n","Step [300/366], Loss: 0.1714, Acc : 90.36, mIoU 81.96\n","Step [350/366], Loss: 0.1951, Acc : 90.29, mIoU 81.93\n","Epoch time : 333.9s\n","Validation . . . \n","Step [50/123], Loss: 0.3157, Acc : 81.59, mIoU 55.45\n","Step [100/123], Loss: 0.5818, Acc : 81.07, mIoU 57.04\n","Epoch time : 42.5s\n","Loss 0.4146,  Acc 81.02,  IoU 56.9933\n","EPOCH 81/100\n","Step [50/366], Loss: 0.2307, Acc : 90.78, mIoU 82.35\n","Step [100/366], Loss: 0.2062, Acc : 90.41, mIoU 82.43\n","Step [150/366], Loss: 0.2994, Acc : 90.22, mIoU 82.27\n","Step [200/366], Loss: 0.2908, Acc : 90.20, mIoU 82.34\n","Step [250/366], Loss: 0.2244, Acc : 90.29, mIoU 82.22\n","Step [300/366], Loss: 0.2449, Acc : 90.16, mIoU 82.03\n","Step [350/366], Loss: 0.2148, Acc : 90.11, mIoU 81.72\n","Epoch time : 336.2s\n","Validation . . . \n","Step [50/123], Loss: 0.4663, Acc : 81.40, mIoU 57.98\n","Step [100/123], Loss: 0.3907, Acc : 81.43, mIoU 57.66\n","Epoch time : 39.9s\n","Loss 0.7017,  Acc 80.96,  IoU 57.0482\n","EPOCH 82/100\n","Step [50/366], Loss: 0.1958, Acc : 90.81, mIoU 83.79\n","Step [100/366], Loss: 0.2452, Acc : 90.62, mIoU 83.03\n","Step [150/366], Loss: 0.1738, Acc : 90.66, mIoU 82.73\n","Step [200/366], Loss: 0.1592, Acc : 90.68, mIoU 82.78\n","Step [250/366], Loss: 0.2391, Acc : 90.67, mIoU 82.41\n","Step [300/366], Loss: 0.2490, Acc : 90.66, mIoU 82.34\n","Step [350/366], Loss: 0.2251, Acc : 90.61, mIoU 82.25\n","Epoch time : 334.1s\n","Validation . . . \n","Step [50/123], Loss: 0.7017, Acc : 79.81, mIoU 56.59\n","Step [100/123], Loss: 0.5259, Acc : 80.27, mIoU 57.28\n","Epoch time : 40.7s\n","Loss 0.5117,  Acc 80.57,  IoU 57.1360\n","EPOCH 83/100\n","Step [50/366], Loss: 0.2877, Acc : 90.50, mIoU 82.65\n","Step [100/366], Loss: 0.2596, Acc : 90.52, mIoU 83.37\n","Step [150/366], Loss: 0.2868, Acc : 90.66, mIoU 83.09\n","Step [200/366], Loss: 0.2344, Acc : 90.64, mIoU 82.98\n","Step [250/366], Loss: 0.2877, Acc : 90.68, mIoU 82.99\n","Step [300/366], Loss: 0.2328, Acc : 90.66, mIoU 82.91\n","Step [350/366], Loss: 0.2694, Acc : 90.55, mIoU 82.63\n","Epoch time : 348.9s\n","Validation . . . \n","Step [50/123], Loss: 0.7482, Acc : 80.87, mIoU 57.01\n","Step [100/123], Loss: 0.6678, Acc : 80.99, mIoU 56.79\n","Epoch time : 42.4s\n","Loss 0.5843,  Acc 81.01,  IoU 56.3947\n","EPOCH 84/100\n","Step [50/366], Loss: 0.2816, Acc : 90.86, mIoU 82.19\n","Step [100/366], Loss: 0.2336, Acc : 90.83, mIoU 82.25\n","Step [150/366], Loss: 0.2555, Acc : 90.69, mIoU 82.08\n","Step [200/366], Loss: 0.1964, Acc : 90.56, mIoU 82.18\n","Step [250/366], Loss: 0.4021, Acc : 90.43, mIoU 82.15\n","Step [300/366], Loss: 0.2155, Acc : 90.49, mIoU 82.01\n","Step [350/366], Loss: 0.2654, Acc : 90.51, mIoU 82.06\n","Epoch time : 345.8s\n","Validation . . . \n","Step [50/123], Loss: 0.5980, Acc : 81.19, mIoU 57.39\n","Step [100/123], Loss: 0.7236, Acc : 81.21, mIoU 58.05\n","Epoch time : 41.8s\n","Loss 0.4774,  Acc 81.15,  IoU 57.8127\n","EPOCH 85/100\n","Step [50/366], Loss: 0.2381, Acc : 90.46, mIoU 82.16\n","Step [100/366], Loss: 0.1926, Acc : 90.54, mIoU 82.67\n","Step [150/366], Loss: 0.2404, Acc : 90.60, mIoU 82.75\n","Step [200/366], Loss: 0.2001, Acc : 90.64, mIoU 82.85\n","Step [250/366], Loss: 0.2542, Acc : 90.71, mIoU 82.81\n","Step [300/366], Loss: 0.2880, Acc : 90.77, mIoU 82.91\n","Step [350/366], Loss: 0.2580, Acc : 90.71, mIoU 82.83\n","Epoch time : 343.8s\n","Validation . . . \n","Step [50/123], Loss: 0.9505, Acc : 81.79, mIoU 59.51\n","Step [100/123], Loss: 0.4922, Acc : 81.10, mIoU 58.77\n","Epoch time : 42.1s\n","Loss 0.4614,  Acc 80.86,  IoU 58.2330\n","EPOCH 86/100\n","Step [50/366], Loss: 0.2113, Acc : 90.72, mIoU 83.39\n","Step [100/366], Loss: 0.3130, Acc : 90.95, mIoU 83.49\n","Step [150/366], Loss: 0.2016, Acc : 90.88, mIoU 83.18\n","Step [200/366], Loss: 0.2603, Acc : 90.84, mIoU 83.08\n","Step [250/366], Loss: 0.3281, Acc : 90.81, mIoU 82.98\n","Step [300/366], Loss: 0.2611, Acc : 90.77, mIoU 82.89\n","Step [350/366], Loss: 0.2463, Acc : 90.63, mIoU 82.57\n","Epoch time : 345.1s\n","Validation . . . \n","Step [50/123], Loss: 0.4465, Acc : 80.40, mIoU 56.02\n","Step [100/123], Loss: 0.9804, Acc : 80.44, mIoU 56.46\n","Epoch time : 42.6s\n","Loss 0.7862,  Acc 80.51,  IoU 56.4027\n","EPOCH 87/100\n","Step [50/366], Loss: 0.2006, Acc : 90.12, mIoU 82.66\n","Step [100/366], Loss: 0.2585, Acc : 90.57, mIoU 82.78\n","Step [150/366], Loss: 0.2777, Acc : 90.61, mIoU 83.43\n","Step [200/366], Loss: 0.2019, Acc : 90.71, mIoU 83.38\n","Step [250/366], Loss: 0.2341, Acc : 90.80, mIoU 83.31\n","Step [300/366], Loss: 0.1948, Acc : 90.73, mIoU 83.22\n","Step [350/366], Loss: 0.2933, Acc : 90.75, mIoU 83.16\n","Epoch time : 348.3s\n","Validation . . . \n","Step [50/123], Loss: 0.6131, Acc : 80.56, mIoU 56.58\n","Step [100/123], Loss: 0.6552, Acc : 80.95, mIoU 56.37\n","Epoch time : 43.1s\n","Loss 0.5338,  Acc 80.82,  IoU 56.6009\n","EPOCH 88/100\n","Step [50/366], Loss: 0.3003, Acc : 91.33, mIoU 82.96\n","Step [100/366], Loss: 0.2256, Acc : 91.04, mIoU 82.72\n","Step [150/366], Loss: 0.2415, Acc : 91.04, mIoU 83.02\n","Step [200/366], Loss: 0.3540, Acc : 90.80, mIoU 82.71\n","Step [250/366], Loss: 0.2586, Acc : 90.85, mIoU 82.88\n","Step [300/366], Loss: 0.2956, Acc : 90.79, mIoU 82.81\n","Step [350/366], Loss: 0.2678, Acc : 90.74, mIoU 82.82\n","Epoch time : 346.5s\n","Validation . . . \n","Step [50/123], Loss: 0.8939, Acc : 79.99, mIoU 53.91\n","Step [100/123], Loss: 0.4597, Acc : 79.71, mIoU 55.79\n","Epoch time : 42.9s\n","Loss 0.8054,  Acc 79.83,  IoU 55.9916\n","EPOCH 89/100\n","Step [50/366], Loss: 0.2063, Acc : 91.96, mIoU 83.50\n","Step [100/366], Loss: 0.2330, Acc : 91.70, mIoU 83.65\n","Step [150/366], Loss: 0.2454, Acc : 91.50, mIoU 83.73\n","Step [200/366], Loss: 0.2020, Acc : 91.42, mIoU 83.79\n","Step [250/366], Loss: 0.2349, Acc : 91.37, mIoU 83.94\n","Step [300/366], Loss: 0.2752, Acc : 91.31, mIoU 83.84\n","Step [350/366], Loss: 0.2886, Acc : 91.18, mIoU 83.72\n","Epoch time : 352.3s\n","Validation . . . \n","Step [50/123], Loss: 0.8739, Acc : 80.04, mIoU 56.54\n","Step [100/123], Loss: 0.5909, Acc : 80.24, mIoU 56.39\n","Epoch time : 42.6s\n","Loss 0.6462,  Acc 80.43,  IoU 56.5728\n","EPOCH 90/100\n","Step [50/366], Loss: 0.2056, Acc : 91.31, mIoU 84.26\n","Step [100/366], Loss: 0.3672, Acc : 91.22, mIoU 83.61\n","Step [150/366], Loss: 0.2176, Acc : 91.01, mIoU 83.20\n","Step [200/366], Loss: 0.2039, Acc : 91.15, mIoU 83.46\n","Step [250/366], Loss: 0.2407, Acc : 91.20, mIoU 83.62\n","Step [300/366], Loss: 0.1893, Acc : 91.11, mIoU 83.46\n","Step [350/366], Loss: 0.2226, Acc : 91.04, mIoU 83.25\n","Epoch time : 346.0s\n","Validation . . . \n","Step [50/123], Loss: 1.0187, Acc : 80.11, mIoU 56.25\n","Step [100/123], Loss: 0.9359, Acc : 80.22, mIoU 56.97\n","Epoch time : 42.6s\n","Loss 0.5206,  Acc 80.27,  IoU 57.1926\n","EPOCH 91/100\n","Step [50/366], Loss: 0.2944, Acc : 91.25, mIoU 82.55\n","Step [100/366], Loss: 0.1471, Acc : 91.42, mIoU 83.41\n","Step [150/366], Loss: 0.2435, Acc : 91.50, mIoU 83.80\n","Step [200/366], Loss: 0.2437, Acc : 91.35, mIoU 83.77\n","Step [250/366], Loss: 0.1433, Acc : 91.29, mIoU 83.75\n","Step [300/366], Loss: 0.3236, Acc : 91.20, mIoU 83.60\n","Step [350/366], Loss: 0.1940, Acc : 91.06, mIoU 83.35\n","Epoch time : 343.8s\n","Validation . . . \n","Step [50/123], Loss: 0.8617, Acc : 80.86, mIoU 55.90\n","Step [100/123], Loss: 0.6627, Acc : 81.27, mIoU 56.38\n","Epoch time : 41.8s\n","Loss 0.7172,  Acc 80.97,  IoU 56.4184\n","EPOCH 92/100\n","Step [50/366], Loss: 0.1889, Acc : 92.07, mIoU 84.58\n","Step [100/366], Loss: 0.3005, Acc : 91.84, mIoU 84.85\n","Step [150/366], Loss: 0.2034, Acc : 91.50, mIoU 84.09\n","Step [200/366], Loss: 0.2568, Acc : 91.06, mIoU 83.54\n","Step [250/366], Loss: 0.2319, Acc : 90.96, mIoU 83.31\n","Step [300/366], Loss: 0.2247, Acc : 90.95, mIoU 83.19\n","Step [350/366], Loss: 0.2048, Acc : 91.01, mIoU 83.34\n","Epoch time : 347.8s\n","Validation . . . \n","Step [50/123], Loss: 0.7796, Acc : 82.30, mIoU 57.89\n","Step [100/123], Loss: 0.5106, Acc : 81.88, mIoU 58.97\n","Epoch time : 42.7s\n","Loss 0.4152,  Acc 81.42,  IoU 58.2120\n","EPOCH 93/100\n","Step [50/366], Loss: 0.1647, Acc : 91.94, mIoU 84.59\n","Step [100/366], Loss: 0.1897, Acc : 91.86, mIoU 84.85\n","Step [150/366], Loss: 0.2251, Acc : 91.94, mIoU 84.87\n","Step [200/366], Loss: 0.2605, Acc : 91.72, mIoU 84.54\n","Step [250/366], Loss: 0.2717, Acc : 91.72, mIoU 84.41\n","Step [300/366], Loss: 0.2322, Acc : 91.51, mIoU 84.18\n","Step [350/366], Loss: 0.2278, Acc : 91.42, mIoU 84.03\n","Epoch time : 346.9s\n","Validation . . . \n","Step [50/123], Loss: 0.5793, Acc : 81.07, mIoU 55.81\n","Step [100/123], Loss: 1.0234, Acc : 81.12, mIoU 56.73\n","Epoch time : 42.5s\n","Loss 0.9868,  Acc 80.87,  IoU 56.7860\n","EPOCH 94/100\n","Step [50/366], Loss: 0.1494, Acc : 91.65, mIoU 83.05\n","Step [100/366], Loss: 0.1490, Acc : 91.77, mIoU 84.02\n","Step [150/366], Loss: 0.2000, Acc : 91.60, mIoU 83.94\n","Step [200/366], Loss: 0.3015, Acc : 91.43, mIoU 83.87\n","Step [250/366], Loss: 0.3315, Acc : 91.38, mIoU 83.83\n","Step [300/366], Loss: 0.1913, Acc : 91.39, mIoU 83.97\n","Step [350/366], Loss: 0.1665, Acc : 91.42, mIoU 83.94\n","Epoch time : 349.3s\n","Validation . . . \n","Step [50/123], Loss: 0.5551, Acc : 81.54, mIoU 58.75\n","Step [100/123], Loss: 0.5647, Acc : 81.18, mIoU 58.17\n","Epoch time : 42.2s\n","Loss 0.8827,  Acc 81.29,  IoU 58.3591\n","EPOCH 95/100\n","Step [50/366], Loss: 0.2057, Acc : 91.76, mIoU 85.07\n","Step [100/366], Loss: 0.1877, Acc : 91.72, mIoU 84.99\n","Step [150/366], Loss: 0.2685, Acc : 91.84, mIoU 85.10\n","Step [200/366], Loss: 0.1744, Acc : 91.77, mIoU 84.91\n","Step [250/366], Loss: 0.1848, Acc : 91.59, mIoU 84.54\n","Step [300/366], Loss: 0.2154, Acc : 91.53, mIoU 84.37\n","Step [350/366], Loss: 0.2025, Acc : 91.34, mIoU 84.03\n","Epoch time : 348.5s\n","Validation . . . \n","Step [50/123], Loss: 0.6593, Acc : 81.57, mIoU 55.88\n","Step [100/123], Loss: 0.8280, Acc : 80.94, mIoU 57.07\n","Epoch time : 43.2s\n","Loss 0.5058,  Acc 81.07,  IoU 57.2151\n","EPOCH 96/100\n","Step [50/366], Loss: 0.2159, Acc : 91.55, mIoU 83.50\n","Step [100/366], Loss: 0.1376, Acc : 91.70, mIoU 84.43\n","Step [150/366], Loss: 0.2430, Acc : 91.60, mIoU 84.47\n","Step [200/366], Loss: 0.2037, Acc : 91.49, mIoU 84.14\n","Step [250/366], Loss: 0.1693, Acc : 91.43, mIoU 84.18\n","Step [300/366], Loss: 0.3065, Acc : 91.40, mIoU 84.22\n","Step [350/366], Loss: 0.2497, Acc : 91.40, mIoU 84.13\n","Epoch time : 348.5s\n","Validation . . . \n","Step [50/123], Loss: 0.6337, Acc : 80.51, mIoU 55.42\n","Step [100/123], Loss: 0.3793, Acc : 80.87, mIoU 56.71\n","Epoch time : 42.6s\n","Loss 0.5843,  Acc 80.81,  IoU 56.6624\n","EPOCH 97/100\n","Step [50/366], Loss: 0.2087, Acc : 92.11, mIoU 85.79\n","Step [100/366], Loss: 0.2303, Acc : 91.90, mIoU 85.36\n","Step [150/366], Loss: 0.2250, Acc : 92.01, mIoU 85.39\n","Step [200/366], Loss: 0.1455, Acc : 91.98, mIoU 85.36\n","Step [250/366], Loss: 0.1858, Acc : 91.91, mIoU 85.18\n","Step [300/366], Loss: 0.2026, Acc : 91.85, mIoU 85.10\n","Step [350/366], Loss: 0.2367, Acc : 91.74, mIoU 84.80\n","Epoch time : 347.9s\n","Validation . . . \n","Step [50/123], Loss: 0.2992, Acc : 81.15, mIoU 58.40\n","Step [100/123], Loss: 1.0629, Acc : 81.15, mIoU 58.00\n","Epoch time : 42.3s\n","Loss 0.6748,  Acc 81.42,  IoU 58.3208\n","EPOCH 98/100\n","Step [50/366], Loss: 0.3105, Acc : 92.03, mIoU 84.88\n","Step [100/366], Loss: 0.2139, Acc : 91.74, mIoU 84.30\n","Step [150/366], Loss: 0.2092, Acc : 91.58, mIoU 83.85\n","Step [200/366], Loss: 0.1958, Acc : 91.53, mIoU 83.83\n","Step [250/366], Loss: 0.2139, Acc : 91.41, mIoU 83.67\n","Step [300/366], Loss: 0.1969, Acc : 91.41, mIoU 83.71\n","Step [350/366], Loss: 0.2054, Acc : 91.46, mIoU 83.79\n","Epoch time : 346.8s\n","Validation . . . \n","Step [50/123], Loss: 0.6920, Acc : 79.87, mIoU 57.14\n","Step [100/123], Loss: 0.9407, Acc : 80.08, mIoU 57.28\n","Epoch time : 42.5s\n","Loss 0.5366,  Acc 80.40,  IoU 57.8667\n","EPOCH 99/100\n","Step [50/366], Loss: 0.2018, Acc : 92.15, mIoU 84.97\n","Step [100/366], Loss: 0.2024, Acc : 91.96, mIoU 85.08\n","Step [150/366], Loss: 0.1929, Acc : 91.94, mIoU 85.30\n","Step [200/366], Loss: 0.2251, Acc : 91.92, mIoU 85.11\n","Step [250/366], Loss: 0.1973, Acc : 91.79, mIoU 84.93\n","Step [300/366], Loss: 0.1828, Acc : 91.77, mIoU 84.81\n","Step [350/366], Loss: 0.2098, Acc : 91.77, mIoU 84.69\n","Epoch time : 346.5s\n","Validation . . . \n","Step [50/123], Loss: 0.4843, Acc : 80.94, mIoU 58.09\n","Step [100/123], Loss: 0.5949, Acc : 81.00, mIoU 58.09\n","Epoch time : 42.1s\n","Loss 0.9122,  Acc 80.96,  IoU 57.9680\n","EPOCH 100/100\n","Step [50/366], Loss: 0.1733, Acc : 92.28, mIoU 85.59\n","Step [100/366], Loss: 0.2477, Acc : 92.24, mIoU 85.53\n","Step [150/366], Loss: 0.2541, Acc : 92.07, mIoU 85.36\n","Step [200/366], Loss: 0.2530, Acc : 91.92, mIoU 85.14\n","Step [250/366], Loss: 0.1935, Acc : 91.88, mIoU 85.11\n","Step [300/366], Loss: 0.2241, Acc : 91.83, mIoU 85.04\n","Step [350/366], Loss: 0.2072, Acc : 91.82, mIoU 84.94\n","Epoch time : 344.7s\n","Validation . . . \n","Step [50/123], Loss: 0.7294, Acc : 82.00, mIoU 57.00\n","Step [100/123], Loss: 0.6248, Acc : 80.97, mIoU 56.67\n","Epoch time : 43.5s\n","Loss 0.5355,  Acc 81.01,  IoU 57.6124\n","Testing best epoch . . .\n","Step [50/118], Loss: 0.7243, Acc : 80.41, mIoU 56.13\n","Step [100/118], Loss: 0.4933, Acc : 80.83, mIoU 56.46\n","Epoch time : 41.1s\n","Loss 0.4513,  Acc 80.93,  IoU 56.8885\n","Reading patch metadata . . .\n","Done.\n","Dataset ready.\n","Reading patch metadata . . .\n","Done.\n","Dataset ready.\n","Reading patch metadata . . .\n","Done.\n","Dataset ready.\n","Train 369, Val 118, Test 120\n","Model has 1553796 trainable params\n","UNet3D(\n","  (en3): Sequential(\n","    (0): Conv3d(10, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (pool_3): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (en4): Sequential(\n","    (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (pool_4): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (center_in): Sequential(\n","    (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (center_out): Sequential(\n","    (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): ConvTranspose3d(128, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n","  )\n","  (dc4): Sequential(\n","    (0): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (trans3): Sequential(\n","    (0): ConvTranspose3d(64, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n","    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (dc3): Sequential(\n","    (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n","    (3): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n","    (4): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n","  )\n","  (final): Conv3d(16, 20, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",")\n","EPOCH 1/100\n","Step [50/369], Loss: 1.5914, Acc : 41.26, mIoU 4.17\n","Step [100/369], Loss: 1.4048, Acc : 46.69, mIoU 5.51\n","Step [150/369], Loss: 1.6876, Acc : 49.67, mIoU 6.19\n","Step [200/369], Loss: 1.2073, Acc : 51.91, mIoU 7.15\n","Step [250/369], Loss: 1.6022, Acc : 53.59, mIoU 7.92\n","Step [300/369], Loss: 1.5379, Acc : 54.76, mIoU 9.12\n","Step [350/369], Loss: 1.3242, Acc : 56.41, mIoU 10.14\n","Epoch time : 351.4s\n","Validation . . . \n","Step [50/118], Loss: 1.1790, Acc : 61.71, mIoU 14.76\n","Step [100/118], Loss: 1.3093, Acc : 61.92, mIoU 14.59\n","Epoch time : 40.1s\n","Loss 0.8281,  Acc 61.80,  IoU 14.8806\n","EPOCH 2/100\n","Step [50/369], Loss: 0.9817, Acc : 67.53, mIoU 22.06\n","Step [100/369], Loss: 0.7255, Acc : 68.86, mIoU 22.99\n","Step [150/369], Loss: 1.0131, Acc : 68.82, mIoU 23.13\n","Step [200/369], Loss: 0.9313, Acc : 68.48, mIoU 23.73\n","Step [250/369], Loss: 1.0412, Acc : 69.09, mIoU 24.55\n","Step [300/369], Loss: 0.9268, Acc : 69.62, mIoU 25.16\n","Step [350/369], Loss: 0.5793, Acc : 69.99, mIoU 25.86\n","Epoch time : 351.2s\n","Validation . . . \n","Step [50/118], Loss: 0.8048, Acc : 72.66, mIoU 28.90\n","Step [100/118], Loss: 0.7339, Acc : 72.86, mIoU 29.51\n","Epoch time : 40.4s\n","Loss 0.7124,  Acc 73.11,  IoU 29.6455\n","EPOCH 3/100\n","Step [50/369], Loss: 0.6688, Acc : 71.44, mIoU 28.94\n","Step [100/369], Loss: 0.7769, Acc : 72.48, mIoU 29.70\n","Step [150/369], Loss: 0.7979, Acc : 72.21, mIoU 29.54\n","Step [200/369], Loss: 1.0646, Acc : 72.11, mIoU 30.27\n","Step [250/369], Loss: 0.7004, Acc : 72.22, mIoU 30.28\n","Step [300/369], Loss: 0.7107, Acc : 72.31, mIoU 30.41\n","Step [350/369], Loss: 1.1612, Acc : 72.62, mIoU 30.85\n","Epoch time : 356.4s\n","Validation . . . \n","Step [50/118], Loss: 1.0573, Acc : 72.93, mIoU 31.26\n","Step [100/118], Loss: 0.8844, Acc : 73.48, mIoU 30.74\n","Epoch time : 40.0s\n","Loss 0.7096,  Acc 73.15,  IoU 30.9043\n","EPOCH 4/100\n","Step [50/369], Loss: 0.6651, Acc : 73.68, mIoU 33.42\n","Step [100/369], Loss: 0.6041, Acc : 73.36, mIoU 32.93\n","Step [150/369], Loss: 0.7312, Acc : 73.65, mIoU 32.98\n","Step [200/369], Loss: 0.9079, Acc : 73.35, mIoU 33.38\n","Step [250/369], Loss: 0.7972, Acc : 73.68, mIoU 33.87\n","Step [300/369], Loss: 0.6406, Acc : 73.84, mIoU 34.05\n","Step [350/369], Loss: 0.6815, Acc : 73.94, mIoU 34.14\n","Epoch time : 348.6s\n","Validation . . . \n","Step [50/118], Loss: 0.5740, Acc : 74.45, mIoU 34.05\n","Step [100/118], Loss: 0.7886, Acc : 74.70, mIoU 33.65\n","Epoch time : 40.3s\n","Loss 0.6862,  Acc 74.36,  IoU 33.2365\n","EPOCH 5/100\n","Step [50/369], Loss: 0.4521, Acc : 76.74, mIoU 37.17\n","Step [100/369], Loss: 0.5320, Acc : 76.15, mIoU 37.18\n","Step [150/369], Loss: 0.9023, Acc : 75.70, mIoU 36.94\n","Step [200/369], Loss: 0.6116, Acc : 75.73, mIoU 36.73\n","Step [250/369], Loss: 0.7303, Acc : 75.33, mIoU 37.07\n","Step [300/369], Loss: 0.7351, Acc : 75.28, mIoU 37.30\n","Step [350/369], Loss: 0.8889, Acc : 75.28, mIoU 37.36\n","Epoch time : 354.3s\n","Validation . . . \n","Step [50/118], Loss: 0.8137, Acc : 73.72, mIoU 36.55\n","Step [100/118], Loss: 0.9956, Acc : 74.48, mIoU 37.08\n","Epoch time : 39.5s\n","Loss 0.6372,  Acc 74.74,  IoU 36.8629\n","EPOCH 6/100\n","Step [50/369], Loss: 0.6972, Acc : 77.82, mIoU 39.91\n","Step [100/369], Loss: 0.7619, Acc : 76.49, mIoU 39.04\n","Step [150/369], Loss: 0.8689, Acc : 75.63, mIoU 38.37\n","Step [200/369], Loss: 0.6607, Acc : 75.83, mIoU 38.42\n","Step [250/369], Loss: 0.6779, Acc : 76.00, mIoU 39.11\n","Step [300/369], Loss: 0.8154, Acc : 75.63, mIoU 39.05\n","Step [350/369], Loss: 0.4708, Acc : 75.84, mIoU 39.56\n","Epoch time : 347.3s\n","Validation . . . \n","Step [50/118], Loss: 1.0814, Acc : 74.28, mIoU 36.09\n","Step [100/118], Loss: 0.8046, Acc : 73.74, mIoU 36.93\n","Epoch time : 39.1s\n","Loss 0.8526,  Acc 73.37,  IoU 36.9904\n","EPOCH 7/100\n","Step [50/369], Loss: 0.6369, Acc : 76.16, mIoU 38.33\n","Step [100/369], Loss: 0.7390, Acc : 75.23, mIoU 39.60\n","Step [150/369], Loss: 0.5396, Acc : 75.76, mIoU 40.66\n","Step [200/369], Loss: 0.8373, Acc : 75.93, mIoU 41.17\n","Step [250/369], Loss: 0.7275, Acc : 75.84, mIoU 41.36\n","Step [300/369], Loss: 0.4527, Acc : 76.15, mIoU 41.52\n","Step [350/369], Loss: 0.6116, Acc : 76.35, mIoU 41.78\n","Epoch time : 349.3s\n","Validation . . . \n","Step [50/118], Loss: 0.6960, Acc : 76.05, mIoU 43.85\n","Step [100/118], Loss: 0.8182, Acc : 76.93, mIoU 43.74\n","Epoch time : 38.3s\n","Loss 0.7209,  Acc 76.41,  IoU 43.2555\n","EPOCH 8/100\n","Step [50/369], Loss: 0.5116, Acc : 76.72, mIoU 41.49\n","Step [100/369], Loss: 0.8167, Acc : 77.18, mIoU 42.80\n","Step [150/369], Loss: 0.5441, Acc : 77.11, mIoU 43.71\n","Step [200/369], Loss: 0.7433, Acc : 77.09, mIoU 44.04\n","Step [250/369], Loss: 0.5803, Acc : 77.07, mIoU 43.63\n","Step [300/369], Loss: 0.9557, Acc : 76.82, mIoU 43.60\n","Step [350/369], Loss: 0.4762, Acc : 77.09, mIoU 43.77\n","Epoch time : 348.8s\n","Validation . . . \n","Step [50/118], Loss: 0.8474, Acc : 74.25, mIoU 38.16\n","Step [100/118], Loss: 1.0049, Acc : 74.00, mIoU 37.95\n","Epoch time : 38.7s\n","Loss 0.7528,  Acc 74.13,  IoU 37.8444\n","EPOCH 9/100\n","Step [50/369], Loss: 0.5177, Acc : 77.35, mIoU 45.37\n","Step [100/369], Loss: 0.7019, Acc : 77.18, mIoU 45.04\n","Step [150/369], Loss: 0.6443, Acc : 77.36, mIoU 45.17\n","Step [200/369], Loss: 0.6652, Acc : 77.64, mIoU 45.02\n","Step [250/369], Loss: 0.7890, Acc : 77.80, mIoU 45.15\n","Step [300/369], Loss: 0.5423, Acc : 77.70, mIoU 45.43\n","Step [350/369], Loss: 0.5986, Acc : 77.60, mIoU 45.16\n","Epoch time : 350.8s\n","Validation . . . \n","Step [50/118], Loss: 1.0175, Acc : 76.30, mIoU 42.73\n","Step [100/118], Loss: 0.9471, Acc : 76.03, mIoU 43.21\n","Epoch time : 40.7s\n","Loss 0.7256,  Acc 75.89,  IoU 42.7358\n","EPOCH 10/100\n","Step [50/369], Loss: 0.6876, Acc : 76.35, mIoU 42.26\n","Step [100/369], Loss: 0.7013, Acc : 77.56, mIoU 43.67\n","Step [150/369], Loss: 0.6454, Acc : 77.53, mIoU 44.83\n","Step [200/369], Loss: 0.6564, Acc : 77.73, mIoU 45.40\n","Step [250/369], Loss: 0.6307, Acc : 77.55, mIoU 45.11\n","Step [300/369], Loss: 0.8094, Acc : 77.52, mIoU 45.54\n","Step [350/369], Loss: 0.8282, Acc : 77.69, mIoU 45.89\n","Epoch time : 353.1s\n","Validation . . . \n","Step [50/118], Loss: 0.7179, Acc : 77.50, mIoU 45.09\n","Step [100/118], Loss: 0.7918, Acc : 77.39, mIoU 45.12\n","Epoch time : 39.6s\n","Loss 0.7188,  Acc 77.68,  IoU 45.5892\n","EPOCH 11/100\n","Step [50/369], Loss: 1.0122, Acc : 79.39, mIoU 45.83\n","Step [100/369], Loss: 0.8074, Acc : 78.97, mIoU 47.91\n","Step [150/369], Loss: 0.6272, Acc : 78.87, mIoU 47.33\n","Step [200/369], Loss: 0.4869, Acc : 78.58, mIoU 46.58\n","Step [250/369], Loss: 0.5374, Acc : 78.61, mIoU 47.21\n","Step [300/369], Loss: 0.6119, Acc : 78.53, mIoU 47.57\n","Step [350/369], Loss: 0.8790, Acc : 78.70, mIoU 47.83\n","Epoch time : 348.6s\n","Validation . . . \n","Step [50/118], Loss: 0.6317, Acc : 78.64, mIoU 48.56\n","Step [100/118], Loss: 0.2936, Acc : 78.48, mIoU 47.38\n","Epoch time : 39.9s\n","Loss 0.8026,  Acc 78.07,  IoU 47.1742\n","EPOCH 12/100\n","Step [50/369], Loss: 0.6849, Acc : 79.62, mIoU 50.57\n","Step [100/369], Loss: 0.5264, Acc : 79.87, mIoU 50.50\n","Step [150/369], Loss: 0.6681, Acc : 79.19, mIoU 49.44\n","Step [200/369], Loss: 0.4463, Acc : 79.06, mIoU 49.27\n","Step [250/369], Loss: 0.3972, Acc : 78.82, mIoU 49.28\n","Step [300/369], Loss: 0.4599, Acc : 78.97, mIoU 49.00\n","Step [350/369], Loss: 0.5034, Acc : 79.01, mIoU 49.20\n","Epoch time : 348.8s\n","Validation . . . \n","Step [50/118], Loss: 0.4665, Acc : 78.27, mIoU 45.07\n","Step [100/118], Loss: 0.5278, Acc : 78.28, mIoU 46.10\n","Epoch time : 39.0s\n","Loss 0.6993,  Acc 78.33,  IoU 46.4015\n","EPOCH 13/100\n","Step [50/369], Loss: 0.4592, Acc : 79.02, mIoU 49.86\n","Step [100/369], Loss: 0.3997, Acc : 78.99, mIoU 51.46\n","Step [150/369], Loss: 0.8889, Acc : 79.05, mIoU 50.70\n","Step [200/369], Loss: 0.5278, Acc : 79.27, mIoU 50.15\n","Step [250/369], Loss: 0.8960, Acc : 79.25, mIoU 49.68\n","Step [300/369], Loss: 0.6400, Acc : 79.28, mIoU 49.76\n","Step [350/369], Loss: 0.6564, Acc : 79.29, mIoU 50.18\n","Epoch time : 353.4s\n","Validation . . . \n","Step [50/118], Loss: 0.6602, Acc : 79.54, mIoU 49.46\n","Step [100/118], Loss: 0.8036, Acc : 78.98, mIoU 48.13\n","Epoch time : 39.2s\n","Loss 0.8045,  Acc 78.81,  IoU 48.0695\n","EPOCH 14/100\n","Step [50/369], Loss: 1.0128, Acc : 80.54, mIoU 50.36\n","Step [100/369], Loss: 0.6759, Acc : 79.60, mIoU 49.32\n","Step [150/369], Loss: 0.5829, Acc : 79.74, mIoU 49.98\n","Step [200/369], Loss: 0.6150, Acc : 79.49, mIoU 49.44\n","Step [250/369], Loss: 0.6795, Acc : 79.29, mIoU 49.88\n","Step [300/369], Loss: 0.6708, Acc : 79.35, mIoU 50.19\n","Step [350/369], Loss: 0.3902, Acc : 79.56, mIoU 50.31\n","Epoch time : 346.8s\n","Validation . . . \n","Step [50/118], Loss: 0.7941, Acc : 78.69, mIoU 47.94\n","Step [100/118], Loss: 0.5539, Acc : 79.01, mIoU 48.29\n","Epoch time : 40.6s\n","Loss 0.5236,  Acc 78.84,  IoU 48.3096\n","EPOCH 15/100\n","Step [50/369], Loss: 0.5691, Acc : 80.67, mIoU 53.97\n","Step [100/369], Loss: 0.6400, Acc : 80.63, mIoU 54.00\n","Step [150/369], Loss: 0.6658, Acc : 80.29, mIoU 53.42\n","Step [200/369], Loss: 0.5450, Acc : 79.88, mIoU 52.58\n","Step [250/369], Loss: 0.7380, Acc : 79.96, mIoU 52.38\n","Step [300/369], Loss: 0.5827, Acc : 80.09, mIoU 52.22\n","Step [350/369], Loss: 0.4511, Acc : 79.95, mIoU 52.08\n","Epoch time : 354.2s\n","Validation . . . \n","Step [50/118], Loss: 0.3819, Acc : 78.26, mIoU 48.50\n","Step [100/118], Loss: 0.6746, Acc : 79.12, mIoU 48.96\n","Epoch time : 40.3s\n","Loss 0.4799,  Acc 79.26,  IoU 49.3496\n","EPOCH 16/100\n","Step [50/369], Loss: 0.3864, Acc : 79.33, mIoU 50.98\n","Step [100/369], Loss: 0.6634, Acc : 79.86, mIoU 51.79\n","Step [150/369], Loss: 1.1245, Acc : 79.96, mIoU 52.17\n","Step [200/369], Loss: 0.5888, Acc : 79.82, mIoU 52.09\n","Step [250/369], Loss: 0.3628, Acc : 80.03, mIoU 52.10\n","Step [300/369], Loss: 0.8542, Acc : 80.03, mIoU 52.41\n","Step [350/369], Loss: 0.6626, Acc : 80.08, mIoU 52.39\n","Epoch time : 351.4s\n","Validation . . . \n","Step [50/118], Loss: 0.8893, Acc : 78.05, mIoU 50.69\n","Step [100/118], Loss: 0.8986, Acc : 78.81, mIoU 50.67\n","Epoch time : 39.7s\n","Loss 0.6497,  Acc 79.05,  IoU 51.0721\n","EPOCH 17/100\n","Step [50/369], Loss: 0.5119, Acc : 80.14, mIoU 50.90\n","Step [100/369], Loss: 0.5578, Acc : 80.03, mIoU 51.67\n","Step [150/369], Loss: 0.4178, Acc : 79.96, mIoU 52.32\n","Step [200/369], Loss: 0.4978, Acc : 80.25, mIoU 53.00\n","Step [250/369], Loss: 0.8844, Acc : 80.33, mIoU 53.13\n","Step [300/369], Loss: 0.5596, Acc : 80.44, mIoU 52.88\n","Step [350/369], Loss: 0.6367, Acc : 80.32, mIoU 53.06\n","Epoch time : 349.5s\n","Validation . . . \n","Step [50/118], Loss: 0.6160, Acc : 78.97, mIoU 52.42\n","Step [100/118], Loss: 0.5683, Acc : 79.30, mIoU 52.73\n","Epoch time : 39.7s\n","Loss 0.7477,  Acc 79.02,  IoU 52.4478\n","EPOCH 18/100\n","Step [50/369], Loss: 0.6804, Acc : 80.53, mIoU 54.69\n","Step [100/369], Loss: 0.7578, Acc : 80.28, mIoU 54.47\n","Step [150/369], Loss: 0.5324, Acc : 80.66, mIoU 54.49\n","Step [200/369], Loss: 0.5944, Acc : 80.71, mIoU 54.61\n","Step [250/369], Loss: 0.6242, Acc : 80.67, mIoU 54.63\n","Step [300/369], Loss: 0.4441, Acc : 80.49, mIoU 54.30\n","Step [350/369], Loss: 0.5481, Acc : 80.33, mIoU 54.16\n","Epoch time : 350.8s\n","Validation . . . \n","Step [50/118], Loss: 0.2754, Acc : 79.29, mIoU 51.29\n","Step [100/118], Loss: 0.5129, Acc : 79.56, mIoU 51.20\n","Epoch time : 39.6s\n","Loss 0.4584,  Acc 79.45,  IoU 51.1922\n","EPOCH 19/100\n","Step [50/369], Loss: 0.4353, Acc : 80.91, mIoU 54.95\n","Step [100/369], Loss: 0.5685, Acc : 80.60, mIoU 54.96\n","Step [150/369], Loss: 0.4301, Acc : 80.48, mIoU 55.31\n","Step [200/369], Loss: 0.3626, Acc : 80.70, mIoU 55.88\n","Step [250/369], Loss: 0.4462, Acc : 80.59, mIoU 55.35\n","Step [300/369], Loss: 0.6266, Acc : 80.57, mIoU 55.21\n","Step [350/369], Loss: 0.3547, Acc : 80.81, mIoU 55.47\n","Epoch time : 350.2s\n","Validation . . . \n","Step [50/118], Loss: 0.7080, Acc : 80.26, mIoU 54.75\n","Step [100/118], Loss: 0.4227, Acc : 79.88, mIoU 53.15\n","Epoch time : 40.3s\n","Loss 0.5315,  Acc 80.07,  IoU 53.2029\n","EPOCH 20/100\n","Step [50/369], Loss: 0.5032, Acc : 80.66, mIoU 56.05\n","Step [100/369], Loss: 0.3364, Acc : 81.35, mIoU 56.62\n","Step [150/369], Loss: 0.6742, Acc : 81.11, mIoU 56.15\n","Step [200/369], Loss: 0.5388, Acc : 81.18, mIoU 56.03\n","Step [250/369], Loss: 0.4302, Acc : 81.18, mIoU 56.09\n","Step [300/369], Loss: 0.5012, Acc : 81.08, mIoU 56.09\n","Step [350/369], Loss: 0.3558, Acc : 81.10, mIoU 56.17\n","Epoch time : 350.9s\n","Validation . . . \n","Step [50/118], Loss: 0.4645, Acc : 78.98, mIoU 52.15\n","Step [100/118], Loss: 0.7267, Acc : 79.80, mIoU 53.59\n","Epoch time : 39.7s\n","Loss 0.8384,  Acc 79.78,  IoU 53.6294\n","EPOCH 21/100\n","Step [50/369], Loss: 0.4237, Acc : 81.80, mIoU 55.66\n","Step [100/369], Loss: 0.4522, Acc : 81.32, mIoU 56.60\n","Step [150/369], Loss: 0.6384, Acc : 81.27, mIoU 57.24\n","Step [200/369], Loss: 0.3533, Acc : 81.36, mIoU 57.03\n","Step [250/369], Loss: 0.5011, Acc : 81.30, mIoU 56.43\n","Step [300/369], Loss: 0.5684, Acc : 81.21, mIoU 56.16\n","Step [350/369], Loss: 0.5219, Acc : 81.17, mIoU 56.07\n","Epoch time : 353.5s\n","Validation . . . \n","Step [50/118], Loss: 0.6765, Acc : 78.98, mIoU 50.22\n","Step [100/118], Loss: 0.4177, Acc : 79.31, mIoU 52.50\n","Epoch time : 39.3s\n","Loss 0.5230,  Acc 79.06,  IoU 52.6883\n","EPOCH 22/100\n","Step [50/369], Loss: 0.4129, Acc : 82.25, mIoU 57.47\n","Step [100/369], Loss: 0.6285, Acc : 82.08, mIoU 57.27\n","Step [150/369], Loss: 0.3179, Acc : 81.56, mIoU 56.72\n","Step [200/369], Loss: 0.5487, Acc : 81.42, mIoU 57.72\n","Step [250/369], Loss: 0.6898, Acc : 81.39, mIoU 57.68\n","Step [300/369], Loss: 0.7076, Acc : 81.32, mIoU 57.61\n","Step [350/369], Loss: 0.5376, Acc : 81.29, mIoU 57.73\n","Epoch time : 349.4s\n","Validation . . . \n","Step [50/118], Loss: 0.5456, Acc : 79.61, mIoU 54.34\n","Step [100/118], Loss: 0.6424, Acc : 79.50, mIoU 53.13\n","Epoch time : 39.9s\n","Loss 0.4224,  Acc 79.57,  IoU 52.8949\n","EPOCH 23/100\n","Step [50/369], Loss: 0.8079, Acc : 81.10, mIoU 58.47\n","Step [100/369], Loss: 0.6707, Acc : 81.35, mIoU 58.03\n","Step [150/369], Loss: 0.4501, Acc : 81.61, mIoU 57.70\n","Step [200/369], Loss: 0.7192, Acc : 81.55, mIoU 57.64\n","Step [250/369], Loss: 0.5808, Acc : 81.74, mIoU 57.78\n","Step [300/369], Loss: 0.4938, Acc : 81.72, mIoU 57.53\n","Step [350/369], Loss: 0.3433, Acc : 81.46, mIoU 57.61\n","Epoch time : 350.4s\n","Validation . . . \n","Step [50/118], Loss: 0.5148, Acc : 80.26, mIoU 55.76\n","Step [100/118], Loss: 0.5985, Acc : 79.90, mIoU 54.56\n","Epoch time : 39.4s\n","Loss 0.6691,  Acc 79.84,  IoU 54.4269\n","EPOCH 24/100\n","Step [50/369], Loss: 0.8031, Acc : 82.63, mIoU 59.10\n","Step [100/369], Loss: 0.5594, Acc : 81.65, mIoU 58.00\n","Step [150/369], Loss: 0.3444, Acc : 81.95, mIoU 58.12\n","Step [200/369], Loss: 0.4067, Acc : 81.69, mIoU 58.17\n","Step [250/369], Loss: 0.5023, Acc : 81.65, mIoU 58.21\n","Step [300/369], Loss: 0.3202, Acc : 81.68, mIoU 58.39\n","Step [350/369], Loss: 0.5159, Acc : 81.72, mIoU 58.42\n","Epoch time : 349.4s\n","Validation . . . \n","Step [50/118], Loss: 0.4003, Acc : 79.89, mIoU 54.26\n","Step [100/118], Loss: 0.5588, Acc : 79.83, mIoU 53.80\n","Epoch time : 39.6s\n","Loss 0.3998,  Acc 79.91,  IoU 54.0239\n","EPOCH 25/100\n","Step [50/369], Loss: 0.4164, Acc : 81.77, mIoU 58.04\n","Step [100/369], Loss: 0.5836, Acc : 82.52, mIoU 59.50\n","Step [150/369], Loss: 0.5243, Acc : 82.51, mIoU 59.26\n","Step [200/369], Loss: 0.3530, Acc : 82.13, mIoU 59.15\n","Step [250/369], Loss: 0.2454, Acc : 82.32, mIoU 59.68\n","Step [300/369], Loss: 0.4093, Acc : 82.32, mIoU 59.83\n","Step [350/369], Loss: 0.7399, Acc : 82.15, mIoU 59.75\n","Epoch time : 352.0s\n","Validation . . . \n","Step [50/118], Loss: 0.7674, Acc : 79.88, mIoU 53.31\n","Step [100/118], Loss: 0.6640, Acc : 79.90, mIoU 54.16\n","Epoch time : 39.2s\n","Loss 0.3250,  Acc 79.66,  IoU 53.9210\n","EPOCH 26/100\n","Step [50/369], Loss: 0.5559, Acc : 82.29, mIoU 61.27\n","Step [100/369], Loss: 0.6448, Acc : 82.07, mIoU 61.50\n","Step [150/369], Loss: 0.5541, Acc : 81.68, mIoU 60.66\n","Step [200/369], Loss: 0.5247, Acc : 82.14, mIoU 60.56\n","Step [250/369], Loss: 0.6267, Acc : 82.12, mIoU 60.33\n","Step [300/369], Loss: 0.4174, Acc : 82.21, mIoU 60.27\n","Step [350/369], Loss: 0.5763, Acc : 82.25, mIoU 60.35\n","Epoch time : 351.5s\n","Validation . . . \n","Step [50/118], Loss: 0.3965, Acc : 80.74, mIoU 56.42\n","Step [100/118], Loss: 0.5604, Acc : 81.09, mIoU 56.68\n","Epoch time : 40.0s\n","Loss 0.5410,  Acc 81.00,  IoU 56.1902\n","EPOCH 27/100\n","Step [50/369], Loss: 0.6083, Acc : 82.11, mIoU 59.41\n","Step [100/369], Loss: 0.4692, Acc : 82.03, mIoU 59.18\n","Step [150/369], Loss: 0.7768, Acc : 82.54, mIoU 60.06\n","Step [200/369], Loss: 0.4700, Acc : 82.25, mIoU 60.16\n","Step [250/369], Loss: 0.5768, Acc : 82.22, mIoU 60.21\n","Step [300/369], Loss: 0.6305, Acc : 82.29, mIoU 60.36\n","Step [350/369], Loss: 0.5969, Acc : 82.23, mIoU 60.35\n","Epoch time : 347.7s\n","Validation . . . \n","Step [50/118], Loss: 0.4659, Acc : 79.51, mIoU 53.20\n","Step [100/118], Loss: 0.6747, Acc : 78.85, mIoU 51.92\n","Epoch time : 40.1s\n","Loss 0.6198,  Acc 79.07,  IoU 51.5629\n","EPOCH 28/100\n","Step [50/369], Loss: 0.4303, Acc : 82.21, mIoU 59.65\n","Step [100/369], Loss: 0.6480, Acc : 81.46, mIoU 59.41\n","Step [150/369], Loss: 0.3419, Acc : 81.59, mIoU 59.33\n","Step [200/369], Loss: 0.4592, Acc : 81.82, mIoU 59.99\n","Step [250/369], Loss: 0.4771, Acc : 81.90, mIoU 60.74\n","Step [300/369], Loss: 0.6120, Acc : 82.19, mIoU 60.71\n","Step [350/369], Loss: 0.3826, Acc : 82.39, mIoU 60.71\n","Epoch time : 355.3s\n","Validation . . . \n","Step [50/118], Loss: 0.5506, Acc : 80.20, mIoU 53.29\n","Step [100/118], Loss: 0.5257, Acc : 79.98, mIoU 52.61\n","Epoch time : 39.6s\n","Loss 0.3338,  Acc 80.04,  IoU 52.5891\n","EPOCH 29/100\n","Step [50/369], Loss: 0.3925, Acc : 83.21, mIoU 60.64\n","Step [100/369], Loss: 0.4052, Acc : 82.73, mIoU 60.98\n","Step [150/369], Loss: 0.5120, Acc : 82.89, mIoU 61.83\n","Step [200/369], Loss: 0.5936, Acc : 82.63, mIoU 61.79\n","Step [250/369], Loss: 0.3787, Acc : 82.61, mIoU 61.35\n","Step [300/369], Loss: 0.6248, Acc : 82.58, mIoU 61.27\n","Step [350/369], Loss: 0.5044, Acc : 82.68, mIoU 61.46\n","Epoch time : 349.3s\n","Validation . . . \n","Step [50/118], Loss: 0.5206, Acc : 82.04, mIoU 55.57\n","Step [100/118], Loss: 0.8507, Acc : 81.27, mIoU 56.16\n","Epoch time : 39.7s\n","Loss 0.4771,  Acc 81.16,  IoU 56.1560\n","EPOCH 30/100\n","Step [50/369], Loss: 0.5959, Acc : 82.83, mIoU 60.96\n","Step [100/369], Loss: 0.5211, Acc : 82.80, mIoU 61.85\n","Step [150/369], Loss: 0.4668, Acc : 82.71, mIoU 61.71\n","Step [200/369], Loss: 0.5246, Acc : 82.62, mIoU 61.70\n","Step [250/369], Loss: 0.3736, Acc : 82.54, mIoU 61.94\n","Step [300/369], Loss: 0.4541, Acc : 82.63, mIoU 61.67\n","Step [350/369], Loss: 0.2261, Acc : 82.66, mIoU 61.61\n","Epoch time : 349.7s\n","Validation . . . \n","Step [50/118], Loss: 0.4879, Acc : 81.71, mIoU 59.59\n","Step [100/118], Loss: 0.4626, Acc : 81.04, mIoU 58.06\n","Epoch time : 39.7s\n","Loss 0.5591,  Acc 80.99,  IoU 58.0167\n","EPOCH 31/100\n","Step [50/369], Loss: 0.5373, Acc : 83.68, mIoU 62.83\n","Step [100/369], Loss: 0.3963, Acc : 83.29, mIoU 61.44\n","Step [150/369], Loss: 0.6150, Acc : 82.94, mIoU 61.40\n","Step [200/369], Loss: 0.4288, Acc : 82.80, mIoU 61.81\n","Step [250/369], Loss: 0.4525, Acc : 82.83, mIoU 62.15\n","Step [300/369], Loss: 0.5519, Acc : 82.87, mIoU 62.18\n","Step [350/369], Loss: 0.4549, Acc : 82.88, mIoU 62.22\n","Epoch time : 352.3s\n","Validation . . . \n","Step [50/118], Loss: 0.4606, Acc : 80.65, mIoU 56.23\n","Step [100/118], Loss: 0.6042, Acc : 80.92, mIoU 56.25\n","Epoch time : 39.8s\n","Loss 0.3801,  Acc 80.94,  IoU 56.5810\n","EPOCH 32/100\n","Step [50/369], Loss: 0.3865, Acc : 83.61, mIoU 62.91\n","Step [100/369], Loss: 0.3668, Acc : 83.31, mIoU 63.73\n","Step [150/369], Loss: 0.5626, Acc : 83.05, mIoU 63.08\n","Step [200/369], Loss: 0.5889, Acc : 83.01, mIoU 62.96\n","Step [250/369], Loss: 0.5261, Acc : 83.08, mIoU 63.45\n","Step [300/369], Loss: 0.4036, Acc : 83.09, mIoU 63.25\n","Step [350/369], Loss: 0.5393, Acc : 83.07, mIoU 63.35\n","Epoch time : 352.6s\n","Validation . . . \n","Step [50/118], Loss: 0.4359, Acc : 81.64, mIoU 55.33\n","Step [100/118], Loss: 0.5262, Acc : 80.78, mIoU 54.40\n","Epoch time : 40.3s\n","Loss 0.4089,  Acc 80.70,  IoU 53.8656\n","EPOCH 33/100\n","Step [50/369], Loss: 0.4718, Acc : 84.36, mIoU 62.53\n","Step [100/369], Loss: 0.4975, Acc : 83.58, mIoU 63.39\n","Step [150/369], Loss: 0.3460, Acc : 83.41, mIoU 63.25\n","Step [200/369], Loss: 0.5033, Acc : 83.32, mIoU 63.84\n","Step [250/369], Loss: 0.4981, Acc : 83.41, mIoU 63.45\n","Step [300/369], Loss: 0.3270, Acc : 83.37, mIoU 63.73\n","Step [350/369], Loss: 0.2772, Acc : 83.31, mIoU 63.82\n","Epoch time : 349.9s\n","Validation . . . \n","Step [50/118], Loss: 0.5163, Acc : 81.24, mIoU 54.09\n","Step [100/118], Loss: 0.4730, Acc : 80.84, mIoU 56.04\n","Epoch time : 40.0s\n","Loss 0.4178,  Acc 80.70,  IoU 55.8557\n","EPOCH 34/100\n","Step [50/369], Loss: 0.3431, Acc : 82.64, mIoU 63.28\n","Step [100/369], Loss: 0.5127, Acc : 83.33, mIoU 65.23\n","Step [150/369], Loss: 0.5285, Acc : 83.25, mIoU 64.44\n","Step [200/369], Loss: 0.5087, Acc : 83.13, mIoU 64.17\n","Step [250/369], Loss: 0.3925, Acc : 83.16, mIoU 63.55\n","Step [300/369], Loss: 0.4229, Acc : 83.16, mIoU 63.86\n","Step [350/369], Loss: 0.3555, Acc : 83.39, mIoU 63.87\n","Epoch time : 352.2s\n","Validation . . . \n","Step [50/118], Loss: 0.6673, Acc : 80.59, mIoU 56.58\n","Step [100/118], Loss: 0.5546, Acc : 81.16, mIoU 55.96\n","Epoch time : 39.9s\n","Loss 0.4591,  Acc 81.07,  IoU 55.8642\n","EPOCH 35/100\n","Step [50/369], Loss: 0.2816, Acc : 84.05, mIoU 65.13\n","Step [100/369], Loss: 0.6498, Acc : 83.91, mIoU 65.15\n","Step [150/369], Loss: 0.2804, Acc : 83.85, mIoU 64.80\n","Step [200/369], Loss: 0.4217, Acc : 83.62, mIoU 64.39\n","Step [250/369], Loss: 0.4306, Acc : 83.75, mIoU 64.63\n","Step [300/369], Loss: 0.4338, Acc : 83.69, mIoU 64.50\n","Step [350/369], Loss: 0.5583, Acc : 83.64, mIoU 64.62\n","Epoch time : 351.2s\n","Validation . . . \n","Step [50/118], Loss: 0.6067, Acc : 80.93, mIoU 55.35\n","Step [100/118], Loss: 0.5952, Acc : 80.69, mIoU 55.08\n","Epoch time : 40.2s\n","Loss 0.6023,  Acc 80.70,  IoU 55.2560\n","EPOCH 36/100\n","Step [50/369], Loss: 0.4638, Acc : 83.64, mIoU 65.39\n","Step [100/369], Loss: 0.2767, Acc : 84.09, mIoU 65.42\n","Step [150/369], Loss: 0.3419, Acc : 83.94, mIoU 65.69\n","Step [200/369], Loss: 0.4262, Acc : 83.46, mIoU 65.17\n","Step [250/369], Loss: 0.5130, Acc : 83.61, mIoU 65.03\n","Step [300/369], Loss: 0.4781, Acc : 83.73, mIoU 64.90\n","Step [350/369], Loss: 0.4850, Acc : 83.70, mIoU 64.91\n","Epoch time : 351.7s\n","Validation . . . \n","Step [50/118], Loss: 0.3962, Acc : 80.57, mIoU 55.36\n","Step [100/118], Loss: 0.4923, Acc : 80.88, mIoU 55.54\n","Epoch time : 39.0s\n","Loss 0.6833,  Acc 80.65,  IoU 55.2952\n","EPOCH 37/100\n","Step [50/369], Loss: 0.2974, Acc : 84.46, mIoU 66.77\n","Step [100/369], Loss: 0.4516, Acc : 84.02, mIoU 65.23\n","Step [150/369], Loss: 0.4067, Acc : 83.84, mIoU 65.81\n","Step [200/369], Loss: 0.3899, Acc : 83.95, mIoU 65.47\n","Step [250/369], Loss: 0.4298, Acc : 83.88, mIoU 65.13\n","Step [300/369], Loss: 0.3749, Acc : 83.86, mIoU 65.10\n","Step [350/369], Loss: 0.3612, Acc : 83.80, mIoU 65.07\n","Epoch time : 349.9s\n","Validation . . . \n","Step [50/118], Loss: 0.4800, Acc : 79.68, mIoU 53.69\n","Step [100/118], Loss: 0.3723, Acc : 80.27, mIoU 54.73\n","Epoch time : 39.6s\n","Loss 0.5676,  Acc 80.31,  IoU 54.6618\n","EPOCH 38/100\n","Step [50/369], Loss: 0.4296, Acc : 82.92, mIoU 65.60\n","Step [100/369], Loss: 0.5687, Acc : 83.57, mIoU 66.81\n","Step [150/369], Loss: 0.4473, Acc : 83.67, mIoU 66.54\n","Step [200/369], Loss: 0.4542, Acc : 84.02, mIoU 66.78\n","Step [250/369], Loss: 0.3154, Acc : 84.08, mIoU 66.50\n","Step [300/369], Loss: 0.4969, Acc : 84.03, mIoU 66.47\n","Step [350/369], Loss: 0.5864, Acc : 83.92, mIoU 66.05\n","Epoch time : 349.3s\n","Validation . . . \n","Step [50/118], Loss: 0.4246, Acc : 80.77, mIoU 54.05\n","Step [100/118], Loss: 0.7765, Acc : 81.11, mIoU 55.19\n","Epoch time : 39.3s\n","Loss 0.5316,  Acc 80.98,  IoU 55.1855\n","EPOCH 39/100\n","Step [50/369], Loss: 0.4870, Acc : 84.42, mIoU 66.77\n","Step [100/369], Loss: 0.3103, Acc : 84.63, mIoU 67.22\n","Step [150/369], Loss: 0.4568, Acc : 84.58, mIoU 67.09\n","Step [200/369], Loss: 0.5609, Acc : 84.62, mIoU 67.29\n","Step [250/369], Loss: 0.4402, Acc : 84.50, mIoU 66.74\n","Step [300/369], Loss: 0.5664, Acc : 84.24, mIoU 66.09\n","Step [350/369], Loss: 0.3959, Acc : 84.24, mIoU 66.33\n","Epoch time : 353.8s\n","Validation . . . \n","Step [50/118], Loss: 0.7335, Acc : 80.16, mIoU 54.65\n","Step [100/118], Loss: 0.5534, Acc : 80.65, mIoU 55.20\n","Epoch time : 40.6s\n","Loss 0.5530,  Acc 80.39,  IoU 54.8927\n","EPOCH 40/100\n","Step [50/369], Loss: 0.4529, Acc : 84.85, mIoU 67.54\n","Step [100/369], Loss: 0.4366, Acc : 84.84, mIoU 68.02\n","Step [150/369], Loss: 0.4896, Acc : 84.63, mIoU 67.15\n","Step [200/369], Loss: 0.2278, Acc : 84.54, mIoU 67.56\n","Step [250/369], Loss: 0.5203, Acc : 84.34, mIoU 67.28\n","Step [300/369], Loss: 0.5035, Acc : 84.35, mIoU 67.16\n","Step [350/369], Loss: 0.4120, Acc : 84.37, mIoU 66.83\n","Epoch time : 350.6s\n","Validation . . . \n","Step [50/118], Loss: 0.3440, Acc : 81.80, mIoU 57.32\n","Step [100/118], Loss: 0.5029, Acc : 81.42, mIoU 57.73\n","Epoch time : 40.1s\n","Loss 0.6939,  Acc 81.04,  IoU 57.8201\n","EPOCH 41/100\n","Step [50/369], Loss: 0.4477, Acc : 84.75, mIoU 67.34\n","Step [100/369], Loss: 0.3396, Acc : 85.10, mIoU 68.21\n","Step [150/369], Loss: 0.6126, Acc : 84.89, mIoU 68.25\n","Step [200/369], Loss: 0.3753, Acc : 84.75, mIoU 68.33\n","Step [250/369], Loss: 0.4204, Acc : 84.79, mIoU 68.19\n","Step [300/369], Loss: 0.4240, Acc : 84.70, mIoU 67.90\n","Step [350/369], Loss: 0.3921, Acc : 84.63, mIoU 67.72\n","Epoch time : 350.8s\n","Validation . . . \n","Step [50/118], Loss: 0.8024, Acc : 79.09, mIoU 54.48\n","Step [100/118], Loss: 0.4428, Acc : 80.49, mIoU 56.89\n","Epoch time : 40.1s\n","Loss 0.4272,  Acc 80.74,  IoU 57.0401\n","EPOCH 42/100\n","Step [50/369], Loss: 0.3809, Acc : 86.04, mIoU 70.59\n","Step [100/369], Loss: 0.5233, Acc : 85.20, mIoU 68.60\n","Step [150/369], Loss: 0.4079, Acc : 85.49, mIoU 68.92\n","Step [200/369], Loss: 0.3410, Acc : 84.97, mIoU 68.66\n","Step [250/369], Loss: 0.5542, Acc : 84.73, mIoU 67.63\n","Step [300/369], Loss: 0.3726, Acc : 84.80, mIoU 67.66\n","Step [350/369], Loss: 0.4689, Acc : 84.66, mIoU 67.27\n","Epoch time : 350.5s\n","Validation . . . \n","Step [50/118], Loss: 0.3683, Acc : 78.71, mIoU 53.10\n","Step [100/118], Loss: 1.1144, Acc : 78.99, mIoU 54.52\n","Epoch time : 39.0s\n","Loss 0.6548,  Acc 78.81,  IoU 54.6628\n","EPOCH 43/100\n","Step [50/369], Loss: 0.4490, Acc : 84.60, mIoU 68.62\n","Step [100/369], Loss: 0.4471, Acc : 84.70, mIoU 68.48\n","Step [150/369], Loss: 0.5562, Acc : 84.62, mIoU 68.71\n","Step [200/369], Loss: 0.3430, Acc : 84.55, mIoU 68.31\n","Step [250/369], Loss: 0.6659, Acc : 84.61, mIoU 68.24\n","Step [300/369], Loss: 0.3165, Acc : 84.59, mIoU 68.14\n","Step [350/369], Loss: 0.4639, Acc : 84.59, mIoU 67.94\n","Epoch time : 353.9s\n","Validation . . . \n","Step [50/118], Loss: 0.2761, Acc : 81.16, mIoU 54.70\n","Step [100/118], Loss: 0.6298, Acc : 80.97, mIoU 55.26\n","Epoch time : 40.1s\n","Loss 0.4885,  Acc 80.74,  IoU 54.9955\n","EPOCH 44/100\n","Step [50/369], Loss: 0.4682, Acc : 84.82, mIoU 67.07\n","Step [100/369], Loss: 0.4061, Acc : 85.16, mIoU 68.64\n","Step [150/369], Loss: 0.4300, Acc : 84.98, mIoU 68.53\n","Step [200/369], Loss: 0.4526, Acc : 85.02, mIoU 68.29\n","Step [250/369], Loss: 0.4700, Acc : 84.94, mIoU 68.44\n","Step [300/369], Loss: 0.4381, Acc : 84.98, mIoU 68.57\n","Step [350/369], Loss: 0.3885, Acc : 84.95, mIoU 68.42\n","Epoch time : 350.7s\n","Validation . . . \n","Step [50/118], Loss: 0.6079, Acc : 80.30, mIoU 53.63\n","Step [100/118], Loss: 0.6718, Acc : 80.62, mIoU 54.91\n","Epoch time : 40.0s\n","Loss 0.3671,  Acc 80.82,  IoU 55.2218\n","EPOCH 45/100\n","Step [50/369], Loss: 0.3604, Acc : 85.33, mIoU 68.58\n","Step [100/369], Loss: 0.3933, Acc : 84.98, mIoU 69.25\n","Step [150/369], Loss: 0.5085, Acc : 84.76, mIoU 69.13\n","Step [200/369], Loss: 0.2714, Acc : 84.57, mIoU 68.20\n","Step [250/369], Loss: 0.3516, Acc : 84.60, mIoU 68.39\n","Step [300/369], Loss: 0.3309, Acc : 84.81, mIoU 68.99\n","Step [350/369], Loss: 0.4041, Acc : 84.88, mIoU 68.89\n","Epoch time : 352.1s\n","Validation . . . \n","Step [50/118], Loss: 0.5690, Acc : 80.90, mIoU 55.91\n","Step [100/118], Loss: 0.7194, Acc : 81.17, mIoU 57.25\n","Epoch time : 40.3s\n","Loss 0.5109,  Acc 81.18,  IoU 57.4999\n","EPOCH 46/100\n","Step [50/369], Loss: 0.3383, Acc : 86.17, mIoU 70.68\n","Step [100/369], Loss: 0.3616, Acc : 85.77, mIoU 70.62\n","Step [150/369], Loss: 0.3136, Acc : 85.58, mIoU 70.00\n","Step [200/369], Loss: 0.3723, Acc : 85.28, mIoU 70.53\n","Step [250/369], Loss: 0.3731, Acc : 85.52, mIoU 70.36\n","Step [300/369], Loss: 0.5580, Acc : 85.54, mIoU 70.21\n","Step [350/369], Loss: 0.4688, Acc : 85.43, mIoU 69.95\n","Epoch time : 351.2s\n","Validation . . . \n","Step [50/118], Loss: 0.3936, Acc : 81.29, mIoU 56.80\n","Step [100/118], Loss: 0.7397, Acc : 80.72, mIoU 56.30\n","Epoch time : 39.1s\n","Loss 0.5027,  Acc 80.62,  IoU 56.5154\n","EPOCH 47/100\n","Step [50/369], Loss: 0.4742, Acc : 85.00, mIoU 69.46\n","Step [100/369], Loss: 0.3030, Acc : 85.61, mIoU 70.89\n","Step [150/369], Loss: 0.3933, Acc : 85.56, mIoU 70.62\n","Step [200/369], Loss: 0.2722, Acc : 85.48, mIoU 70.36\n","Step [250/369], Loss: 0.3019, Acc : 85.57, mIoU 70.15\n","Step [300/369], Loss: 0.4427, Acc : 85.57, mIoU 70.05\n","Step [350/369], Loss: 0.4756, Acc : 85.43, mIoU 69.75\n","Epoch time : 350.1s\n","Validation . . . \n","Step [50/118], Loss: 0.5120, Acc : 80.24, mIoU 56.37\n","Step [100/118], Loss: 0.6995, Acc : 80.47, mIoU 57.66\n","Epoch time : 40.3s\n","Loss 0.4819,  Acc 80.44,  IoU 57.4879\n","EPOCH 48/100\n","Step [50/369], Loss: 0.3860, Acc : 86.34, mIoU 71.48\n","Step [100/369], Loss: 0.4551, Acc : 86.01, mIoU 71.01\n","Step [150/369], Loss: 0.2825, Acc : 85.86, mIoU 70.92\n","Step [200/369], Loss: 0.4235, Acc : 85.79, mIoU 71.25\n","Step [250/369], Loss: 0.4242, Acc : 85.72, mIoU 70.92\n","Step [300/369], Loss: 0.2445, Acc : 85.70, mIoU 70.75\n","Step [350/369], Loss: 0.3631, Acc : 85.68, mIoU 70.54\n","Epoch time : 352.4s\n","Validation . . . \n","Step [50/118], Loss: 0.6112, Acc : 81.64, mIoU 58.45\n","Step [100/118], Loss: 0.7397, Acc : 80.86, mIoU 56.47\n","Epoch time : 39.6s\n","Loss 0.9482,  Acc 80.96,  IoU 56.4976\n","EPOCH 49/100\n","Step [50/369], Loss: 0.4099, Acc : 85.62, mIoU 70.47\n","Step [100/369], Loss: 0.3991, Acc : 86.10, mIoU 70.58\n","Step [150/369], Loss: 0.3315, Acc : 85.92, mIoU 70.05\n","Step [200/369], Loss: 0.4653, Acc : 85.73, mIoU 69.69\n","Step [250/369], Loss: 0.4918, Acc : 85.56, mIoU 69.53\n","Step [300/369], Loss: 0.6327, Acc : 85.41, mIoU 69.34\n","Step [350/369], Loss: 0.3843, Acc : 85.28, mIoU 69.34\n","Epoch time : 353.3s\n","Validation . . . \n","Step [50/118], Loss: 0.5364, Acc : 81.39, mIoU 57.79\n","Step [100/118], Loss: 0.5410, Acc : 81.23, mIoU 57.26\n","Epoch time : 40.0s\n","Loss 0.5145,  Acc 81.22,  IoU 57.0151\n","EPOCH 50/100\n","Step [50/369], Loss: 0.4142, Acc : 86.73, mIoU 72.19\n","Step [100/369], Loss: 0.3913, Acc : 86.46, mIoU 71.93\n","Step [150/369], Loss: 0.3898, Acc : 86.28, mIoU 72.00\n","Step [200/369], Loss: 0.4780, Acc : 86.12, mIoU 71.90\n","Step [250/369], Loss: 0.3507, Acc : 85.91, mIoU 71.62\n","Step [300/369], Loss: 0.4026, Acc : 85.94, mIoU 71.29\n","Step [350/369], Loss: 0.5172, Acc : 85.98, mIoU 71.19\n","Epoch time : 352.1s\n","Validation . . . \n","Step [50/118], Loss: 0.4429, Acc : 81.80, mIoU 58.21\n","Step [100/118], Loss: 0.3032, Acc : 81.59, mIoU 58.61\n","Epoch time : 39.1s\n","Loss 0.5488,  Acc 80.97,  IoU 57.9049\n","EPOCH 51/100\n","Step [50/369], Loss: 0.3876, Acc : 86.87, mIoU 72.93\n","Step [100/369], Loss: 0.3870, Acc : 86.55, mIoU 72.23\n","Step [150/369], Loss: 0.4077, Acc : 86.24, mIoU 72.10\n","Step [200/369], Loss: 0.3725, Acc : 86.05, mIoU 71.56\n","Step [250/369], Loss: 0.3705, Acc : 85.93, mIoU 71.40\n","Step [300/369], Loss: 0.4936, Acc : 86.08, mIoU 71.60\n","Step [350/369], Loss: 0.3749, Acc : 86.09, mIoU 71.28\n","Epoch time : 348.9s\n","Validation . . . \n","Step [50/118], Loss: 0.5076, Acc : 80.61, mIoU 57.16\n","Step [100/118], Loss: 0.4514, Acc : 80.33, mIoU 56.31\n","Epoch time : 39.2s\n","Loss 0.6178,  Acc 80.11,  IoU 56.3493\n","EPOCH 52/100\n","Step [50/369], Loss: 0.5847, Acc : 86.17, mIoU 69.86\n","Step [100/369], Loss: 0.5579, Acc : 86.43, mIoU 71.27\n","Step [150/369], Loss: 0.5919, Acc : 86.36, mIoU 71.24\n","Step [200/369], Loss: 0.3108, Acc : 86.25, mIoU 71.58\n","Step [250/369], Loss: 0.3289, Acc : 86.14, mIoU 71.30\n","Step [300/369], Loss: 0.3149, Acc : 86.17, mIoU 71.82\n","Step [350/369], Loss: 0.4843, Acc : 86.09, mIoU 71.77\n","Epoch time : 351.8s\n","Validation . . . \n","Step [50/118], Loss: 0.8457, Acc : 80.51, mIoU 55.18\n","Step [100/118], Loss: 0.5755, Acc : 80.69, mIoU 56.15\n","Epoch time : 40.3s\n","Loss 0.4065,  Acc 80.67,  IoU 55.9635\n","EPOCH 53/100\n","Step [50/369], Loss: 0.3154, Acc : 86.64, mIoU 72.66\n","Step [100/369], Loss: 0.3335, Acc : 87.03, mIoU 74.07\n","Step [150/369], Loss: 0.3847, Acc : 86.62, mIoU 73.51\n","Step [200/369], Loss: 0.3753, Acc : 86.20, mIoU 72.75\n","Step [250/369], Loss: 0.4083, Acc : 86.24, mIoU 72.77\n","Step [300/369], Loss: 0.4290, Acc : 86.17, mIoU 72.15\n","Step [350/369], Loss: 0.3344, Acc : 86.02, mIoU 71.72\n","Epoch time : 351.5s\n","Validation . . . \n","Step [50/118], Loss: 0.4179, Acc : 79.64, mIoU 56.14\n","Step [100/118], Loss: 0.6211, Acc : 80.57, mIoU 56.52\n","Epoch time : 39.9s\n","Loss 0.7599,  Acc 80.54,  IoU 56.2276\n","EPOCH 54/100\n","Step [50/369], Loss: 0.3264, Acc : 86.30, mIoU 72.12\n","Step [100/369], Loss: 0.4589, Acc : 86.25, mIoU 72.70\n","Step [150/369], Loss: 0.2802, Acc : 86.37, mIoU 72.99\n","Step [200/369], Loss: 0.3024, Acc : 86.29, mIoU 72.97\n","Step [250/369], Loss: 0.3872, Acc : 86.34, mIoU 72.99\n","Step [300/369], Loss: 0.4026, Acc : 86.29, mIoU 72.79\n","Step [350/369], Loss: 0.3907, Acc : 86.39, mIoU 72.73\n","Epoch time : 348.0s\n","Validation . . . \n","Step [50/118], Loss: 0.4514, Acc : 79.02, mIoU 54.12\n","Step [100/118], Loss: 0.3026, Acc : 79.65, mIoU 53.45\n","Epoch time : 39.6s\n","Loss 0.6277,  Acc 79.74,  IoU 53.7366\n","EPOCH 55/100\n","Step [50/369], Loss: 0.2377, Acc : 85.33, mIoU 71.44\n","Step [100/369], Loss: 0.2735, Acc : 85.48, mIoU 70.96\n","Step [150/369], Loss: 0.2215, Acc : 85.88, mIoU 71.23\n","Step [200/369], Loss: 0.3086, Acc : 86.05, mIoU 71.49\n","Step [250/369], Loss: 0.3575, Acc : 85.96, mIoU 71.55\n","Step [300/369], Loss: 0.3948, Acc : 86.00, mIoU 71.53\n","Step [350/369], Loss: 0.3790, Acc : 85.93, mIoU 71.64\n","Epoch time : 350.2s\n","Validation . . . \n","Step [50/118], Loss: 0.3464, Acc : 80.44, mIoU 54.30\n","Step [100/118], Loss: 0.4362, Acc : 81.22, mIoU 56.10\n","Epoch time : 39.6s\n","Loss 0.7633,  Acc 81.20,  IoU 55.7631\n","EPOCH 56/100\n","Step [50/369], Loss: 0.2749, Acc : 87.34, mIoU 76.09\n","Step [100/369], Loss: 0.4639, Acc : 87.05, mIoU 75.95\n","Step [150/369], Loss: 0.2635, Acc : 87.10, mIoU 75.60\n","Step [200/369], Loss: 0.3844, Acc : 86.97, mIoU 75.28\n","Step [250/369], Loss: 0.4184, Acc : 87.06, mIoU 75.08\n","Step [300/369], Loss: 0.3022, Acc : 86.96, mIoU 74.77\n","Step [350/369], Loss: 0.3369, Acc : 87.01, mIoU 74.53\n","Epoch time : 353.3s\n","Validation . . . \n","Step [50/118], Loss: 1.0349, Acc : 81.33, mIoU 57.21\n","Step [100/118], Loss: 0.9181, Acc : 80.96, mIoU 56.22\n","Epoch time : 39.8s\n","Loss 0.3461,  Acc 80.89,  IoU 55.9479\n","EPOCH 57/100\n","Step [50/369], Loss: 0.3354, Acc : 87.78, mIoU 76.75\n","Step [100/369], Loss: 0.3438, Acc : 87.11, mIoU 75.23\n","Step [150/369], Loss: 0.4127, Acc : 87.24, mIoU 74.98\n","Step [200/369], Loss: 0.3722, Acc : 87.03, mIoU 74.66\n","Step [250/369], Loss: 0.4265, Acc : 86.71, mIoU 73.83\n","Step [300/369], Loss: 0.4214, Acc : 86.68, mIoU 73.69\n","Step [350/369], Loss: 0.4666, Acc : 86.69, mIoU 73.56\n","Epoch time : 347.3s\n","Validation . . . \n","Step [50/118], Loss: 0.4678, Acc : 82.16, mIoU 59.15\n","Step [100/118], Loss: 0.5164, Acc : 81.29, mIoU 58.51\n","Epoch time : 39.7s\n","Loss 0.3330,  Acc 80.95,  IoU 57.9353\n","EPOCH 58/100\n","Step [50/369], Loss: 0.4305, Acc : 86.47, mIoU 71.79\n","Step [100/369], Loss: 0.3452, Acc : 87.00, mIoU 74.10\n","Step [150/369], Loss: 0.4038, Acc : 87.07, mIoU 74.11\n","Step [200/369], Loss: 0.5107, Acc : 87.09, mIoU 74.27\n","Step [250/369], Loss: 0.3666, Acc : 86.95, mIoU 73.93\n","Step [300/369], Loss: 0.2508, Acc : 86.83, mIoU 73.73\n","Step [350/369], Loss: 0.3538, Acc : 86.76, mIoU 73.65\n","Epoch time : 352.4s\n","Validation . . . \n","Step [50/118], Loss: 0.4941, Acc : 79.92, mIoU 54.90\n","Step [100/118], Loss: 0.3247, Acc : 80.84, mIoU 56.10\n","Epoch time : 39.4s\n","Loss 0.5722,  Acc 80.99,  IoU 56.3000\n","EPOCH 59/100\n","Step [50/369], Loss: 0.3216, Acc : 87.23, mIoU 75.42\n","Step [100/369], Loss: 0.3053, Acc : 87.35, mIoU 74.84\n","Step [150/369], Loss: 0.3551, Acc : 87.38, mIoU 74.59\n","Step [200/369], Loss: 0.2604, Acc : 87.22, mIoU 74.17\n","Step [250/369], Loss: 0.4357, Acc : 86.98, mIoU 74.03\n","Step [300/369], Loss: 0.2378, Acc : 86.95, mIoU 73.96\n","Step [350/369], Loss: 0.3915, Acc : 86.95, mIoU 74.03\n","Epoch time : 349.3s\n","Validation . . . \n","Step [50/118], Loss: 0.7249, Acc : 80.99, mIoU 55.82\n","Step [100/118], Loss: 0.4830, Acc : 81.32, mIoU 56.01\n","Epoch time : 39.0s\n","Loss 0.6049,  Acc 81.17,  IoU 56.1784\n","EPOCH 60/100\n","Step [50/369], Loss: 0.3009, Acc : 87.72, mIoU 74.61\n","Step [100/369], Loss: 0.3187, Acc : 87.63, mIoU 75.36\n","Step [150/369], Loss: 0.3079, Acc : 87.53, mIoU 75.48\n","Step [200/369], Loss: 0.3334, Acc : 87.59, mIoU 75.70\n","Step [250/369], Loss: 0.2838, Acc : 87.48, mIoU 75.34\n","Step [300/369], Loss: 0.2223, Acc : 87.33, mIoU 74.96\n","Step [350/369], Loss: 0.2848, Acc : 87.29, mIoU 74.87\n","Epoch time : 350.1s\n","Validation . . . \n","Step [50/118], Loss: 1.0027, Acc : 81.70, mIoU 58.76\n","Step [100/118], Loss: 0.5589, Acc : 81.03, mIoU 56.75\n","Epoch time : 40.3s\n","Loss 0.4897,  Acc 80.97,  IoU 56.6667\n","EPOCH 61/100\n","Step [50/369], Loss: 0.1946, Acc : 87.16, mIoU 74.89\n","Step [100/369], Loss: 0.3554, Acc : 86.60, mIoU 73.82\n","Step [150/369], Loss: 0.1794, Acc : 86.72, mIoU 73.88\n","Step [200/369], Loss: 0.4203, Acc : 86.89, mIoU 73.99\n","Step [250/369], Loss: 0.2442, Acc : 87.04, mIoU 74.12\n","Step [300/369], Loss: 0.2917, Acc : 87.03, mIoU 73.85\n","Step [350/369], Loss: 0.4711, Acc : 87.01, mIoU 74.03\n","Epoch time : 351.0s\n","Validation . . . \n","Step [50/118], Loss: 0.2861, Acc : 80.56, mIoU 57.13\n","Step [100/118], Loss: 0.7583, Acc : 81.01, mIoU 56.99\n","Epoch time : 39.4s\n","Loss 0.4657,  Acc 81.11,  IoU 57.1167\n","EPOCH 62/100\n","Step [50/369], Loss: 0.2906, Acc : 87.84, mIoU 76.02\n","Step [100/369], Loss: 0.4487, Acc : 87.88, mIoU 75.50\n","Step [150/369], Loss: 0.2712, Acc : 87.61, mIoU 75.03\n","Step [200/369], Loss: 0.2819, Acc : 87.51, mIoU 74.71\n","Step [250/369], Loss: 0.3058, Acc : 87.43, mIoU 74.85\n","Step [300/369], Loss: 0.3474, Acc : 87.32, mIoU 74.52\n","Step [350/369], Loss: 0.3200, Acc : 87.31, mIoU 74.59\n","Epoch time : 345.7s\n","Validation . . . \n","Step [50/118], Loss: 0.7132, Acc : 80.14, mIoU 55.26\n","Step [100/118], Loss: 0.3884, Acc : 80.51, mIoU 55.82\n","Epoch time : 39.3s\n","Loss 0.7811,  Acc 80.65,  IoU 56.0886\n","EPOCH 63/100\n","Step [50/369], Loss: 0.2773, Acc : 88.52, mIoU 76.60\n","Step [100/369], Loss: 0.4372, Acc : 88.44, mIoU 77.18\n","Step [150/369], Loss: 0.2772, Acc : 87.94, mIoU 76.58\n","Step [200/369], Loss: 0.2439, Acc : 87.85, mIoU 76.08\n","Step [250/369], Loss: 0.2223, Acc : 87.62, mIoU 75.94\n","Step [300/369], Loss: 0.3846, Acc : 87.60, mIoU 75.80\n","Step [350/369], Loss: 0.2327, Acc : 87.49, mIoU 75.62\n","Epoch time : 351.3s\n","Validation . . . \n","Step [50/118], Loss: 0.4398, Acc : 81.21, mIoU 55.45\n","Step [100/118], Loss: 0.4698, Acc : 81.54, mIoU 57.08\n","Epoch time : 41.1s\n","Loss 1.2162,  Acc 80.96,  IoU 56.3486\n","EPOCH 64/100\n","Step [50/369], Loss: 0.4475, Acc : 87.15, mIoU 75.94\n","Step [100/369], Loss: 0.2729, Acc : 87.31, mIoU 76.23\n","Step [150/369], Loss: 0.3587, Acc : 87.73, mIoU 75.89\n","Step [200/369], Loss: 0.3413, Acc : 87.63, mIoU 75.76\n","Step [250/369], Loss: 0.3426, Acc : 87.70, mIoU 75.69\n","Step [300/369], Loss: 0.2980, Acc : 87.72, mIoU 75.47\n","Step [350/369], Loss: 0.3325, Acc : 87.71, mIoU 75.51\n","Epoch time : 351.9s\n","Validation . . . \n","Step [50/118], Loss: 1.1695, Acc : 80.59, mIoU 56.16\n","Step [100/118], Loss: 0.4711, Acc : 80.39, mIoU 56.16\n","Epoch time : 39.9s\n","Loss 0.5896,  Acc 80.45,  IoU 55.8625\n","EPOCH 65/100\n","Step [50/369], Loss: 0.2337, Acc : 88.34, mIoU 75.48\n","Step [100/369], Loss: 0.3971, Acc : 88.19, mIoU 75.95\n","Step [150/369], Loss: 0.3880, Acc : 88.09, mIoU 76.23\n","Step [200/369], Loss: 0.3035, Acc : 88.07, mIoU 76.66\n","Step [250/369], Loss: 0.4803, Acc : 88.03, mIoU 76.61\n","Step [300/369], Loss: 0.4221, Acc : 87.89, mIoU 76.31\n","Step [350/369], Loss: 0.3743, Acc : 87.85, mIoU 76.39\n","Epoch time : 349.6s\n","Validation . . . \n","Step [50/118], Loss: 0.5686, Acc : 80.62, mIoU 56.14\n","Step [100/118], Loss: 0.3872, Acc : 80.96, mIoU 56.82\n","Epoch time : 40.1s\n","Loss 0.6823,  Acc 80.92,  IoU 57.3810\n","EPOCH 66/100\n","Step [50/369], Loss: 0.2507, Acc : 88.83, mIoU 78.86\n","Step [100/369], Loss: 0.2949, Acc : 88.41, mIoU 77.76\n","Step [150/369], Loss: 0.3066, Acc : 88.17, mIoU 77.26\n","Step [200/369], Loss: 0.3599, Acc : 87.89, mIoU 77.16\n","Step [250/369], Loss: 0.2161, Acc : 87.65, mIoU 76.65\n","Step [300/369], Loss: 0.3332, Acc : 87.69, mIoU 76.42\n","Step [350/369], Loss: 0.2548, Acc : 87.75, mIoU 76.37\n","Epoch time : 348.1s\n","Validation . . . \n","Step [50/118], Loss: 0.5045, Acc : 80.97, mIoU 55.61\n","Step [100/118], Loss: 0.9591, Acc : 80.83, mIoU 57.04\n","Epoch time : 39.6s\n","Loss 0.5050,  Acc 80.70,  IoU 56.9294\n","EPOCH 67/100\n","Step [50/369], Loss: 0.2826, Acc : 88.05, mIoU 78.34\n","Step [100/369], Loss: 0.3154, Acc : 87.88, mIoU 78.09\n","Step [150/369], Loss: 0.3148, Acc : 88.05, mIoU 77.42\n","Step [200/369], Loss: 0.2587, Acc : 88.06, mIoU 77.17\n","Step [250/369], Loss: 0.2958, Acc : 88.03, mIoU 77.30\n","Step [300/369], Loss: 0.2955, Acc : 87.94, mIoU 77.10\n","Step [350/369], Loss: 0.3248, Acc : 87.86, mIoU 76.76\n","Epoch time : 350.5s\n","Validation . . . \n","Step [50/118], Loss: 0.7105, Acc : 81.15, mIoU 54.73\n","Step [100/118], Loss: 0.7389, Acc : 81.05, mIoU 56.05\n","Epoch time : 39.3s\n","Loss 0.7837,  Acc 80.69,  IoU 55.7587\n","EPOCH 68/100\n","Step [50/369], Loss: 0.2751, Acc : 88.17, mIoU 74.74\n","Step [100/369], Loss: 0.3997, Acc : 87.90, mIoU 74.67\n","Step [150/369], Loss: 0.3720, Acc : 87.77, mIoU 75.31\n","Step [200/369], Loss: 0.3554, Acc : 87.73, mIoU 75.43\n","Step [250/369], Loss: 0.2991, Acc : 87.72, mIoU 75.57\n","Step [300/369], Loss: 0.1743, Acc : 87.83, mIoU 75.84\n","Step [350/369], Loss: 0.3151, Acc : 87.89, mIoU 76.02\n","Epoch time : 351.2s\n","Validation . . . \n","Step [50/118], Loss: 0.5593, Acc : 80.81, mIoU 54.57\n","Step [100/118], Loss: 0.6463, Acc : 80.38, mIoU 56.58\n","Epoch time : 40.5s\n","Loss 0.5439,  Acc 80.00,  IoU 56.6509\n","EPOCH 69/100\n","Step [50/369], Loss: 0.3006, Acc : 87.81, mIoU 78.07\n","Step [100/369], Loss: 0.2430, Acc : 88.00, mIoU 77.66\n","Step [150/369], Loss: 0.4006, Acc : 88.18, mIoU 77.86\n","Step [200/369], Loss: 0.2390, Acc : 88.09, mIoU 77.53\n","Step [250/369], Loss: 0.3226, Acc : 88.09, mIoU 77.25\n","Step [300/369], Loss: 0.1945, Acc : 88.13, mIoU 77.19\n","Step [350/369], Loss: 0.3621, Acc : 88.13, mIoU 76.89\n","Epoch time : 354.6s\n","Validation . . . \n","Step [50/118], Loss: 0.5089, Acc : 79.63, mIoU 56.66\n","Step [100/118], Loss: 0.8356, Acc : 80.00, mIoU 56.96\n","Epoch time : 39.6s\n","Loss 0.8624,  Acc 80.37,  IoU 57.0493\n","EPOCH 70/100\n","Step [50/369], Loss: 0.3632, Acc : 88.46, mIoU 78.69\n","Step [100/369], Loss: 0.3383, Acc : 88.49, mIoU 77.78\n","Step [150/369], Loss: 0.2956, Acc : 88.44, mIoU 77.54\n","Step [200/369], Loss: 0.3556, Acc : 88.35, mIoU 77.53\n","Step [250/369], Loss: 0.2729, Acc : 88.42, mIoU 77.93\n","Step [300/369], Loss: 0.3323, Acc : 88.44, mIoU 77.66\n","Step [350/369], Loss: 0.3136, Acc : 88.33, mIoU 77.36\n","Epoch time : 352.4s\n","Validation . . . \n","Step [50/118], Loss: 0.5150, Acc : 81.62, mIoU 57.14\n","Step [100/118], Loss: 0.5625, Acc : 81.09, mIoU 56.34\n","Epoch time : 40.0s\n","Loss 0.6002,  Acc 81.01,  IoU 56.0728\n","EPOCH 71/100\n","Step [50/369], Loss: 0.2843, Acc : 89.11, mIoU 80.01\n","Step [100/369], Loss: 0.4802, Acc : 89.00, mIoU 79.64\n","Step [150/369], Loss: 0.3046, Acc : 88.83, mIoU 78.55\n","Step [200/369], Loss: 0.3306, Acc : 88.74, mIoU 78.33\n","Step [250/369], Loss: 0.3581, Acc : 88.58, mIoU 78.20\n","Step [300/369], Loss: 0.3317, Acc : 88.48, mIoU 77.87\n","Step [350/369], Loss: 0.3166, Acc : 88.44, mIoU 77.69\n","Epoch time : 352.7s\n","Validation . . . \n","Step [50/118], Loss: 0.6062, Acc : 81.82, mIoU 57.40\n","Step [100/118], Loss: 0.3924, Acc : 81.08, mIoU 55.91\n","Epoch time : 40.2s\n","Loss 0.5361,  Acc 81.10,  IoU 55.7482\n","EPOCH 72/100\n","Step [50/369], Loss: 0.3493, Acc : 89.14, mIoU 78.72\n","Step [100/369], Loss: 0.4218, Acc : 88.73, mIoU 78.40\n","Step [150/369], Loss: 0.2487, Acc : 88.64, mIoU 78.09\n","Step [200/369], Loss: 0.2373, Acc : 88.63, mIoU 78.19\n","Step [250/369], Loss: 0.2304, Acc : 88.73, mIoU 78.12\n","Step [300/369], Loss: 0.3576, Acc : 88.68, mIoU 78.00\n","Step [350/369], Loss: 0.3980, Acc : 88.60, mIoU 77.83\n","Epoch time : 350.3s\n","Validation . . . \n","Step [50/118], Loss: 0.6060, Acc : 81.06, mIoU 56.08\n","Step [100/118], Loss: 0.4733, Acc : 80.74, mIoU 57.43\n","Epoch time : 40.0s\n","Loss 0.7230,  Acc 80.75,  IoU 57.1720\n","EPOCH 73/100\n","Step [50/369], Loss: 0.2604, Acc : 88.55, mIoU 78.99\n","Step [100/369], Loss: 0.3396, Acc : 88.51, mIoU 78.83\n","Step [150/369], Loss: 0.2567, Acc : 88.49, mIoU 78.59\n","Step [200/369], Loss: 0.3818, Acc : 88.45, mIoU 78.48\n","Step [250/369], Loss: 0.2204, Acc : 88.47, mIoU 78.01\n","Step [300/369], Loss: 0.3047, Acc : 88.45, mIoU 77.75\n","Step [350/369], Loss: 0.3102, Acc : 88.48, mIoU 77.95\n","Epoch time : 351.2s\n","Validation . . . \n","Step [50/118], Loss: 0.7294, Acc : 80.68, mIoU 54.96\n","Step [100/118], Loss: 1.0037, Acc : 80.99, mIoU 56.18\n","Epoch time : 39.9s\n","Loss 0.9745,  Acc 80.68,  IoU 55.8236\n","EPOCH 74/100\n","Step [50/369], Loss: 0.6077, Acc : 87.46, mIoU 72.54\n","Step [100/369], Loss: 0.2559, Acc : 87.99, mIoU 75.82\n","Step [150/369], Loss: 0.2919, Acc : 88.13, mIoU 76.73\n","Step [200/369], Loss: 0.3601, Acc : 88.34, mIoU 76.93\n","Step [250/369], Loss: 0.3987, Acc : 88.30, mIoU 76.97\n","Step [300/369], Loss: 0.2670, Acc : 88.50, mIoU 77.49\n","Step [350/369], Loss: 0.3929, Acc : 88.58, mIoU 77.60\n","Epoch time : 354.1s\n","Validation . . . \n","Step [50/118], Loss: 0.4434, Acc : 79.52, mIoU 57.46\n","Step [100/118], Loss: 0.6371, Acc : 80.55, mIoU 57.59\n","Epoch time : 40.3s\n","Loss 0.5912,  Acc 80.61,  IoU 57.3773\n","EPOCH 75/100\n","Step [50/369], Loss: 0.2342, Acc : 89.58, mIoU 79.46\n","Step [100/369], Loss: 0.3497, Acc : 89.37, mIoU 78.91\n","Step [150/369], Loss: 0.3688, Acc : 88.96, mIoU 78.47\n","Step [200/369], Loss: 0.2592, Acc : 88.72, mIoU 78.45\n","Step [250/369], Loss: 0.3133, Acc : 88.92, mIoU 78.66\n","Step [300/369], Loss: 0.3917, Acc : 88.80, mIoU 78.48\n","Step [350/369], Loss: 0.2569, Acc : 88.68, mIoU 78.39\n","Epoch time : 349.2s\n","Validation . . . \n","Step [50/118], Loss: 0.6273, Acc : 80.60, mIoU 55.97\n","Step [100/118], Loss: 0.5016, Acc : 80.50, mIoU 56.64\n","Epoch time : 41.1s\n","Loss 0.8882,  Acc 80.43,  IoU 56.3645\n","EPOCH 76/100\n","Step [50/369], Loss: 0.3767, Acc : 89.10, mIoU 78.80\n","Step [100/369], Loss: 0.3020, Acc : 89.05, mIoU 77.67\n","Step [150/369], Loss: 0.2708, Acc : 88.76, mIoU 77.86\n","Step [200/369], Loss: 0.2652, Acc : 88.72, mIoU 77.85\n","Step [250/369], Loss: 0.4193, Acc : 88.58, mIoU 77.93\n","Step [300/369], Loss: 0.2220, Acc : 88.50, mIoU 78.07\n","Step [350/369], Loss: 0.2525, Acc : 88.56, mIoU 78.09\n","Epoch time : 354.0s\n","Validation . . . \n","Step [50/118], Loss: 0.5548, Acc : 80.74, mIoU 55.09\n","Step [100/118], Loss: 0.4608, Acc : 80.72, mIoU 56.49\n","Epoch time : 39.8s\n","Loss 0.4585,  Acc 80.68,  IoU 56.2846\n","EPOCH 77/100\n","Step [50/369], Loss: 0.4403, Acc : 89.11, mIoU 80.22\n","Step [100/369], Loss: 0.3636, Acc : 89.28, mIoU 80.69\n","Step [150/369], Loss: 0.3483, Acc : 89.21, mIoU 80.45\n","Step [200/369], Loss: 0.2374, Acc : 89.14, mIoU 80.02\n","Step [250/369], Loss: 0.3100, Acc : 89.10, mIoU 79.78\n","Step [300/369], Loss: 0.3054, Acc : 89.09, mIoU 79.36\n","Step [350/369], Loss: 0.2376, Acc : 89.10, mIoU 79.26\n","Epoch time : 349.7s\n","Validation . . . \n","Step [50/118], Loss: 0.6386, Acc : 80.92, mIoU 57.10\n","Step [100/118], Loss: 0.5659, Acc : 81.22, mIoU 57.57\n","Epoch time : 40.6s\n","Loss 0.5466,  Acc 80.89,  IoU 57.2980\n","EPOCH 78/100\n","Step [50/369], Loss: 0.4487, Acc : 88.96, mIoU 78.84\n","Step [100/369], Loss: 0.3096, Acc : 88.86, mIoU 78.85\n","Step [150/369], Loss: 0.2961, Acc : 88.87, mIoU 78.62\n","Step [200/369], Loss: 0.2541, Acc : 88.91, mIoU 79.01\n","Step [250/369], Loss: 0.3022, Acc : 88.90, mIoU 78.92\n","Step [300/369], Loss: 0.2685, Acc : 89.07, mIoU 79.03\n","Step [350/369], Loss: 0.3032, Acc : 89.17, mIoU 79.16\n","Epoch time : 354.3s\n","Validation . . . \n","Step [50/118], Loss: 0.6434, Acc : 80.57, mIoU 56.43\n","Step [100/118], Loss: 0.3354, Acc : 80.66, mIoU 56.73\n","Epoch time : 39.1s\n","Loss 0.5759,  Acc 81.00,  IoU 57.5773\n","EPOCH 79/100\n","Step [50/369], Loss: 0.3715, Acc : 88.81, mIoU 79.06\n","Step [100/369], Loss: 0.3402, Acc : 88.76, mIoU 78.69\n","Step [150/369], Loss: 0.2570, Acc : 88.83, mIoU 78.08\n","Step [200/369], Loss: 0.3117, Acc : 88.82, mIoU 77.82\n","Step [250/369], Loss: 0.3246, Acc : 88.70, mIoU 77.94\n","Step [300/369], Loss: 0.4162, Acc : 88.69, mIoU 78.01\n","Step [350/369], Loss: 0.2251, Acc : 88.69, mIoU 77.90\n","Epoch time : 351.3s\n","Validation . . . \n","Step [50/118], Loss: 0.3513, Acc : 79.80, mIoU 56.24\n","Step [100/118], Loss: 0.4323, Acc : 79.93, mIoU 56.12\n","Epoch time : 40.0s\n","Loss 1.1464,  Acc 79.89,  IoU 56.2725\n","EPOCH 80/100\n","Step [50/369], Loss: 0.2709, Acc : 89.73, mIoU 80.74\n","Step [100/369], Loss: 0.2902, Acc : 89.43, mIoU 80.93\n","Step [150/369], Loss: 0.2311, Acc : 89.40, mIoU 80.86\n","Step [200/369], Loss: 0.2862, Acc : 89.31, mIoU 80.36\n","Step [250/369], Loss: 0.3106, Acc : 89.45, mIoU 80.42\n","Step [300/369], Loss: 0.4604, Acc : 89.40, mIoU 80.32\n","Step [350/369], Loss: 0.3167, Acc : 89.41, mIoU 80.07\n","Epoch time : 355.8s\n","Validation . . . \n","Step [50/118], Loss: 0.3530, Acc : 82.01, mIoU 57.41\n","Step [100/118], Loss: 0.7121, Acc : 80.68, mIoU 56.33\n","Epoch time : 40.7s\n","Loss 0.6189,  Acc 80.82,  IoU 56.7224\n","EPOCH 81/100\n","Step [50/369], Loss: 0.2817, Acc : 89.52, mIoU 79.05\n","Step [100/369], Loss: 0.1752, Acc : 89.91, mIoU 80.45\n","Step [150/369], Loss: 0.2413, Acc : 89.69, mIoU 80.39\n","Step [200/369], Loss: 0.2754, Acc : 89.50, mIoU 80.13\n","Step [250/369], Loss: 0.3830, Acc : 89.56, mIoU 80.33\n","Step [300/369], Loss: 0.2571, Acc : 89.56, mIoU 80.27\n","Step [350/369], Loss: 0.2613, Acc : 89.54, mIoU 80.27\n","Epoch time : 352.0s\n","Validation . . . \n","Step [50/118], Loss: 0.3497, Acc : 81.89, mIoU 57.34\n","Step [100/118], Loss: 1.0332, Acc : 80.65, mIoU 55.06\n","Epoch time : 40.9s\n","Loss 1.0367,  Acc 80.43,  IoU 54.9275\n","EPOCH 82/100\n","Step [50/369], Loss: 0.3247, Acc : 90.02, mIoU 80.44\n","Step [100/369], Loss: 0.2852, Acc : 90.00, mIoU 81.07\n","Step [150/369], Loss: 0.1652, Acc : 89.72, mIoU 80.56\n","Step [200/369], Loss: 0.2750, Acc : 89.70, mIoU 80.58\n","Step [250/369], Loss: 0.2312, Acc : 89.53, mIoU 80.26\n","Step [300/369], Loss: 0.2419, Acc : 89.53, mIoU 80.16\n","Step [350/369], Loss: 0.2837, Acc : 89.43, mIoU 80.05\n","Epoch time : 356.3s\n","Validation . . . \n","Step [50/118], Loss: 0.6668, Acc : 81.02, mIoU 58.30\n","Step [100/118], Loss: 0.6429, Acc : 79.98, mIoU 56.45\n","Epoch time : 41.0s\n","Loss 0.5208,  Acc 79.93,  IoU 56.1404\n","EPOCH 83/100\n","Step [50/369], Loss: 0.2665, Acc : 90.21, mIoU 81.21\n","Step [100/369], Loss: 0.2902, Acc : 89.69, mIoU 80.87\n","Step [150/369], Loss: 0.2770, Acc : 89.64, mIoU 80.65\n","Step [200/369], Loss: 0.2446, Acc : 89.80, mIoU 80.86\n","Step [250/369], Loss: 0.2999, Acc : 89.75, mIoU 80.70\n","Step [300/369], Loss: 0.1938, Acc : 89.69, mIoU 80.64\n","Step [350/369], Loss: 0.2706, Acc : 89.71, mIoU 80.50\n","Epoch time : 349.2s\n","Validation . . . \n","Step [50/118], Loss: 0.5317, Acc : 79.96, mIoU 54.25\n","Step [100/118], Loss: 0.8994, Acc : 80.49, mIoU 54.90\n","Epoch time : 40.7s\n","Loss 1.0098,  Acc 80.59,  IoU 55.0872\n","EPOCH 84/100\n","Step [50/369], Loss: 0.3988, Acc : 89.71, mIoU 80.53\n","Step [100/369], Loss: 0.2714, Acc : 89.27, mIoU 80.00\n","Step [150/369], Loss: 0.2140, Acc : 89.39, mIoU 79.97\n","Step [200/369], Loss: 0.2851, Acc : 89.26, mIoU 79.69\n","Step [250/369], Loss: 0.3205, Acc : 89.26, mIoU 79.65\n","Step [300/369], Loss: 0.2448, Acc : 89.22, mIoU 79.53\n","Step [350/369], Loss: 0.2773, Acc : 89.25, mIoU 79.32\n","Epoch time : 356.1s\n","Validation . . . \n","Step [50/118], Loss: 0.8080, Acc : 80.54, mIoU 57.99\n","Step [100/118], Loss: 0.5336, Acc : 80.74, mIoU 58.24\n","Epoch time : 40.3s\n","Loss 0.7318,  Acc 80.60,  IoU 57.7143\n","EPOCH 85/100\n","Step [50/369], Loss: 0.2617, Acc : 90.23, mIoU 79.96\n","Step [100/369], Loss: 0.3396, Acc : 90.01, mIoU 80.16\n","Step [150/369], Loss: 0.2541, Acc : 89.79, mIoU 80.33\n","Step [200/369], Loss: 0.1993, Acc : 89.66, mIoU 80.65\n","Step [250/369], Loss: 0.2857, Acc : 89.71, mIoU 80.83\n","Step [300/369], Loss: 0.2415, Acc : 89.65, mIoU 80.62\n","Step [350/369], Loss: 0.1951, Acc : 89.61, mIoU 80.53\n","Epoch time : 350.1s\n","Validation . . . \n","Step [50/118], Loss: 0.8206, Acc : 79.36, mIoU 57.36\n","Step [100/118], Loss: 0.5472, Acc : 79.45, mIoU 56.09\n","Epoch time : 40.5s\n","Loss 1.2139,  Acc 79.51,  IoU 55.8861\n","EPOCH 86/100\n","Step [50/369], Loss: 0.2106, Acc : 90.40, mIoU 82.27\n","Step [100/369], Loss: 0.2143, Acc : 90.53, mIoU 81.89\n","Step [150/369], Loss: 0.1779, Acc : 90.42, mIoU 81.50\n","Step [200/369], Loss: 0.3449, Acc : 90.15, mIoU 81.09\n","Step [250/369], Loss: 0.2716, Acc : 89.98, mIoU 81.00\n","Step [300/369], Loss: 0.2444, Acc : 89.84, mIoU 80.80\n","Step [350/369], Loss: 0.2828, Acc : 89.92, mIoU 80.93\n","Epoch time : 354.4s\n","Validation . . . \n","Step [50/118], Loss: 0.6053, Acc : 80.41, mIoU 54.82\n","Step [100/118], Loss: 0.6824, Acc : 80.92, mIoU 55.85\n","Epoch time : 41.0s\n","Loss 0.7304,  Acc 80.78,  IoU 55.7714\n","EPOCH 87/100\n","Step [50/369], Loss: 0.2212, Acc : 89.58, mIoU 79.81\n","Step [100/369], Loss: 0.2155, Acc : 89.98, mIoU 80.77\n","Step [150/369], Loss: 0.2728, Acc : 90.16, mIoU 80.97\n","Step [200/369], Loss: 0.3026, Acc : 90.14, mIoU 81.01\n","Step [250/369], Loss: 0.3548, Acc : 90.17, mIoU 81.16\n","Step [300/369], Loss: 0.1994, Acc : 90.05, mIoU 81.04\n","Step [350/369], Loss: 0.2186, Acc : 90.02, mIoU 80.85\n","Epoch time : 350.6s\n","Validation . . . \n","Step [50/118], Loss: 0.6653, Acc : 80.56, mIoU 57.18\n","Step [100/118], Loss: 0.6002, Acc : 80.38, mIoU 56.60\n","Epoch time : 40.5s\n","Loss 0.5019,  Acc 80.31,  IoU 56.8629\n","EPOCH 88/100\n","Step [50/369], Loss: 0.2840, Acc : 90.08, mIoU 81.38\n","Step [100/369], Loss: 0.1988, Acc : 90.24, mIoU 81.25\n","Step [150/369], Loss: 0.3321, Acc : 90.32, mIoU 81.36\n","Step [200/369], Loss: 0.2686, Acc : 90.18, mIoU 81.37\n","Step [250/369], Loss: 0.2971, Acc : 90.14, mIoU 81.39\n","Step [300/369], Loss: 0.2536, Acc : 90.09, mIoU 81.45\n","Step [350/369], Loss: 0.3000, Acc : 90.10, mIoU 81.25\n","Epoch time : 352.8s\n","Validation . . . \n","Step [50/118], Loss: 0.6343, Acc : 80.82, mIoU 56.74\n","Step [100/118], Loss: 0.4540, Acc : 80.53, mIoU 55.99\n","Epoch time : 40.7s\n","Loss 0.7098,  Acc 80.44,  IoU 55.9426\n","EPOCH 89/100\n","Step [50/369], Loss: 0.1742, Acc : 90.71, mIoU 82.68\n","Step [100/369], Loss: 0.2777, Acc : 90.38, mIoU 82.22\n","Step [150/369], Loss: 0.2345, Acc : 90.29, mIoU 81.66\n","Step [200/369], Loss: 0.2172, Acc : 90.19, mIoU 81.68\n","Step [250/369], Loss: 0.1394, Acc : 90.05, mIoU 81.36\n","Step [300/369], Loss: 0.3154, Acc : 90.04, mIoU 81.39\n","Step [350/369], Loss: 0.2865, Acc : 89.92, mIoU 81.01\n","Epoch time : 348.7s\n","Validation . . . \n","Step [50/118], Loss: 0.7789, Acc : 80.51, mIoU 53.61\n","Step [100/118], Loss: 0.7015, Acc : 80.62, mIoU 54.24\n","Epoch time : 39.7s\n","Loss 0.6885,  Acc 80.49,  IoU 54.0735\n","EPOCH 90/100\n","Step [50/369], Loss: 0.2497, Acc : 89.86, mIoU 79.65\n","Step [100/369], Loss: 0.2274, Acc : 90.38, mIoU 81.00\n","Step [150/369], Loss: 0.2366, Acc : 90.20, mIoU 81.46\n","Step [200/369], Loss: 0.2389, Acc : 90.27, mIoU 81.40\n","Step [250/369], Loss: 0.1870, Acc : 90.33, mIoU 81.49\n","Step [300/369], Loss: 0.2719, Acc : 90.35, mIoU 81.78\n","Step [350/369], Loss: 0.3093, Acc : 90.29, mIoU 81.69\n","Epoch time : 355.7s\n","Validation . . . \n","Step [50/118], Loss: 0.9788, Acc : 81.55, mIoU 58.68\n","Step [100/118], Loss: 0.6747, Acc : 81.21, mIoU 57.76\n","Epoch time : 39.8s\n","Loss 0.8137,  Acc 80.62,  IoU 56.8043\n","EPOCH 91/100\n","Step [50/369], Loss: 0.2159, Acc : 91.00, mIoU 82.84\n","Step [100/369], Loss: 0.1876, Acc : 90.89, mIoU 82.95\n","Step [150/369], Loss: 0.2808, Acc : 90.78, mIoU 82.65\n","Step [200/369], Loss: 0.2963, Acc : 90.65, mIoU 82.46\n","Step [250/369], Loss: 0.3358, Acc : 90.46, mIoU 82.15\n","Step [300/369], Loss: 0.3485, Acc : 90.21, mIoU 81.39\n","Step [350/369], Loss: 0.2604, Acc : 90.10, mIoU 81.20\n","Epoch time : 352.0s\n","Validation . . . \n","Step [50/118], Loss: 0.4172, Acc : 81.05, mIoU 56.65\n","Step [100/118], Loss: 0.7560, Acc : 80.99, mIoU 56.02\n","Epoch time : 39.8s\n","Loss 0.6649,  Acc 80.97,  IoU 56.1379\n","EPOCH 92/100\n","Step [50/369], Loss: 0.2271, Acc : 90.34, mIoU 81.60\n","Step [100/369], Loss: 0.2491, Acc : 90.12, mIoU 81.09\n","Step [150/369], Loss: 0.2553, Acc : 89.57, mIoU 80.00\n","Step [200/369], Loss: 0.2993, Acc : 89.27, mIoU 79.64\n","Step [250/369], Loss: 0.2402, Acc : 89.32, mIoU 79.58\n","Step [300/369], Loss: 0.2935, Acc : 89.55, mIoU 80.15\n","Step [350/369], Loss: 0.2237, Acc : 89.67, mIoU 80.30\n","Epoch time : 354.9s\n","Validation . . . \n","Step [50/118], Loss: 0.5720, Acc : 80.54, mIoU 55.55\n","Step [100/118], Loss: 0.9749, Acc : 80.11, mIoU 56.18\n","Epoch time : 40.1s\n","Loss 1.0034,  Acc 79.91,  IoU 56.1164\n","EPOCH 93/100\n","Step [50/369], Loss: 0.2091, Acc : 90.81, mIoU 82.52\n","Step [100/369], Loss: 0.2184, Acc : 90.76, mIoU 82.01\n","Step [150/369], Loss: 0.2466, Acc : 90.74, mIoU 82.45\n","Step [200/369], Loss: 0.2102, Acc : 90.57, mIoU 82.42\n","Step [250/369], Loss: 0.2115, Acc : 90.58, mIoU 82.45\n","Step [300/369], Loss: 0.2775, Acc : 90.52, mIoU 82.28\n","Step [350/369], Loss: 0.2002, Acc : 90.41, mIoU 82.08\n","Epoch time : 351.7s\n","Validation . . . \n","Step [50/118], Loss: 1.1075, Acc : 80.35, mIoU 55.18\n","Step [100/118], Loss: 0.5029, Acc : 80.42, mIoU 55.94\n","Epoch time : 39.5s\n","Loss 0.4217,  Acc 80.50,  IoU 55.8615\n","EPOCH 94/100\n","Step [50/369], Loss: 0.2208, Acc : 90.79, mIoU 79.96\n","Step [100/369], Loss: 0.2201, Acc : 90.49, mIoU 80.70\n","Step [150/369], Loss: 0.2075, Acc : 90.57, mIoU 81.44\n","Step [200/369], Loss: 0.3604, Acc : 90.42, mIoU 81.78\n","Step [250/369], Loss: 0.2290, Acc : 90.35, mIoU 81.59\n","Step [300/369], Loss: 0.2591, Acc : 90.40, mIoU 81.83\n","Step [350/369], Loss: 0.2858, Acc : 90.33, mIoU 81.80\n","Epoch time : 354.7s\n","Validation . . . \n","Step [50/118], Loss: 0.8665, Acc : 80.45, mIoU 55.67\n","Step [100/118], Loss: 0.4003, Acc : 80.80, mIoU 57.39\n","Epoch time : 40.3s\n","Loss 0.7025,  Acc 80.77,  IoU 57.0697\n","EPOCH 95/100\n","Step [50/369], Loss: 0.1712, Acc : 91.25, mIoU 83.47\n","Step [100/369], Loss: 0.2376, Acc : 90.94, mIoU 82.42\n","Step [150/369], Loss: 0.3422, Acc : 90.60, mIoU 81.99\n","Step [200/369], Loss: 0.2333, Acc : 90.48, mIoU 81.87\n","Step [250/369], Loss: 0.2340, Acc : 90.44, mIoU 81.73\n","Step [300/369], Loss: 0.1872, Acc : 90.49, mIoU 82.02\n","Step [350/369], Loss: 0.2447, Acc : 90.52, mIoU 82.07\n","Epoch time : 347.6s\n","Validation . . . \n","Step [50/118], Loss: 0.4959, Acc : 79.62, mIoU 55.76\n","Step [100/118], Loss: 0.6126, Acc : 80.62, mIoU 56.28\n","Epoch time : 39.6s\n","Loss 0.8199,  Acc 80.40,  IoU 56.0798\n","EPOCH 96/100\n","Step [50/369], Loss: 0.1842, Acc : 90.54, mIoU 82.01\n","Step [100/369], Loss: 0.2344, Acc : 91.05, mIoU 82.92\n","Step [150/369], Loss: 0.3164, Acc : 90.83, mIoU 82.84\n","Step [200/369], Loss: 0.3426, Acc : 90.79, mIoU 82.76\n","Step [250/369], Loss: 0.2164, Acc : 90.69, mIoU 82.64\n","Step [300/369], Loss: 0.2650, Acc : 90.63, mIoU 82.30\n","Step [350/369], Loss: 0.2073, Acc : 90.56, mIoU 82.37\n","Epoch time : 352.5s\n","Validation . . . \n","Step [50/118], Loss: 0.4826, Acc : 80.32, mIoU 54.18\n","Step [100/118], Loss: 0.5608, Acc : 80.22, mIoU 56.19\n","Epoch time : 39.8s\n","Loss 0.8425,  Acc 80.42,  IoU 55.9082\n","EPOCH 97/100\n","Step [50/369], Loss: 0.2595, Acc : 90.77, mIoU 82.22\n","Step [100/369], Loss: 0.2000, Acc : 90.61, mIoU 81.93\n","Step [150/369], Loss: 0.2085, Acc : 90.61, mIoU 82.17\n","Step [200/369], Loss: 0.2410, Acc : 90.58, mIoU 82.23\n","Step [250/369], Loss: 0.2098, Acc : 90.50, mIoU 82.19\n","Step [300/369], Loss: 0.2494, Acc : 90.49, mIoU 82.20\n","Step [350/369], Loss: 0.2818, Acc : 90.29, mIoU 81.67\n","Epoch time : 351.0s\n","Validation . . . \n","Step [50/118], Loss: 1.0123, Acc : 80.18, mIoU 55.10\n","Step [100/118], Loss: 0.7085, Acc : 80.34, mIoU 55.60\n","Epoch time : 40.4s\n","Loss 0.7330,  Acc 80.55,  IoU 56.2951\n","EPOCH 98/100\n","Step [50/369], Loss: 0.2818, Acc : 90.91, mIoU 83.40\n","Step [100/369], Loss: 0.2082, Acc : 90.77, mIoU 82.64\n","Step [150/369], Loss: 0.1817, Acc : 90.85, mIoU 82.69\n","Step [200/369], Loss: 0.2233, Acc : 90.84, mIoU 82.77\n","Step [250/369], Loss: 0.3343, Acc : 90.85, mIoU 82.78\n","Step [300/369], Loss: 0.1519, Acc : 90.80, mIoU 82.81\n","Step [350/369], Loss: 0.2030, Acc : 90.79, mIoU 82.64\n","Epoch time : 355.0s\n","Validation . . . \n","Step [50/118], Loss: 1.5138, Acc : 79.98, mIoU 54.96\n","Step [100/118], Loss: 0.8493, Acc : 80.24, mIoU 55.87\n","Epoch time : 41.0s\n","Loss 0.6342,  Acc 80.30,  IoU 56.1938\n","EPOCH 99/100\n","Step [50/369], Loss: 0.4708, Acc : 91.08, mIoU 83.20\n","Step [100/369], Loss: 0.2936, Acc : 91.13, mIoU 83.91\n","Step [150/369], Loss: 0.3602, Acc : 90.99, mIoU 83.33\n","Step [200/369], Loss: 0.2226, Acc : 90.88, mIoU 83.21\n","Step [250/369], Loss: 0.2311, Acc : 90.90, mIoU 83.12\n","Step [300/369], Loss: 0.2246, Acc : 90.88, mIoU 82.95\n","Step [350/369], Loss: 0.2738, Acc : 90.84, mIoU 82.88\n","Epoch time : 348.4s\n","Validation . . . \n","Step [50/118], Loss: 0.5838, Acc : 80.00, mIoU 55.72\n","Step [100/118], Loss: 0.7016, Acc : 79.96, mIoU 55.17\n","Epoch time : 40.6s\n","Loss 1.0210,  Acc 80.22,  IoU 56.2316\n","EPOCH 100/100\n","Step [50/369], Loss: 0.1984, Acc : 90.46, mIoU 83.38\n","Step [100/369], Loss: 0.2692, Acc : 90.64, mIoU 82.81\n","Step [150/369], Loss: 0.3482, Acc : 90.71, mIoU 83.24\n","Step [200/369], Loss: 0.2769, Acc : 90.76, mIoU 83.07\n","Step [250/369], Loss: 0.2428, Acc : 90.84, mIoU 83.10\n","Step [300/369], Loss: 0.2343, Acc : 90.84, mIoU 82.78\n","Step [350/369], Loss: 0.2534, Acc : 90.86, mIoU 82.83\n","Epoch time : 354.6s\n","Validation . . . \n","Step [50/118], Loss: 0.5871, Acc : 79.97, mIoU 57.23\n","Step [100/118], Loss: 0.4236, Acc : 80.33, mIoU 56.55\n","Epoch time : 41.5s\n","Loss 0.4126,  Acc 80.30,  IoU 56.5654\n","Testing best epoch . . .\n","Step [50/120], Loss: 0.6787, Acc : 81.20, mIoU 57.43\n","Step [100/120], Loss: 0.4675, Acc : 81.14, mIoU 58.74\n","Epoch time : 40.8s\n","Loss 0.4582,  Acc 81.54,  IoU 58.9443\n"]}],"source":["DEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"\n","LEARNING_RATE = 0.001\n","NUM_CLASSES = 20\n","IGNORE_INDEX = -1\n","EPOCHS=100\n","PAD_VALUE = 0\n","DISPLAY_SETUP = 50\n","BATCH_SIZE=4\n","VAL_EVERY = 1\n","VAL_AFTER=0\n","RES_DIR = 'models'\n","if not os.path.exists(RES_DIR):\n","    os.makedirs(RES_DIR)\n","\n","fold_sequence = [\n","        [[1, 2, 3], [4], [5]],\n","        [[2, 3, 4], [5], [1]],\n","        [[3, 4, 5], [1], [2]],\n","        [[4, 5, 1], [2], [3]],\n","        [[5, 1, 2], [3], [4]],\n","    ]\n","\n","PASTIS_PATH = 'archive/PASTIS'\n","\n","for fold, (train_folds, val_fold, test_fold) in enumerate(fold_sequence):\n","    \n","    train_dataset =  PASTIS_Dataset(PASTIS_PATH, folds = train_folds )\n","    vali_dataset =  PASTIS_Dataset(PASTIS_PATH, folds = val_fold )\n","    test_dataset =  PASTIS_Dataset(PASTIS_PATH, folds = test_fold )\n","\n","\n","    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=pad_collate, drop_last=True,shuffle=True)\n","    vali_dataloader = torch.utils.data.DataLoader(vali_dataset, batch_size=BATCH_SIZE, collate_fn=pad_collate, drop_last=True,shuffle=True)\n","    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=pad_collate, drop_last=True,shuffle=True)\n","    \n","    print(\n","            \"Train {}, Val {}, Test {}\".format(len(train_dataloader), len(vali_dataloader), len(test_dataloader))\n","        )\n","    \n","    \n","    model = UNet3D(\n","                    in_channel=10, n_classes=NUM_CLASSES, pad_value=PAD_VALUE\n","                )\n","    N_PARAMS = get_ntrainparams(model)\n","    print(f\"Model has {N_PARAMS} trainable params\")\n","    print(model)\n","    model = model.to(DEVICE)\n","    model.apply(weight_init)\n","\n","    # Optimizer and Loss\n","    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","    weights = torch.ones(NUM_CLASSES, device=DEVICE).float()\n","    weights[IGNORE_INDEX] = 0\n","    criterion = nn.CrossEntropyLoss(weight=weights)\n","    TRAIN_LOG = {}\n","    best_mIoU = 0\n","    for epoch in range(1, EPOCHS + 1):\n","        print(\"EPOCH {}/{}\".format(epoch, EPOCHS))\n","\n","        model.train()\n","        train_metrics = iterate(\n","            model,\n","            data_loader=train_dataloader,\n","            criterion=criterion,\n","            optimizer=optimizer,\n","            mode=\"train\",\n","            device=DEVICE,\n","        )\n","        if epoch % VAL_EVERY == 0 and epoch > VAL_AFTER:\n","            print(\"Validation . . . \")\n","            model.eval()\n","            val_metrics = iterate(\n","                model,\n","                data_loader=vali_dataloader,\n","                criterion=criterion,\n","                optimizer=optimizer,\n","                mode=\"val\",\n","                device=DEVICE,\n","            )\n","\n","            print(\n","                \"Loss {:.4f},  Acc {:.2f},  IoU {:.4f}\".format(\n","                    val_metrics[\"val_loss\"],\n","                    val_metrics[\"val_accuracy\"],\n","                    val_metrics[\"val_IoU\"],\n","                )\n","            )\n","            TRAIN_LOG[epoch] = {**train_metrics, **val_metrics}\n","            if val_metrics[\"val_IoU\"] >= best_mIoU:\n","                best_mIoU = val_metrics[\"val_IoU\"]\n","                torch.save(\n","                    {\n","                        \"epoch\": epoch,\n","                        \"state_dict\": model.state_dict(),\n","                        \"optimizer\": optimizer.state_dict(),\n","                    },\n","                    os.path.join(\n","                        RES_DIR, f\"Fold_{fold+1}_model.pth.tar\"\n","                    ),\n","                )\n","        else:\n","            TRAIN_LOG[epoch] = {**train_metrics}\n","        \n","        \n","    print(\"Testing best epoch . . .\")\n","    model.load_state_dict(\n","        torch.load(\n","            os.path.join(\n","                RES_DIR, f\"Fold_{fold+1}_model.pth.tar\"\n","            )\n","        )[\"state_dict\"]\n","    )\n","    model.eval()\n","\n","    test_metrics, conf_mat = iterate(\n","        model,\n","        data_loader=test_dataloader,\n","        criterion=criterion,\n","        optimizer=optimizer,\n","        mode=\"test\",\n","        device=DEVICE,\n","    )\n","    print(\n","        \"Loss {:.4f},  Acc {:.2f},  IoU {:.4f}\".format(\n","            test_metrics[\"test_loss\"],\n","            test_metrics[\"test_accuracy\"],\n","            test_metrics[\"test_IoU\"],\n","        )\n","    )\n","    save_results(fold + 1, test_metrics, conf_mat.cpu().numpy())\n","    with open(f'Fold_{fold}_trainlogs.json', 'w') as file:\n","     file.write(json.dumps(TRAIN_LOG)) \n","            "]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing best epoch . . .\n","torch.Size([4, 61, 10, 128, 128])\n","torch.Size([4, 128, 128])\n","torch.Size([4, 128, 128])\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_294405/1452866350.py:50: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n","  cm = matplotlib.cm.get_cmap('tab20')\n"]},{"data":{"text/plain":["Text(0.5, 1.0, 'UNET-3D prediction')"]},"execution_count":40,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 2000x2000 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# TRAIN_LOG\n","fold = 0\n","print(\"Testing best epoch . . .\")\n","model.load_state_dict(\n","    torch.load(\n","        os.path.join(\n","            RES_DIR, f\"Fold_{fold+1}_model.pth.tar\"\n","        )\n","    )[\"state_dict\"]\n",")\n","model.eval()\n","\n","for i, batch in enumerate(test_dataloader):\n","    if i==0:\n","        if DEVICE is not None:\n","            batch = recursive_todevice(batch, DEVICE)\n","        (x, dates), y = batch\n","        (x, dates) = x['S2'], dates['S2']\n","        y = y.long()\n","        print(x.shape)\n","        print(y.shape)\n","\n","        with torch.no_grad():\n","            out = model(x, batch_positions=dates)\n","\n","\n","        loss = criterion(out, y)\n","\n","\n","        with torch.no_grad():\n","            pred = out.argmax(dim=1)\n","            print(pred.shape)\n","    else:\n","        break\n","\n","import torchvision.transforms as T\n","from PIL import Image\n","import cv2\n","\n","# print(pred[1].shape)\n","label1 = y[1].cpu().numpy()\n","tensor1 = pred[1].cpu().numpy()\n","# cv2.imwrite(\"tensor1.png\",tensor1)\n","# cv2.imwrite(\"label1.png\",label1)\n","\n","import matplotlib.pyplot as plt \n","import matplotlib\n","from matplotlib.colors import ListedColormap\n","\n","cm = matplotlib.cm.get_cmap('tab20')\n","def_colors = cm.colors\n","cus_colors = ['k']+[def_colors[i] for i in range(1,20)]+['w']\n","cmap = ListedColormap(colors= cus_colors, name='agri', N=21)\n","\n","fix, axes = plt.subplots(1,2, figsize=(20,20))\n","axes[0].imshow(label1,cmap=cmap, vmin=0, vmax=20 )\n","axes[1].imshow(tensor1,cmap=cmap, vmin=0, vmax=20 )\n","\n","axes[0].set_title('Ground Truth')\n","axes[1].set_title('UNET-3D prediction')\n","\n","# label1 = label1.unsqueeze(0).cpu().numpy().astype(np.uint8).transpose(1,2,0)\n","# print(label1.shape)\n","# img_transform = T.ToPILImage()\n","# label1 = img_transform(label1)\n","# output = img_transform(tensor1)\n","# label1.show()\n","# img = Image.fromarray((255*label1[0]).cpu().numpy().astype(np.uint8).transpose(1,2,0))\n","# img.show()\n","# output.show()\n","\n","\n","\n","\n","# test_metrics, conf_mat = iterate(\n","#     model,\n","#     data_loader=test_dataloader,\n","#     criterion=criterion,\n","#     optimizer=optimizer,\n","#     mode=\"test\",\n","#     device=DEVICE,\n","# )\n","# print(\n","#     \"Loss {:.4f},  Acc {:.2f},  IoU {:.4f}\".format(\n","#         test_metrics[\"test_loss\"],\n","#         test_metrics[\"test_accuracy\"],\n","#         test_metrics[\"test_IoU\"],\n","#     )\n","# )\n"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":["# !zip -r unet_allresults.zip /kaggle/working"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":2543627,"sourceId":4318633,"sourceType":"datasetVersion"},{"datasetId":4052070,"sourceId":7042517,"sourceType":"datasetVersion"}],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
